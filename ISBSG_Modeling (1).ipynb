{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0P6zKo5EEvch"},"outputs":[],"source":["#importing neccessary libraries\n","import pandas as pd\n","import numpy as np\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4142,"status":"ok","timestamp":1720053189932,"user":{"displayName":"Shabana Yasmin","userId":"18302913856572837226"},"user_tz":-330},"id":"Cg7ygG_zHJZC","outputId":"264e2914-9499-4fdb-b1ea-18da967e42aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AmMCAaLYHN44"},"outputs":[],"source":["df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/processed_dataset (2).csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":261},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1720053189933,"user":{"displayName":"Shabana Yasmin","userId":"18302913856572837226"},"user_tz":-330},"id":"XSIjLxBiHgz5","outputId":"552da465-f4af-4aed-ec8d-6735927f4f06"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Year of Project  Industry Sector  Application Type  Development Type  \\\n","0             2015                2               376                 0   \n","1             2016                2               142                 0   \n","2             1996                3               492                 3   \n","3             2002               22                25                 0   \n","4             2004                2               219                 0   \n","\n","   Development Platform  Language Type  Primary Programming Language  \\\n","0                     4              1                            79   \n","1                     4              1                            79   \n","2                     4              2                            19   \n","3                     2              1                            41   \n","4                     4              1                            79   \n","\n","   Count Approach  Functional Size  Relative Size  Summary Work Effort  \\\n","0               6             67.0              3                608.0   \n","1               6             51.0              3                288.0   \n","2               6            443.0              2                796.0   \n","3               6             76.0              3               1100.0   \n","4               6              3.0              8                 28.0   \n","\n","   Normalised PDR (ufp)  Project Elapsed Time  Max Team Size  \\\n","0                  11.1                   4.4            6.0   \n","1                   6.2                   1.0            6.0   \n","2                   1.9                   2.6            5.0   \n","3                  14.5                   4.0            6.0   \n","4                   9.3                   6.0            6.0   \n","\n","   Development Methodologies  FP Standard  Package Customisation  \\\n","0                         34           49                      1   \n","1                         34           49                      1   \n","2                         34            0                      1   \n","3                         34           45                      1   \n","4                         34           30                      1   \n","\n","   Total Project Cost (USD)  \n","0              51311.800081  \n","1              32217.932055  \n","2              30488.742753  \n","3              55728.768217  \n","4               4666.998461  "],"text/html":["\n","  <div id=\"df-248fb507-d7d3-4121-be5a-b61842fd5aef\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Year of Project</th>\n","      <th>Industry Sector</th>\n","      <th>Application Type</th>\n","      <th>Development Type</th>\n","      <th>Development Platform</th>\n","      <th>Language Type</th>\n","      <th>Primary Programming Language</th>\n","      <th>Count Approach</th>\n","      <th>Functional Size</th>\n","      <th>Relative Size</th>\n","      <th>Summary Work Effort</th>\n","      <th>Normalised PDR (ufp)</th>\n","      <th>Project Elapsed Time</th>\n","      <th>Max Team Size</th>\n","      <th>Development Methodologies</th>\n","      <th>FP Standard</th>\n","      <th>Package Customisation</th>\n","      <th>Total Project Cost (USD)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2015</td>\n","      <td>2</td>\n","      <td>376</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>79</td>\n","      <td>6</td>\n","      <td>67.0</td>\n","      <td>3</td>\n","      <td>608.0</td>\n","      <td>11.1</td>\n","      <td>4.4</td>\n","      <td>6.0</td>\n","      <td>34</td>\n","      <td>49</td>\n","      <td>1</td>\n","      <td>51311.800081</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2016</td>\n","      <td>2</td>\n","      <td>142</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>79</td>\n","      <td>6</td>\n","      <td>51.0</td>\n","      <td>3</td>\n","      <td>288.0</td>\n","      <td>6.2</td>\n","      <td>1.0</td>\n","      <td>6.0</td>\n","      <td>34</td>\n","      <td>49</td>\n","      <td>1</td>\n","      <td>32217.932055</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1996</td>\n","      <td>3</td>\n","      <td>492</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>19</td>\n","      <td>6</td>\n","      <td>443.0</td>\n","      <td>2</td>\n","      <td>796.0</td>\n","      <td>1.9</td>\n","      <td>2.6</td>\n","      <td>5.0</td>\n","      <td>34</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>30488.742753</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2002</td>\n","      <td>22</td>\n","      <td>25</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>41</td>\n","      <td>6</td>\n","      <td>76.0</td>\n","      <td>3</td>\n","      <td>1100.0</td>\n","      <td>14.5</td>\n","      <td>4.0</td>\n","      <td>6.0</td>\n","      <td>34</td>\n","      <td>45</td>\n","      <td>1</td>\n","      <td>55728.768217</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2004</td>\n","      <td>2</td>\n","      <td>219</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>79</td>\n","      <td>6</td>\n","      <td>3.0</td>\n","      <td>8</td>\n","      <td>28.0</td>\n","      <td>9.3</td>\n","      <td>6.0</td>\n","      <td>6.0</td>\n","      <td>34</td>\n","      <td>30</td>\n","      <td>1</td>\n","      <td>4666.998461</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-248fb507-d7d3-4121-be5a-b61842fd5aef')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-248fb507-d7d3-4121-be5a-b61842fd5aef button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-248fb507-d7d3-4121-be5a-b61842fd5aef');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-c7bbe74a-ae6d-4fd7-a61a-1521d5ce1426\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c7bbe74a-ae6d-4fd7-a61a-1521d5ce1426')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-c7bbe74a-ae6d-4fd7-a61a-1521d5ce1426 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 8473,\n  \"fields\": [\n    {\n      \"column\": \"Year of Project\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 1989,\n        \"max\": 2020,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          1991,\n          2003,\n          1995\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Industry Sector\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 0,\n        \"max\": 22,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          12,\n          20,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Application Type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 152,\n        \"min\": 0,\n        \"max\": 567,\n        \"num_unique_values\": 525,\n        \"samples\": [\n          412,\n          40,\n          148\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Development Type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3,\n          2,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Development Platform\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 7,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          2,\n          6,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Language Type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Primary Programming Language\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 35,\n        \"min\": 0,\n        \"max\": 160,\n        \"num_unique_values\": 145,\n        \"samples\": [\n          13,\n          149,\n          103\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Count Approach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 12,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          3,\n          5,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Functional Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 413.98235259110686,\n        \"min\": 1.0,\n        \"max\": 2797.0,\n        \"num_unique_values\": 1389,\n        \"samples\": [\n          131.0,\n          366.0,\n          199.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Relative Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 8,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          7,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Summary Work Effort\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4052.3442204684707,\n        \"min\": 0.0,\n        \"max\": 35415.0,\n        \"num_unique_values\": 4516,\n        \"samples\": [\n          21700.0,\n          4045.0,\n          4116.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Normalised PDR (ufp)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 26.841792883125017,\n        \"min\": 0.0,\n        \"max\": 282.7,\n        \"num_unique_values\": 885,\n        \"samples\": [\n          0.8,\n          29.1,\n          61.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Project Elapsed Time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.375780601527492,\n        \"min\": 0.03,\n        \"max\": 65.0,\n        \"num_unique_values\": 299,\n        \"samples\": [\n          0.78,\n          23.1,\n          10.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Max Team Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.8274700960694075,\n        \"min\": 0.3,\n        \"max\": 63.0,\n        \"num_unique_values\": 158,\n        \"samples\": [\n          1.2,\n          4.2,\n          1.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Development Methodologies\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13,\n        \"min\": 0,\n        \"max\": 35,\n        \"num_unique_values\": 34,\n        \"samples\": [\n          16,\n          3,\n          29\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FP Standard\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17,\n        \"min\": 0,\n        \"max\": 74,\n        \"num_unique_values\": 72,\n        \"samples\": [\n          41,\n          23,\n          27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Package Customisation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total Project Cost (USD)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1389049.6502602175,\n        \"min\": 106.335,\n        \"max\": 105839940.0,\n        \"num_unique_values\": 8460,\n        \"samples\": [\n          6369.776036258389\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":4}],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1720053189933,"user":{"displayName":"Shabana Yasmin","userId":"18302913856572837226"},"user_tz":-330},"id":"jN8zGINRCo21","outputId":"8d217709-5ec0-4557-9fea-9f432c0bae68"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8473, 18)"]},"metadata":{},"execution_count":5}],"source":["df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1720053189933,"user":{"displayName":"Shabana Yasmin","userId":"18302913856572837226"},"user_tz":-330},"id":"zYQ8uu7L_UdW","outputId":"b8aa90c3-fdef-47ba-e03d-119f3b6101ee"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['Year of Project', 'Industry Sector', 'Application Type',\n","       'Development Type', 'Development Platform', 'Language Type',\n","       'Primary Programming Language', 'Count Approach', 'Functional Size',\n","       'Relative Size', 'Summary Work Effort', 'Normalised PDR (ufp)',\n","       'Project Elapsed Time', 'Max Team Size', 'Development Methodologies',\n","       'FP Standard', 'Package Customisation', 'Total Project Cost (USD)'],\n","      dtype='object')"]},"metadata":{},"execution_count":6}],"source":["df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7143,"status":"ok","timestamp":1720053197069,"user":{"displayName":"Shabana Yasmin","userId":"18302913856572837226"},"user_tz":-330},"outputId":"2784dfc8-a5c1-44c8-d806-1fd62d3534c8","id":"_J9H_vpjgp71"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikeras in /usr/local/lib/python3.10/dist-packages (0.13.0)\n","Collecting keras>=3.2.0 (from scikeras)\n","  Using cached keras-3.4.1-py3-none-any.whl (1.1 MB)\n","Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.5.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.25.2)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (13.7.1)\n","Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (3.9.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.11.0)\n","Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.2.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (24.1)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.11.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n","Installing collected packages: keras\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.15.0\n","    Uninstalling keras-2.15.0:\n","      Successfully uninstalled keras-2.15.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed keras-3.4.1\n"]}],"source":["!pip install scikeras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LwWtcNcj_IVp"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","from keras.optimizers import RMSprop\n","from keras.layers import LeakyReLU, ELU, PReLU\n","from keras.layers import Dense, Dropout, Activation\n","from keras.models import Sequential\n","from keras.regularizers import l1, l2\n","from keras.callbacks import EarlyStopping\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from scikeras.wrappers import KerasRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nq-nxO3mzYeI"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fi-vU7lg987Z"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"nY8gQllSN1SV"},"source":["Feed-Forward Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7nwIBOTIP01z","executionInfo":{"status":"ok","timestamp":1720052594602,"user_tz":-330,"elapsed":2761261,"user":{"displayName":"Shabana Yasmin","userId":"18302913856572837226"}},"outputId":"11b89324-95a7-4576-c3bd-468fdbda2b7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 3 folds for each of 20 candidates, totalling 60 fits\n","Best Hyperparameters: {'model__regularization': None, 'model__optimizer': 'rmsprop', 'model__neurons': 256, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.3, 'model__activation': 'leakyrelu', 'epochs': 100, 'batch_size': 16}\n","MAE: 37273.11280210122\n","MAE as percentage of average actual cost: 18.289836695150296\n","Mean Squared Error: 10852252254.077131\n","RMSE: 104174.14388454138\n","RMSE as percentage of average actual cost: 51.11802949277533\n","R-squared: 0.925278358690833\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, LeakyReLU\n","from scikeras.wrappers import KerasRegressor\n","from tensorflow.keras.optimizers import RMSprop, Adam\n","from tensorflow.keras.regularizers import l1_l2\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","# Normalize features\n","scaler_X = StandardScaler()\n","X = scaler_X.fit_transform(X)\n","X = pd.DataFrame(X)\n","\n","X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n","\n","# Define the function to create the Keras model\n","def create_model(learning_rate=0.001, neurons=64, dropout_rate=0.5, activation='leakyrelu', optimizer='adam', regularization=None):\n","    model = Sequential()\n","    if regularization == 'l1_l2':\n","        reg = l1_l2(l1=0.01, l2=0.01)\n","    else:\n","        reg = None\n","\n","    model.add(Dense(neurons, input_shape=(X_train.shape[1],), kernel_regularizer=reg))\n","    if activation == 'leakyrelu':\n","        model.add(LeakyReLU(alpha=0.01))\n","    else:\n","        model.add(Activation(activation))\n","    model.add(Dropout(dropout_rate))\n","\n","    model.add(Dense(neurons, kernel_regularizer=reg))\n","    if activation == 'leakyrelu':\n","        model.add(LeakyReLU(alpha=0.01))\n","    else:\n","        model.add(Activation(activation))\n","    model.add(Dropout(dropout_rate))\n","\n","    model.add(Dense(neurons // 2, kernel_regularizer=reg))\n","    if activation == 'leakyrelu':\n","        model.add(LeakyReLU(alpha=0.01))\n","    else:\n","        model.add(Activation(activation))\n","    model.add(Dropout(dropout_rate))\n","\n","    model.add(Dense(neurons // 2, kernel_regularizer=reg))\n","    if activation == 'leakyrelu':\n","        model.add(LeakyReLU(alpha=0.01))\n","    else:\n","        model.add(Activation(activation))\n","    model.add(Dense(1))\n","\n","    if optimizer == 'rmsprop':\n","        opt = RMSprop(learning_rate=learning_rate)\n","    elif optimizer == 'adam':\n","        opt = Adam(learning_rate=learning_rate)\n","\n","    model.compile(loss='mse', optimizer=opt, metrics=['mae'])\n","    return model\n","\n","# Wrap the model using KerasRegressor\n","model = KerasRegressor(model=create_model, verbose=0)\n","\n","# Define the hyperparameters to tune\n","param_dist = {\n","    'model__learning_rate': [0.0001, 0.001, 0.01, 0.1],\n","    'model__neurons': [32, 64, 128, 256],\n","    'model__dropout_rate': [0.2, 0.3, 0.5, 0.7],\n","    'model__activation': ['leakyrelu', 'relu', 'elu'],\n","    'model__optimizer': ['rmsprop', 'adam'],\n","    'model__regularization': [None, 'l1_l2'],\n","    'epochs': [50, 100, 200, 300],\n","    'batch_size': [16, 32, 64, 128]\n","}\n","\n","# Use RandomizedSearchCV to find the best hyperparameters\n","random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=20, cv=3, random_state=42, verbose=1, n_jobs=-1)\n","random_search_result = random_search.fit(X_train, y_train)\n","\n","# Print the best hyperparameters\n","print(f\"Best Hyperparameters: {random_search_result.best_params_}\")\n","\n","# Evaluate the best model on the test set\n","best_model = random_search_result.best_estimator_\n","y_pred = best_model.predict(X_test)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Calculate the average of the actual 'Total Project Cost (USD)'\n","average_actual_cost = np.mean(y_test)\n","\n","# Calculate MAE and RMSE as percentages of the average actual cost\n","mae_percentage = (mae / average_actual_cost) * 100\n","rmse_percentage = (rmse / average_actual_cost) * 100\n","\n","# Print results\n","print(\"MAE:\", mae)\n","print(\"MAE as percentage of average actual cost:\", mae_percentage)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"RMSE as percentage of average actual cost:\", rmse_percentage)\n","print(\"R-squared:\", r2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QCudDRCUfsuA","outputId":"9495ed08-027c-426d-d468-aacbfddcf0f8","executionInfo":{"status":"ok","timestamp":1719533452077,"user_tz":-330,"elapsed":3027828,"user":{"displayName":"Shabana Yasmin","userId":"02459626693949289733"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","Best Hyperparameters: {'model__neurons': 256, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.5, 'epochs': 100, 'batch_size': 16}\n","MAE: 38690.73773656785\n","Mean Squared Error: 15051366702.83993\n","RMSE: 122684.0116023271\n","R-squared: 0.8963659526473119\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, LeakyReLU\n","from scikeras.wrappers import KerasRegressor\n","from tensorflow.keras.optimizers import RMSprop\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","\n","# Load your dataset here (replace with your actual data loading code)\n","# For example:\n","# df = pd.read_csv('your_dataset.csv')\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","# Normalize features\n","scaler_X = StandardScaler()\n","X = scaler_X.fit_transform(X)\n","X = pd.DataFrame(X)\n","\n","# Split data into train and test sets\n","X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n","\n","# Define the function to create the Keras model\n","def create_model(learning_rate=0.001, neurons=256, dropout_rate=0.3): # no change here\n","    model = Sequential()\n","    model.add(Dense(neurons, input_shape=(X_train.shape[1],)))\n","    model.add(LeakyReLU(alpha=0.01))\n","    model.add(Dropout(dropout_rate))\n","\n","    model.add(Dense(neurons))\n","    model.add(LeakyReLU(alpha=0.01))\n","    model.add(Dropout(dropout_rate))\n","\n","    model.add(Dense(neurons // 2))\n","    model.add(LeakyReLU(alpha=0.01))\n","    model.add(Dropout(dropout_rate))\n","\n","    model.add(Dense(neurons // 2))\n","    model.add(LeakyReLU(alpha=0.01))\n","\n","    model.add(Dense(1))\n","\n","    opt = RMSprop(learning_rate=learning_rate)\n","    model.compile(loss='mse', optimizer=opt, metrics=['mae'])\n","    return model\n","\n","# Wrap the model using KerasRegressor, exposing 'neurons' as a tunable hyperparameter\n","model = KerasRegressor(model=create_model, learning_rate=0.001, neurons=256, dropout_rate=0.3, verbose=0)\n","\n","# Define the hyperparameters to tune\n","param_dist = {\n","    'model__learning_rate': [0.0001, 0.001, 0.01],\n","    'model__neurons': [128, 256, 512], # Access the 'neurons' parameter through 'model__' prefix\n","    'model__dropout_rate': [0.2, 0.3, 0.5], # Access the 'dropout_rate' parameter through 'model__' prefix\n","    'epochs': [100],\n","    'batch_size': [16, 32, 64]\n","}\n","\n","# Use KFold cross-validation\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Define callbacks for learning rate scheduling and early stopping\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","\n","\n","# Use RandomizedSearchCV with KFold cross-validation\n","random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=kf, scoring='neg_mean_absolute_error', verbose=1, random_state=42, n_jobs=-1)\n","random_search_result = random_search.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=[reduce_lr, early_stopping])\n","\n","# Print the best hyperparameters\n","print(f\"Best Hyperparameters: {random_search_result.best_params_}\")\n","\n","# Evaluate the best model on the test set\n","best_model = random_search_result.best_estimator_\n","y_pred = best_model.predict(X_test)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CNiwaXpQMovz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719522414258,"user_tz":-330,"elapsed":45380,"user":{"displayName":"Shabana Yasmin","userId":"18302913856572837226"}},"outputId":"dc1736ec-b8fa-4a54-90b3-010ea5cd953e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","MAE: 49058.52355075877\n","Mean Squared Error: 19989859409.16255\n","RMSE: 141385.49928886822\n","R-squared: 0.862362662641669\n"]}],"source":["from keras.optimizers import RMSprop\n","from keras.layers import Dense, Dropout, Activation, LeakyReLU # Removed ThresholdedReLU import\n","from keras.models import Sequential\n","from keras.regularizers import l1, l2\n","from keras.callbacks import EarlyStopping\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import numpy as np\n","import pandas as pd\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","X = pd.DataFrame(X)\n","\n","X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n","\n","# Define the model\n","model1 = Sequential([\n","    Dense(128, input_shape=(X_train.shape[1],), activation=LeakyReLU(alpha=0.01)),\n","    Dropout(0.5),\n","    Dense(128, activation=LeakyReLU(alpha=0.01)),\n","    Dropout(0.5),\n","    Dense(64, activation=LeakyReLU(alpha=0.01)),\n","    Dropout(0.5),\n","    Dense(64, activation='relu'),  # Replace ThresholdedReLU with a standard ReLU activation\n","    Dense(1)\n","])\n","\n","learning_rate = 0.001\n","optimizer = RMSprop(learning_rate)\n","model1.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n","\n","# Early stopping callback\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","\n","# Train the model with early stopping for validation performance\n","model1.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val), verbose=0, callbacks=[early_stopping])\n","\n","# Predict\n","y_pred1 = model1.predict(X_test)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, y_pred1)\n","mse = mean_squared_error(y_test, y_pred1)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred1)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ay-dag14N0m2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719522846675,"user_tz":-330,"elapsed":54615,"user":{"displayName":"Shabana Yasmin","userId":"18302913856572837226"}},"outputId":"ba131b63-68e9-42a7-bdff-a2330dec320c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","MAE: 50763.61174275873\n","Mean Squared Error: 20808879049.188763\n","RMSE: 144252.83029871117\n","R-squared: 0.8567234192538082\n"]}],"source":["from keras.optimizers import RMSprop\n","from keras.layers import Dense, Dropout, Activation\n","from keras.models import Sequential\n","from keras.regularizers import l1, l2\n","from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.layers import ThresholdedReLU  # Assuming you have TensorFlow installed\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import numpy as np\n","import pandas as pd\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","# Normalize features\n","scaler_X = StandardScaler()\n","X = scaler_X.fit_transform(X)\n","X = pd.DataFrame(X)\n","\n","X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n","\n","# Define the model\n","model1 = Sequential([\n","    Dense(128, input_shape=(X_train.shape[1],), activation=LeakyReLU(alpha=0.01), kernel_regularizer=l2(0.01)),\n","    Dropout(0.5),\n","    Dense(128, activation=LeakyReLU(alpha=0.01), kernel_regularizer=l2(0.01)),\n","    Dropout(0.5),\n","    Dense(64, activation=LeakyReLU(alpha=0.01), kernel_regularizer=l2(0.01)),\n","    Dropout(0.5),\n","    Dense(64, activation=ThresholdedReLU(theta=1.0), kernel_regularizer=l2(0.01)),\n","    Dense(1)\n","])\n","\n","learning_rate = 0.001\n","optimizer = RMSprop(learning_rate)\n","model1.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n","\n","# Early stopping and learning rate reduction on plateau\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n","\n","# Train the model with early stopping and learning rate reduction on plateau\n","model1.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val), verbose=0, callbacks=[early_stopping, reduce_lr])\n","\n","# Predict\n","y_pred1 = model1.predict(X_test)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, y_pred1)\n","mse = mean_squared_error(y_test, y_pred1)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred1)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"14WlMlIROlBK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719522871311,"user_tz":-330,"elapsed":24640,"user":{"displayName":"Shabana Yasmin","userId":"18302913856572837226"}},"outputId":"30c63f80-9e1f-4f6e-bee2-251ee56c2d90"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","MAE: 79743.67317726274\n","Mean Squared Error: 26194289744.135487\n","RMSE: 161846.5005619074\n","R-squared: 0.8196429389231765\n"]}],"source":["from keras.optimizers import RMSprop, Adam\n","from keras.layers import Dense, Dropout, Activation\n","from keras.models import Sequential\n","from keras.regularizers import l1_l2\n","from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.layers import ThresholdedReLU  # Assuming you have TensorFlow installed\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import numpy as np\n","import pandas as pd\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","# Normalize features\n","scaler_X = StandardScaler()\n","X = scaler_X.fit_transform(X)\n","X = pd.DataFrame(X)\n","\n","X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n","\n","# Define the model\n","model1 = Sequential([\n","    Dense(128, input_shape=(X_train.shape[1],), activation=LeakyReLU(alpha=0.01), kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n","    Dropout(0.5),\n","    Dense(128, activation=LeakyReLU(alpha=0.01), kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n","    Dropout(0.5),\n","    Dense(64, activation=LeakyReLU(alpha=0.01), kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n","    Dropout(0.5),\n","    Dense(64, activation=ThresholdedReLU(theta=1.0), kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n","    Dense(1)\n","])\n","\n","# Experiment with Adam optimizer\n","learning_rate = 0.001\n","optimizer = Adam(learning_rate)\n","model1.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n","\n","# Early stopping and learning rate reduction on plateau\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n","\n","# Train the model with early stopping and learning rate reduction on plateau\n","model1.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val), verbose=0, callbacks=[early_stopping, reduce_lr])\n","\n","# Predict\n","y_pred1 = model1.predict(X_test)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, y_pred1)\n","mse = mean_squared_error(y_test, y_pred1)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred1)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlMUrdKW4XX5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719533529348,"user_tz":-330,"elapsed":77291,"user":{"displayName":"Shabana Yasmin","userId":"02459626693949289733"}},"outputId":"71a07510-33bf-4ba5-86fc-0afe23cb435f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 20ms/step - loss: 5610297884672.0000 - mae: 279117.6875 - val_loss: 153197674496.0000 - val_mae: 170047.6719 - learning_rate: 0.0010\n","Epoch 2/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 1204625866752.0000 - mae: 161997.2656 - val_loss: 62796976128.0000 - val_mae: 87493.2656 - learning_rate: 0.0010\n","Epoch 3/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4058653655040.0000 - mae: 143179.0312 - val_loss: 53351243776.0000 - val_mae: 85106.4609 - learning_rate: 0.0010\n","Epoch 4/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 856491753472.0000 - mae: 112218.8516 - val_loss: 48893833216.0000 - val_mae: 82245.5547 - learning_rate: 0.0010\n","Epoch 5/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1880391155712.0000 - mae: 114144.5000 - val_loss: 45729132544.0000 - val_mae: 79931.6484 - learning_rate: 0.0010\n","Epoch 6/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 320113442816.0000 - mae: 98598.5234 - val_loss: 43239391232.0000 - val_mae: 76444.1875 - learning_rate: 0.0010\n","Epoch 7/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1387933728768.0000 - mae: 106136.3516 - val_loss: 41024606208.0000 - val_mae: 77193.9766 - learning_rate: 0.0010\n","Epoch 8/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2867987480576.0000 - mae: 122937.7500 - val_loss: 39495028736.0000 - val_mae: 73124.2266 - learning_rate: 0.0010\n","Epoch 9/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1332148174848.0000 - mae: 101640.3828 - val_loss: 38015160320.0000 - val_mae: 71047.2578 - learning_rate: 0.0010\n","Epoch 10/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 758662692864.0000 - mae: 96391.6250 - val_loss: 36907364352.0000 - val_mae: 70486.3438 - learning_rate: 0.0010\n","Epoch 11/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 735024381952.0000 - mae: 96948.2891 - val_loss: 35745378304.0000 - val_mae: 69510.3281 - learning_rate: 0.0010\n","Epoch 12/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 1809150509056.0000 - mae: 99242.5859 - val_loss: 34775097344.0000 - val_mae: 68612.2266 - learning_rate: 0.0010\n","Epoch 13/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3839902351360.0000 - mae: 124148.2969 - val_loss: 33884692480.0000 - val_mae: 66904.5000 - learning_rate: 0.0010\n","Epoch 14/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3080816689152.0000 - mae: 114729.4141 - val_loss: 33062033408.0000 - val_mae: 67082.5391 - learning_rate: 0.0010\n","Epoch 15/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1468969910272.0000 - mae: 99750.1094 - val_loss: 32416423936.0000 - val_mae: 67163.6484 - learning_rate: 0.0010\n","Epoch 16/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 614160072704.0000 - mae: 88460.2188 - val_loss: 31882854400.0000 - val_mae: 67150.1875 - learning_rate: 0.0010\n","Epoch 17/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 877799997440.0000 - mae: 84484.0859 - val_loss: 31679422464.0000 - val_mae: 69353.4297 - learning_rate: 0.0010\n","Epoch 18/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2352752099328.0000 - mae: 115647.2812 - val_loss: 31451684864.0000 - val_mae: 68896.9297 - learning_rate: 0.0010\n","Epoch 19/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2724289052672.0000 - mae: 113760.9609 - val_loss: 31240947712.0000 - val_mae: 68669.8672 - learning_rate: 0.0010\n","Epoch 20/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2974410604544.0000 - mae: 121154.4766 - val_loss: 31086047232.0000 - val_mae: 68755.7188 - learning_rate: 0.0010\n","Epoch 21/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4702986043392.0000 - mae: 137039.8750 - val_loss: 30894055424.0000 - val_mae: 68315.1484 - learning_rate: 0.0010\n","Epoch 22/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1448868052992.0000 - mae: 98103.3984 - val_loss: 30627024896.0000 - val_mae: 69285.2891 - learning_rate: 0.0010\n","Epoch 23/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 975003320320.0000 - mae: 98424.5547 - val_loss: 29958369280.0000 - val_mae: 66583.6172 - learning_rate: 0.0010\n","Epoch 24/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 601776586752.0000 - mae: 89503.4766 - val_loss: 30228809728.0000 - val_mae: 70703.7031 - learning_rate: 0.0010\n","Epoch 25/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1357550845952.0000 - mae: 95886.6016 - val_loss: 30092912640.0000 - val_mae: 70738.1875 - learning_rate: 0.0010\n","Epoch 26/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1862744014848.0000 - mae: 106778.6953 - val_loss: 29874286592.0000 - val_mae: 70153.7188 - learning_rate: 0.0010\n","Epoch 27/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1825342488576.0000 - mae: 115498.5156 - val_loss: 29833261056.0000 - val_mae: 70245.9219 - learning_rate: 0.0010\n","Epoch 28/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 853145485312.0000 - mae: 91807.4844 - val_loss: 29764798464.0000 - val_mae: 70751.2188 - learning_rate: 0.0010\n","Epoch 29/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2243259006976.0000 - mae: 121108.3516 - val_loss: 29725112320.0000 - val_mae: 71516.2891 - learning_rate: 0.0010\n","Epoch 30/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3871058952192.0000 - mae: 121244.7500 - val_loss: 29702739968.0000 - val_mae: 71772.4844 - learning_rate: 0.0010\n","Epoch 31/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3421404659712.0000 - mae: 122298.7969 - val_loss: 29929523200.0000 - val_mae: 74130.8281 - learning_rate: 0.0010\n","Epoch 32/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3683943186432.0000 - mae: 129832.1172 - val_loss: 29744076800.0000 - val_mae: 73329.6641 - learning_rate: 0.0010\n","Epoch 33/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2371676012544.0000 - mae: 105761.1641 - val_loss: 29663604736.0000 - val_mae: 73587.4531 - learning_rate: 0.0010\n","Epoch 34/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1434871398400.0000 - mae: 103941.5703 - val_loss: 29742772224.0000 - val_mae: 74161.3594 - learning_rate: 0.0010\n","Epoch 35/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6350899249152.0000 - mae: 151544.2188 - val_loss: 29905496064.0000 - val_mae: 75438.1328 - learning_rate: 0.0010\n","Epoch 36/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3150217216000.0000 - mae: 121130.9531 - val_loss: 29798328320.0000 - val_mae: 75341.3438 - learning_rate: 0.0010\n","Epoch 37/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1226247766016.0000 - mae: 103960.9219 - val_loss: 29575487488.0000 - val_mae: 74437.6641 - learning_rate: 0.0010\n","Epoch 38/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2819365797888.0000 - mae: 120098.8281 - val_loss: 29565788160.0000 - val_mae: 74841.8203 - learning_rate: 0.0010\n","Epoch 39/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2341817810944.0000 - mae: 112777.1875 - val_loss: 29631952896.0000 - val_mae: 75793.7891 - learning_rate: 0.0010\n","Epoch 40/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 999600619520.0000 - mae: 113727.5859 - val_loss: 29521004544.0000 - val_mae: 75473.0469 - learning_rate: 0.0010\n","Epoch 41/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2850898575360.0000 - mae: 114981.5469 - val_loss: 29459625984.0000 - val_mae: 75511.9531 - learning_rate: 0.0010\n","Epoch 42/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2175401328640.0000 - mae: 116200.3203 - val_loss: 29747888128.0000 - val_mae: 76664.6719 - learning_rate: 0.0010\n","Epoch 43/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 992987840512.0000 - mae: 108045.7812 - val_loss: 29972940800.0000 - val_mae: 78588.2891 - learning_rate: 0.0010\n","Epoch 44/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4361903407104.0000 - mae: 135534.7188 - val_loss: 30208935936.0000 - val_mae: 79219.5312 - learning_rate: 0.0010\n","Epoch 45/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 424306180096.0000 - mae: 97334.2734 - val_loss: 30348003328.0000 - val_mae: 79868.5156 - learning_rate: 0.0010\n","Epoch 46/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1417223733248.0000 - mae: 111466.5547 - val_loss: 30178842624.0000 - val_mae: 78989.5156 - learning_rate: 0.0010\n","Epoch 47/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 930928197632.0000 - mae: 100631.0391 - val_loss: 30169260032.0000 - val_mae: 79006.1562 - learning_rate: 2.0000e-04\n","Epoch 48/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1505112358912.0000 - mae: 112321.6797 - val_loss: 30108727296.0000 - val_mae: 78738.5625 - learning_rate: 2.0000e-04\n","Epoch 49/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6397701914624.0000 - mae: 164860.1719 - val_loss: 30213943296.0000 - val_mae: 79223.6250 - learning_rate: 2.0000e-04\n","Epoch 50/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2185210626048.0000 - mae: 121123.9453 - val_loss: 30228459520.0000 - val_mae: 79183.0312 - learning_rate: 2.0000e-04\n","Epoch 51/100\n","\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3566283522048.0000 - mae: 135047.0156 - val_loss: 30279636992.0000 - val_mae: 79435.4062 - learning_rate: 2.0000e-04\n","\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n","MAE: 79690.33518368358\n","Mean Squared Error: 26431481176.714417\n","RMSE: 162577.61585382663\n","R-squared: 0.8180097910077185\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, LeakyReLU\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","\n","# Load your dataset here (replace with your actual data loading code)\n","# For example:\n","# df = pd.read_csv('your_dataset.csv')\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","# Normalize features\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","\n","# Define the model\n","model = Sequential([\n","    Dense(128, input_shape=(X_train.shape[1],)),\n","    LeakyReLU(alpha=0.01),\n","    Dropout(0.5),\n","    Dense(128),\n","    LeakyReLU(alpha=0.01),\n","    Dropout(0.5),\n","    Dense(64),\n","    LeakyReLU(alpha=0.01),\n","    Dropout(0.5),\n","    Dense(1)\n","])\n","\n","# Compile the model with Adam optimizer and reduced learning rate\n","optimizer = Adam(learning_rate=0.001)\n","model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n","\n","# Define callbacks for learning rate scheduling and early stopping\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_val, y_val), callbacks=[reduce_lr, early_stopping])\n","\n","# Predict on test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1rVbg6QFgwz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719533602504,"user_tz":-330,"elapsed":73177,"user":{"displayName":"Shabana Yasmin","userId":"02459626693949289733"}},"outputId":"d179aeba-c849-4e7c-80bb-d28f28f4c801"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n","MAE: 102346.46217496958\n","Mean Squared Error: 33341199565.939247\n","RMSE: 182595.72712946832\n","R-squared: 0.7704339065793926\n"]}],"source":["from keras.optimizers import Adam\n","from keras.layers import Dense, Dropout, LeakyReLU, Activation # Import LeakyReLU\n","from keras.models import Sequential\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","# Normalize features\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Split data into train and test sets\n","X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n","\n","# Define the model\n","model = Sequential([\n","    Dense(256, input_shape=(X_train.shape[1],)),\n","    LeakyReLU(), # Use LeakyReLU layer instead of Activation\n","    Dropout(0.5),\n","    Dense(128),\n","    LeakyReLU(), # Use LeakyReLU layer instead of Activation\n","    Dropout(0.5),\n","    Dense(64),\n","    LeakyReLU(), # Use LeakyReLU layer instead of Activation\n","    Dropout(0.5),\n","    Dense(1)\n","])\n","\n","# Compile the model with a lower learning rate\n","optimizer = Adam(learning_rate=0.001)\n","model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n","\n","# Train the model with early stopping for validation performance\n","model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), verbose=0)\n","\n","# Predict\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1719456967929,"user":{"displayName":"Shabana Yasmin","userId":"18302913856572837226"},"user_tz":-330},"id":"ypeul0EEUlnA","outputId":"b18f8f44-2cc4-47c7-84e0-af6be0a24721"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_25 (Dense)            (None, 128)               2304      \n","                                                                 \n"," dropout_10 (Dropout)        (None, 128)               0         \n","                                                                 \n"," batch_normalization (Batch  (None, 128)               512       \n"," Normalization)                                                  \n","                                                                 \n"," dense_26 (Dense)            (None, 128)               16512     \n","                                                                 \n"," dropout_11 (Dropout)        (None, 128)               0         \n","                                                                 \n"," batch_normalization_1 (Bat  (None, 128)               512       \n"," chNormalization)                                                \n","                                                                 \n"," dense_27 (Dense)            (None, 64)                8256      \n","                                                                 \n"," dropout_12 (Dropout)        (None, 64)                0         \n","                                                                 \n"," batch_normalization_2 (Bat  (None, 64)                256       \n"," chNormalization)                                                \n","                                                                 \n"," dense_28 (Dense)            (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 28417 (111.00 KB)\n","Trainable params: 27777 (108.50 KB)\n","Non-trainable params: 640 (2.50 KB)\n","_________________________________________________________________\n"]}],"source":["from keras.optimizers import Adam\n","from keras.layers import LeakyReLU, ELU, PReLU, ThresholdedReLU\n","from keras.layers import Dense, Dropout, Activation\n","from keras.models import Sequential\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, BatchNormalization\n","from keras.regularizers import l1, l2\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","scaler = StandardScaler()\n","x = scaler.fit_transform(X)\n","X=pd.DataFrame(x)\n","\n","X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n","\n","# Define the model\n","model5 = Sequential([\n","    Dense(128, input_shape=(X_train.shape[1],), activation='relu', kernel_regularizer=l1(0.01)),  # L1 regularization\n","    Dropout(0.5),  # Dropout layer with 50% dropout rate\n","    BatchNormalization(),  # Batch normalization layer\n","    Dense(128, activation='LeakyReLU', kernel_regularizer=l2(0.01)),  # L2 regularization\n","    Dropout(0.5),  # Dropout layer with 50% dropout rate\n","    BatchNormalization(),  # Batch normalization layer\n","    Dense(64, activation='LeakyReLU', kernel_regularizer=l2(0.01)),  # L2 regularization\n","    Dropout(0.5),  # Dropout layer with 50% dropout rate\n","    BatchNormalization(),  # Batch normalization layer\n","    Dense(1, activation='ThresholdedReLU')\n","])\n","\n","# Compile the model5 with a custom learning rate\n","optimizer = Adam(learning_rate=0.1)  # Experiment with different learning rates\n","model5.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n","model5.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":87152,"status":"ok","timestamp":1719457055076,"user":{"displayName":"Shabana Yasmin","userId":"18302913856572837226"},"user_tz":-330},"id":"PA_MEovVrlKq","outputId":"33b15426-e547-4133-dc9c-27d5f63b6fe9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","170/170 [==============================] - 4s 9ms/step - loss: 2982637469696.0000 - mae: 223021.0156 - val_loss: 128560652288.0000 - val_mae: 167557.8438\n","Epoch 2/100\n","170/170 [==============================] - 1s 7ms/step - loss: 2957242007552.0000 - mae: 193844.7969 - val_loss: 81299013632.0000 - val_mae: 118851.1406\n","Epoch 3/100\n","170/170 [==============================] - 1s 7ms/step - loss: 2922173169664.0000 - mae: 160934.2031 - val_loss: 83207536640.0000 - val_mae: 145778.0938\n","Epoch 4/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2894816346112.0000 - mae: 148768.1562 - val_loss: 40951029760.0000 - val_mae: 111275.8359\n","Epoch 5/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2872010342400.0000 - mae: 145488.9531 - val_loss: 48211533824.0000 - val_mae: 82599.4844\n","Epoch 6/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2868069793792.0000 - mae: 162092.5781 - val_loss: 33798014976.0000 - val_mae: 83685.6094\n","Epoch 7/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2860349915136.0000 - mae: 157639.4531 - val_loss: 37101662208.0000 - val_mae: 85061.5703\n","Epoch 8/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2861399277568.0000 - mae: 161614.2656 - val_loss: 48097247232.0000 - val_mae: 152198.8750\n","Epoch 9/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2845192486912.0000 - mae: 161220.5625 - val_loss: 29594028032.0000 - val_mae: 64458.7031\n","Epoch 10/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2860664750080.0000 - mae: 166795.4219 - val_loss: 34994491392.0000 - val_mae: 94640.0703\n","Epoch 11/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2847033524224.0000 - mae: 166107.4531 - val_loss: 40210345984.0000 - val_mae: 107556.9766\n","Epoch 12/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2854101712896.0000 - mae: 165099.2500 - val_loss: 32089577472.0000 - val_mae: 74919.8906\n","Epoch 13/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2845391716352.0000 - mae: 163948.2812 - val_loss: 33259812864.0000 - val_mae: 87159.1953\n","Epoch 14/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2825466150912.0000 - mae: 157765.4375 - val_loss: 27989020672.0000 - val_mae: 75652.0469\n","Epoch 15/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2841906774016.0000 - mae: 168698.4219 - val_loss: 42182471680.0000 - val_mae: 96488.5938\n","Epoch 16/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2830699855872.0000 - mae: 158389.8125 - val_loss: 41770131456.0000 - val_mae: 99450.0078\n","Epoch 17/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2832586506240.0000 - mae: 168040.1875 - val_loss: 63145897984.0000 - val_mae: 168487.4531\n","Epoch 18/100\n","170/170 [==============================] - 1s 6ms/step - loss: 2860206260224.0000 - mae: 154317.6875 - val_loss: 37797695488.0000 - val_mae: 103130.1328\n","Epoch 19/100\n","170/170 [==============================] - 1s 7ms/step - loss: 2832808017920.0000 - mae: 160785.5469 - val_loss: 48258826240.0000 - val_mae: 79686.6172\n","Epoch 20/100\n","170/170 [==============================] - 1s 7ms/step - loss: 2828774146048.0000 - mae: 158679.3438 - val_loss: 42827264000.0000 - val_mae: 92920.9453\n","Epoch 21/100\n","170/170 [==============================] - 1s 6ms/step - loss: 2842730692608.0000 - mae: 158407.1250 - val_loss: 30067075072.0000 - val_mae: 60305.8828\n","Epoch 22/100\n","170/170 [==============================] - 1s 6ms/step - loss: 2851355754496.0000 - mae: 153279.0312 - val_loss: 37367033856.0000 - val_mae: 68999.8203\n","Epoch 23/100\n","170/170 [==============================] - 1s 6ms/step - loss: 2835028639744.0000 - mae: 151898.7188 - val_loss: 77446856704.0000 - val_mae: 151557.6250\n","Epoch 24/100\n","170/170 [==============================] - 1s 7ms/step - loss: 2815372296192.0000 - mae: 159267.7812 - val_loss: 45560516608.0000 - val_mae: 74531.4844\n","Epoch 25/100\n","170/170 [==============================] - 1s 7ms/step - loss: 2821530058752.0000 - mae: 154482.6562 - val_loss: 68213817344.0000 - val_mae: 139925.6250\n","Epoch 26/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2839407230976.0000 - mae: 160200.9375 - val_loss: 82185609216.0000 - val_mae: 132554.4688\n","Epoch 27/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2828174622720.0000 - mae: 162226.5938 - val_loss: 43240120320.0000 - val_mae: 71123.9141\n","Epoch 28/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2853227200512.0000 - mae: 161529.0000 - val_loss: 35925041152.0000 - val_mae: 76293.9844\n","Epoch 29/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2853966708736.0000 - mae: 166209.9219 - val_loss: 32501893120.0000 - val_mae: 70698.7969\n","Epoch 30/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2820551213056.0000 - mae: 159678.6406 - val_loss: 42884390912.0000 - val_mae: 76163.2344\n","Epoch 31/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2831285485568.0000 - mae: 156684.6719 - val_loss: 40602091520.0000 - val_mae: 81171.7109\n","Epoch 32/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2828879003648.0000 - mae: 157777.4531 - val_loss: 48115830784.0000 - val_mae: 82391.7344\n","Epoch 33/100\n","170/170 [==============================] - 1s 6ms/step - loss: 2835968163840.0000 - mae: 166241.4844 - val_loss: 40862396416.0000 - val_mae: 83298.2031\n","Epoch 34/100\n","170/170 [==============================] - 1s 6ms/step - loss: 2817674706944.0000 - mae: 160419.1875 - val_loss: 34836131840.0000 - val_mae: 77477.0703\n","Epoch 35/100\n","170/170 [==============================] - 1s 7ms/step - loss: 2797022478336.0000 - mae: 165292.3750 - val_loss: 48452947968.0000 - val_mae: 120154.9141\n","Epoch 36/100\n","170/170 [==============================] - 1s 7ms/step - loss: 2810154319872.0000 - mae: 162509.8906 - val_loss: 65174319104.0000 - val_mae: 93609.7422\n","Epoch 37/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2819945922560.0000 - mae: 171935.5469 - val_loss: 49677488128.0000 - val_mae: 78216.9609\n","Epoch 38/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2820525785088.0000 - mae: 158673.2031 - val_loss: 47897214976.0000 - val_mae: 87560.9297\n","Epoch 39/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2824020688896.0000 - mae: 164261.9688 - val_loss: 39498412032.0000 - val_mae: 84792.1953\n","Epoch 40/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2798144454656.0000 - mae: 157196.8594 - val_loss: 74322763776.0000 - val_mae: 168955.5781\n","Epoch 41/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2806883287040.0000 - mae: 166699.3906 - val_loss: 41423609856.0000 - val_mae: 85257.0000\n","Epoch 42/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2822266683392.0000 - mae: 169028.0312 - val_loss: 47396237312.0000 - val_mae: 83681.1953\n","Epoch 43/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2790367952896.0000 - mae: 164027.2500 - val_loss: 44055322624.0000 - val_mae: 77258.0938\n","Epoch 44/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2804851146752.0000 - mae: 163629.5000 - val_loss: 45138624512.0000 - val_mae: 88235.0469\n","Epoch 45/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2805379629056.0000 - mae: 171654.8438 - val_loss: 77167124480.0000 - val_mae: 82315.8281\n","Epoch 46/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2793680666624.0000 - mae: 167618.2656 - val_loss: 114576408576.0000 - val_mae: 98977.8984\n","Epoch 47/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2836534657024.0000 - mae: 173565.5625 - val_loss: 108243279872.0000 - val_mae: 99892.2031\n","Epoch 48/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2817971191808.0000 - mae: 170888.3281 - val_loss: 31634690048.0000 - val_mae: 86889.2656\n","Epoch 49/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2841088884736.0000 - mae: 167287.5781 - val_loss: 39236857856.0000 - val_mae: 75603.4062\n","Epoch 50/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2794911694848.0000 - mae: 167307.6562 - val_loss: 45443219456.0000 - val_mae: 89343.5469\n","Epoch 51/100\n","170/170 [==============================] - 1s 7ms/step - loss: 2804995325952.0000 - mae: 160680.2656 - val_loss: 32419745792.0000 - val_mae: 81716.1094\n","Epoch 52/100\n","170/170 [==============================] - 1s 7ms/step - loss: 2824172470272.0000 - mae: 169465.6250 - val_loss: 33558310912.0000 - val_mae: 85536.6094\n","Epoch 53/100\n","170/170 [==============================] - 1s 7ms/step - loss: 2808655380480.0000 - mae: 156527.5625 - val_loss: 27175421952.0000 - val_mae: 77396.7734\n","Epoch 54/100\n","170/170 [==============================] - 1s 7ms/step - loss: 2809195659264.0000 - mae: 159559.9219 - val_loss: 36878864384.0000 - val_mae: 79663.0938\n","Epoch 55/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2798635712512.0000 - mae: 152553.0312 - val_loss: 103088144384.0000 - val_mae: 143721.5000\n","Epoch 56/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2780022439936.0000 - mae: 169965.5312 - val_loss: 78266228736.0000 - val_mae: 120021.2422\n","Epoch 57/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2776074813440.0000 - mae: 165868.3750 - val_loss: 166759694336.0000 - val_mae: 214957.5625\n","Epoch 58/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2783956434944.0000 - mae: 169728.3594 - val_loss: 63629811712.0000 - val_mae: 95883.0078\n","Epoch 59/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2788668997632.0000 - mae: 183016.9375 - val_loss: 64786337792.0000 - val_mae: 104429.5547\n","Epoch 60/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2796401197056.0000 - mae: 168898.2031 - val_loss: 57465749504.0000 - val_mae: 91520.1797\n","Epoch 61/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2802097324032.0000 - mae: 169546.4062 - val_loss: 43647852544.0000 - val_mae: 98410.1406\n","Epoch 62/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2792669315072.0000 - mae: 177843.2031 - val_loss: 47805206528.0000 - val_mae: 106956.9531\n","Epoch 63/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2784531316736.0000 - mae: 176511.9062 - val_loss: 31985772544.0000 - val_mae: 81848.7812\n","Epoch 64/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2813690904576.0000 - mae: 170020.8438 - val_loss: 46182199296.0000 - val_mae: 94010.5703\n","Epoch 65/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2767642165248.0000 - mae: 169786.9375 - val_loss: 41923403776.0000 - val_mae: 88602.0625\n","Epoch 66/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2771779846144.0000 - mae: 173828.4375 - val_loss: 40508469248.0000 - val_mae: 96783.8125\n","Epoch 67/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2801846452224.0000 - mae: 169122.4844 - val_loss: 36258914304.0000 - val_mae: 84161.5312\n","Epoch 68/100\n","170/170 [==============================] - 1s 6ms/step - loss: 2768239067136.0000 - mae: 160955.6250 - val_loss: 186407976960.0000 - val_mae: 210095.4062\n","Epoch 69/100\n","170/170 [==============================] - 1s 7ms/step - loss: 2767641903104.0000 - mae: 184013.5312 - val_loss: 44759392256.0000 - val_mae: 102155.0234\n","Epoch 70/100\n","170/170 [==============================] - 1s 7ms/step - loss: 2833686986752.0000 - mae: 172499.7812 - val_loss: 38556971008.0000 - val_mae: 106601.6875\n","Epoch 71/100\n","170/170 [==============================] - 1s 7ms/step - loss: 2790041321472.0000 - mae: 164109.5781 - val_loss: 60460040192.0000 - val_mae: 99497.0000\n","Epoch 72/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2808247484416.0000 - mae: 178752.1875 - val_loss: 51117965312.0000 - val_mae: 91818.1250\n","Epoch 73/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2792083423232.0000 - mae: 168052.4375 - val_loss: 37887549440.0000 - val_mae: 85319.7891\n","Epoch 74/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2778861928448.0000 - mae: 169003.0469 - val_loss: 40228544512.0000 - val_mae: 87194.6016\n","Epoch 75/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2776197234688.0000 - mae: 165585.6875 - val_loss: 45076819968.0000 - val_mae: 90273.5078\n","Epoch 76/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2738767265792.0000 - mae: 170215.9531 - val_loss: 40835928064.0000 - val_mae: 86826.3047\n","Epoch 77/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2770041569280.0000 - mae: 171694.8438 - val_loss: 37914427392.0000 - val_mae: 89627.4062\n","Epoch 78/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2768161210368.0000 - mae: 173026.6406 - val_loss: 36715073536.0000 - val_mae: 84252.1016\n","Epoch 79/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2759641530368.0000 - mae: 174941.6719 - val_loss: 35200532480.0000 - val_mae: 83962.3047\n","Epoch 80/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2790320242688.0000 - mae: 176858.6875 - val_loss: 70084395008.0000 - val_mae: 128923.8125\n","Epoch 81/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2775082074112.0000 - mae: 190373.3125 - val_loss: 46634106880.0000 - val_mae: 98209.9844\n","Epoch 82/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2789577850880.0000 - mae: 168877.2969 - val_loss: 45476446208.0000 - val_mae: 90231.8906\n","Epoch 83/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2810171359232.0000 - mae: 182305.9062 - val_loss: 43306364928.0000 - val_mae: 96850.2734\n","Epoch 84/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2781364092928.0000 - mae: 174889.8281 - val_loss: 62358994944.0000 - val_mae: 116159.7422\n","Epoch 85/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2762365206528.0000 - mae: 179864.3281 - val_loss: 43750985728.0000 - val_mae: 105572.3047\n","Epoch 86/100\n","170/170 [==============================] - 1s 6ms/step - loss: 2747841642496.0000 - mae: 174113.6562 - val_loss: 110839218176.0000 - val_mae: 181873.9531\n","Epoch 87/100\n","170/170 [==============================] - 1s 7ms/step - loss: 2759706804224.0000 - mae: 187315.9219 - val_loss: 35614269440.0000 - val_mae: 84473.1719\n","Epoch 88/100\n","170/170 [==============================] - 1s 7ms/step - loss: 2720233684992.0000 - mae: 173153.5156 - val_loss: 42853101568.0000 - val_mae: 98642.8594\n","Epoch 89/100\n","170/170 [==============================] - 1s 6ms/step - loss: 2784785596416.0000 - mae: 169821.2656 - val_loss: 36151898112.0000 - val_mae: 95697.8672\n","Epoch 90/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2761951543296.0000 - mae: 176223.7969 - val_loss: 41484357632.0000 - val_mae: 91685.4219\n","Epoch 91/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2786718384128.0000 - mae: 186051.0625 - val_loss: 54297632768.0000 - val_mae: 105571.9531\n","Epoch 92/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2772715700224.0000 - mae: 184375.1875 - val_loss: 33527949312.0000 - val_mae: 90988.5391\n","Epoch 93/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2758375899136.0000 - mae: 179661.7812 - val_loss: 30810146816.0000 - val_mae: 83389.6484\n","Epoch 94/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2771924811776.0000 - mae: 175778.2031 - val_loss: 39874953216.0000 - val_mae: 91042.5547\n","Epoch 95/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2767576629248.0000 - mae: 180588.0938 - val_loss: 37254459392.0000 - val_mae: 92265.8828\n","Epoch 96/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2772373602304.0000 - mae: 185877.3125 - val_loss: 43057864704.0000 - val_mae: 101679.7188\n","Epoch 97/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2758704889856.0000 - mae: 177471.0469 - val_loss: 37359464448.0000 - val_mae: 95936.2266\n","Epoch 98/100\n","170/170 [==============================] - 1s 5ms/step - loss: 2773174714368.0000 - mae: 176572.4531 - val_loss: 36837171200.0000 - val_mae: 101584.3438\n","Epoch 99/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2761444032512.0000 - mae: 170087.3906 - val_loss: 45511106560.0000 - val_mae: 99407.7656\n","Epoch 100/100\n","170/170 [==============================] - 1s 4ms/step - loss: 2788237770752.0000 - mae: 180873.5781 - val_loss: 41652920320.0000 - val_mae: 87630.9141\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x7a9ba617d540>"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["model5.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":866,"status":"ok","timestamp":1719457055934,"user":{"displayName":"Shabana Yasmin","userId":"18302913856572837226"},"user_tz":-330},"id":"Ptt3fz_IvC1m","outputId":"e962d529-e883-445e-c2f3-88d90b6fa8ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["53/53 [==============================] - 0s 2ms/step\n","MAE: 92710.16955768263\n","Mean Squared Error: 35767145529.36627\n","RMSE: 189122.0387193578\n","R-squared: 0.753730400259171\n"]}],"source":["# Predict\n","y_pred5 = model5.predict(X_test)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, y_pred5)\n","mse = mean_squared_error(y_test, y_pred5)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred5)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)"]},{"cell_type":"markdown","metadata":{"id":"orb5TeGt_mM8"},"source":["BPNN"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1055677,"status":"ok","timestamp":1719509606547,"user":{"displayName":"Shabana Yasmin","userId":"02459626693949289733"},"user_tz":-330},"id":"ItxgFEdcqiyj","outputId":"46b28446-c723-4898-a94a-d515d5b2b15c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 3 folds for each of 50 candidates, totalling 150 fits\n","Best Hyperparameters: {'solver': 'adam', 'learning_rate': 'constant', 'hidden_layer_sizes': (100, 100, 50), 'alpha': 0.01, 'activation': 'relu'}\n","MAE: 83038.192489451\n","Mean Squared Error: 28841263393.005005\n","RMSE: 169827.15740718562\n","R-squared: 0.8014175778723092\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","# Scale the features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","# Define the MLPRegressor\n","model = MLPRegressor(max_iter=1000, early_stopping=True, n_iter_no_change=50)\n","\n","# Define the parameter grid\n","param_dist = {\n","    'hidden_layer_sizes': [(50,), (100,), (100, 50), (100, 100, 50)],\n","    'activation': ['relu', 'tanh'],\n","    'solver': ['adam', 'sgd'],\n","    'alpha': [0.0001, 0.001, 0.01],\n","    'learning_rate': ['constant', 'adaptive']\n","}\n","\n","# Use RandomizedSearchCV for hyperparameter tuning\n","random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=50, cv=3, scoring='neg_mean_absolute_error', verbose=2, random_state=42, n_jobs=-1)\n","random_search_result = random_search.fit(X_train, y_train)\n","\n","# Print the best hyperparameters\n","print(f\"Best Hyperparameters: {random_search_result.best_params_}\")\n","\n","# Evaluate the best model on the test set\n","best_model = random_search_result.best_estimator_\n","y_pred = best_model.predict(X_test)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)"]},{"cell_type":"markdown","metadata":{"id":"4jrLlT-HDvJ_"},"source":["Radial Basis Neural Networks (RBFNN)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13165,"status":"ok","timestamp":1719457140435,"user":{"displayName":"Shabana Yasmin","userId":"18302913856572837226"},"user_tz":-330},"id":"_wmKOnoO_Y0b","outputId":"e2c1cb9c-3ddf-4c6e-9c32-66143afaf4f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["MAE: 180186.20895521005\n","Mean Squared Error: 157829887479.49667\n","RMSE: 397278.0984140665\n","R-squared: -0.08671527015771896\n"]}],"source":["from sklearn.neural_network import MLPRegressor\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","scaler = StandardScaler()\n","x = scaler.fit_transform(X)\n","X=pd.DataFrame(x)\n","\n","# Assuming X and y are your datasets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create a pipeline with a scaler and MLPRegressor with RBF activation\n","model = make_pipeline(StandardScaler(), MLPRegressor(hidden_layer_sizes=(10,), activation='relu', solver='adam', max_iter=1000))\n","\n","# Train the model\n","model.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7842,"status":"ok","timestamp":1719509614386,"user":{"displayName":"Shabana Yasmin","userId":"02459626693949289733"},"user_tz":-330},"id":"1sgmZcQRtU0B","outputId":"b2f92a2a-cd3d-42f5-bf9e-fcf31da3cc90"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 3 folds for each of 50 candidates, totalling 150 fits\n","Best Hyperparameters: {'rbfsampler__gamma': 0.1055221171236024, 'ridge__alpha': 8.25461428454834}\n","MAE: 195414.9110495229\n","Mean Squared Error: 125397942299.48141\n","RMSE: 354115.71879751596\n","R-squared: 0.13659028135019258\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.kernel_approximation import RBFSampler\n","from sklearn.linear_model import Ridge\n","from sklearn.pipeline import make_pipeline\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from scipy.stats import uniform\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","# Scale the features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","# Define the RBFSampler and linear model pipeline\n","model = make_pipeline(RBFSampler(), Ridge())\n","\n","# Define the parameter grid\n","param_dist = {\n","    'rbfsampler__gamma': uniform(0.1, 1),\n","    'ridge__alpha': uniform(0.1, 10)\n","}\n","\n","# Use RandomizedSearchCV for hyperparameter tuning\n","random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=50, cv=3, scoring='neg_mean_absolute_error', verbose=2, random_state=42, n_jobs=-1)\n","random_search_result = random_search.fit(X_train, y_train)\n","\n","# Print the best hyperparameters\n","print(f\"Best Hyperparameters: {random_search_result.best_params_}\")\n","\n","# Evaluate the best model on the test set\n","best_model = random_search_result.best_estimator_\n","y_pred = best_model.predict(X_test)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KSG2Uyr6_Yx0"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ERHKaQf2KRWT"},"source":["Multilayer Perceptron Neural Network (MLPNN)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15182,"status":"ok","timestamp":1719457155612,"user":{"displayName":"Shabana Yasmin","userId":"18302913856572837226"},"user_tz":-330},"id":"Zp4-nuKSHgho","outputId":"7c03a06b-129a-4722-f559-5de21ae40ef3"},"outputs":[{"name":"stdout","output_type":"stream","text":["MAE: 89367.97210969104\n","Mean Squared Error: 34266223944.533813\n","RMSE: 185111.38253639027\n","R-squared: 0.7640647826223255\n"]}],"source":["from sklearn.neural_network import MLPRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","scaler = StandardScaler()\n","x = scaler.fit_transform(X)\n","X=pd.DataFrame(x)\n","\n","# Assuming X and y are your datasets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define and train the MLPNN model\n","mlp = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', random_state=42)\n","mlp.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = mlp.predict(X_test)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83337,"status":"ok","timestamp":1719457238908,"user":{"displayName":"Shabana Yasmin","userId":"18302913856572837226"},"user_tz":-330},"id":"wv-u9_UyIjgP","outputId":"33f547b6-750a-4185-8037-b76decc2b5a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","170/170 [==============================] - 2s 5ms/step - loss: 861537107968.0000 - val_loss: 8672604848128.0000\n","Epoch 2/100\n","170/170 [==============================] - 1s 5ms/step - loss: 860866871296.0000 - val_loss: 8670846910464.0000\n","Epoch 3/100\n","170/170 [==============================] - 1s 4ms/step - loss: 858241302528.0000 - val_loss: 8666052820992.0000\n","Epoch 4/100\n","170/170 [==============================] - 1s 5ms/step - loss: 852473151488.0000 - val_loss: 8656803332096.0000\n","Epoch 5/100\n","170/170 [==============================] - 1s 5ms/step - loss: 842891001856.0000 - val_loss: 8643092676608.0000\n","Epoch 6/100\n","170/170 [==============================] - 1s 5ms/step - loss: 829835313152.0000 - val_loss: 8624940253184.0000\n","Epoch 7/100\n","170/170 [==============================] - 1s 5ms/step - loss: 814522105856.0000 - val_loss: 8604663939072.0000\n","Epoch 8/100\n","170/170 [==============================] - 1s 4ms/step - loss: 798279335936.0000 - val_loss: 8584148025344.0000\n","Epoch 9/100\n","170/170 [==============================] - 1s 4ms/step - loss: 783208087552.0000 - val_loss: 8565404729344.0000\n","Epoch 10/100\n","170/170 [==============================] - 0s 2ms/step - loss: 770716270592.0000 - val_loss: 8550096044032.0000\n","Epoch 11/100\n","170/170 [==============================] - 0s 3ms/step - loss: 761282166784.0000 - val_loss: 8538681769984.0000\n","Epoch 12/100\n","170/170 [==============================] - 0s 2ms/step - loss: 754874712064.0000 - val_loss: 8531004096512.0000\n","Epoch 13/100\n","170/170 [==============================] - 0s 3ms/step - loss: 750710816768.0000 - val_loss: 8525402603520.0000\n","Epoch 14/100\n","170/170 [==============================] - 0s 2ms/step - loss: 747954503680.0000 - val_loss: 8522204971008.0000\n","Epoch 15/100\n","170/170 [==============================] - 0s 2ms/step - loss: 746022895616.0000 - val_loss: 8519877132288.0000\n","Epoch 16/100\n","170/170 [==============================] - 0s 2ms/step - loss: 744488304640.0000 - val_loss: 8517959286784.0000\n","Epoch 17/100\n","170/170 [==============================] - 0s 2ms/step - loss: 743225425920.0000 - val_loss: 8516603478016.0000\n","Epoch 18/100\n","170/170 [==============================] - 0s 3ms/step - loss: 742067142656.0000 - val_loss: 8515298525184.0000\n","Epoch 19/100\n","170/170 [==============================] - 0s 2ms/step - loss: 741021646848.0000 - val_loss: 8514295037952.0000\n","Epoch 20/100\n","170/170 [==============================] - 0s 2ms/step - loss: 740047257600.0000 - val_loss: 8513253277696.0000\n","Epoch 21/100\n","170/170 [==============================] - 0s 3ms/step - loss: 739125755904.0000 - val_loss: 8512243499008.0000\n","Epoch 22/100\n","170/170 [==============================] - 0s 2ms/step - loss: 738258124800.0000 - val_loss: 8511259410432.0000\n","Epoch 23/100\n","170/170 [==============================] - 0s 2ms/step - loss: 737437614080.0000 - val_loss: 8509996924928.0000\n","Epoch 24/100\n","170/170 [==============================] - 0s 2ms/step - loss: 736634994688.0000 - val_loss: 8508841918464.0000\n","Epoch 25/100\n","170/170 [==============================] - 0s 2ms/step - loss: 735897255936.0000 - val_loss: 8507915501568.0000\n","Epoch 26/100\n","170/170 [==============================] - 0s 2ms/step - loss: 735188221952.0000 - val_loss: 8506926170112.0000\n","Epoch 27/100\n","170/170 [==============================] - 0s 2ms/step - loss: 734488494080.0000 - val_loss: 8505796853760.0000\n","Epoch 28/100\n","170/170 [==============================] - 0s 2ms/step - loss: 733835034624.0000 - val_loss: 8504686936064.0000\n","Epoch 29/100\n","170/170 [==============================] - 0s 2ms/step - loss: 733217619968.0000 - val_loss: 8503430217728.0000\n","Epoch 30/100\n","170/170 [==============================] - 0s 3ms/step - loss: 732625305600.0000 - val_loss: 8502445080576.0000\n","Epoch 31/100\n","170/170 [==============================] - 0s 2ms/step - loss: 732083519488.0000 - val_loss: 8501276442624.0000\n","Epoch 32/100\n","170/170 [==============================] - 0s 3ms/step - loss: 731567751168.0000 - val_loss: 8500215283712.0000\n","Epoch 33/100\n","170/170 [==============================] - 1s 3ms/step - loss: 731083636736.0000 - val_loss: 8499003129856.0000\n","Epoch 34/100\n","170/170 [==============================] - 1s 5ms/step - loss: 730585563136.0000 - val_loss: 8497718624256.0000\n","Epoch 35/100\n","170/170 [==============================] - 1s 4ms/step - loss: 730128384000.0000 - val_loss: 8496516431872.0000\n","Epoch 36/100\n","170/170 [==============================] - 1s 4ms/step - loss: 729708363776.0000 - val_loss: 8495423291392.0000\n","Epoch 37/100\n","170/170 [==============================] - 1s 5ms/step - loss: 729285787648.0000 - val_loss: 8494271954944.0000\n","Epoch 38/100\n","170/170 [==============================] - 1s 4ms/step - loss: 728901943296.0000 - val_loss: 8493007372288.0000\n","Epoch 39/100\n","170/170 [==============================] - 1s 3ms/step - loss: 728539463680.0000 - val_loss: 8492079382528.0000\n","Epoch 40/100\n","170/170 [==============================] - 0s 2ms/step - loss: 728193171456.0000 - val_loss: 8490741923840.0000\n","Epoch 41/100\n","170/170 [==============================] - 0s 2ms/step - loss: 727844847616.0000 - val_loss: 8489559654400.0000\n","Epoch 42/100\n","170/170 [==============================] - 0s 2ms/step - loss: 727534731264.0000 - val_loss: 8488523661312.0000\n","Epoch 43/100\n","170/170 [==============================] - 0s 3ms/step - loss: 727202332672.0000 - val_loss: 8487334576128.0000\n","Epoch 44/100\n","170/170 [==============================] - 0s 2ms/step - loss: 726920658944.0000 - val_loss: 8486291767296.0000\n","Epoch 45/100\n","170/170 [==============================] - 0s 2ms/step - loss: 726647439360.0000 - val_loss: 8485429313536.0000\n","Epoch 46/100\n","170/170 [==============================] - 0s 2ms/step - loss: 726395781120.0000 - val_loss: 8484229742592.0000\n","Epoch 47/100\n","170/170 [==============================] - 0s 2ms/step - loss: 726141108224.0000 - val_loss: 8483345793024.0000\n","Epoch 48/100\n","170/170 [==============================] - 0s 2ms/step - loss: 725896003584.0000 - val_loss: 8482221719552.0000\n","Epoch 49/100\n","170/170 [==============================] - 0s 2ms/step - loss: 725674360832.0000 - val_loss: 8481308409856.0000\n","Epoch 50/100\n","170/170 [==============================] - 0s 2ms/step - loss: 725472116736.0000 - val_loss: 8480327467008.0000\n","Epoch 51/100\n","170/170 [==============================] - 0s 3ms/step - loss: 725261746176.0000 - val_loss: 8479297241088.0000\n","Epoch 52/100\n","170/170 [==============================] - 0s 2ms/step - loss: 725055832064.0000 - val_loss: 8478424825856.0000\n","Epoch 53/100\n","170/170 [==============================] - 0s 2ms/step - loss: 724860796928.0000 - val_loss: 8477593305088.0000\n","Epoch 54/100\n","170/170 [==============================] - 0s 2ms/step - loss: 724685881344.0000 - val_loss: 8476778561536.0000\n","Epoch 55/100\n","170/170 [==============================] - 0s 3ms/step - loss: 724542357504.0000 - val_loss: 8476130541568.0000\n","Epoch 56/100\n","170/170 [==============================] - 0s 2ms/step - loss: 724341424128.0000 - val_loss: 8474923106304.0000\n","Epoch 57/100\n","170/170 [==============================] - 0s 3ms/step - loss: 724192788480.0000 - val_loss: 8474183335936.0000\n","Epoch 58/100\n","170/170 [==============================] - 0s 2ms/step - loss: 724051165184.0000 - val_loss: 8473484984320.0000\n","Epoch 59/100\n","170/170 [==============================] - 0s 3ms/step - loss: 723902332928.0000 - val_loss: 8472723193856.0000\n","Epoch 60/100\n","170/170 [==============================] - 0s 3ms/step - loss: 723773161472.0000 - val_loss: 8472221450240.0000\n","Epoch 61/100\n","170/170 [==============================] - 0s 3ms/step - loss: 723637895168.0000 - val_loss: 8471377870848.0000\n","Epoch 62/100\n","170/170 [==============================] - 0s 2ms/step - loss: 723524452352.0000 - val_loss: 8470785425408.0000\n","Epoch 63/100\n","170/170 [==============================] - 1s 4ms/step - loss: 723404849152.0000 - val_loss: 8470069248000.0000\n","Epoch 64/100\n","170/170 [==============================] - 1s 4ms/step - loss: 723280461824.0000 - val_loss: 8469464743936.0000\n","Epoch 65/100\n","170/170 [==============================] - 1s 4ms/step - loss: 723178356736.0000 - val_loss: 8468805189632.0000\n","Epoch 66/100\n","170/170 [==============================] - 1s 4ms/step - loss: 723070877696.0000 - val_loss: 8468513161216.0000\n","Epoch 67/100\n","170/170 [==============================] - 1s 4ms/step - loss: 722970673152.0000 - val_loss: 8468044447744.0000\n","Epoch 68/100\n","170/170 [==============================] - 1s 4ms/step - loss: 722872041472.0000 - val_loss: 8467653853184.0000\n","Epoch 69/100\n","170/170 [==============================] - 1s 3ms/step - loss: 722782322688.0000 - val_loss: 8467116457984.0000\n","Epoch 70/100\n","170/170 [==============================] - 0s 2ms/step - loss: 722694569984.0000 - val_loss: 8466958123008.0000\n","Epoch 71/100\n","170/170 [==============================] - 0s 3ms/step - loss: 722600132608.0000 - val_loss: 8466202624000.0000\n","Epoch 72/100\n","170/170 [==============================] - 0s 3ms/step - loss: 722533810176.0000 - val_loss: 8465622237184.0000\n","Epoch 73/100\n","170/170 [==============================] - 0s 2ms/step - loss: 722435899392.0000 - val_loss: 8465418289152.0000\n","Epoch 74/100\n","170/170 [==============================] - 0s 3ms/step - loss: 722359025664.0000 - val_loss: 8465063870464.0000\n","Epoch 75/100\n","170/170 [==============================] - 0s 2ms/step - loss: 722289557504.0000 - val_loss: 8464636051456.0000\n","Epoch 76/100\n","170/170 [==============================] - 0s 2ms/step - loss: 722233196544.0000 - val_loss: 8464270622720.0000\n","Epoch 77/100\n","170/170 [==============================] - 0s 3ms/step - loss: 722139480064.0000 - val_loss: 8464025780224.0000\n","Epoch 78/100\n","170/170 [==============================] - 0s 2ms/step - loss: 722082267136.0000 - val_loss: 8463829172224.0000\n","Epoch 79/100\n","170/170 [==============================] - 0s 3ms/step - loss: 722008014848.0000 - val_loss: 8463630467072.0000\n","Epoch 80/100\n","170/170 [==============================] - 0s 2ms/step - loss: 721944903680.0000 - val_loss: 8463375663104.0000\n","Epoch 81/100\n","170/170 [==============================] - 0s 3ms/step - loss: 721888542720.0000 - val_loss: 8463550775296.0000\n","Epoch 82/100\n","170/170 [==============================] - 0s 2ms/step - loss: 721822416896.0000 - val_loss: 8463089926144.0000\n","Epoch 83/100\n","170/170 [==============================] - 0s 3ms/step - loss: 721765728256.0000 - val_loss: 8462801567744.0000\n","Epoch 84/100\n","170/170 [==============================] - 0s 2ms/step - loss: 721696391168.0000 - val_loss: 8462490140672.0000\n","Epoch 85/100\n","170/170 [==============================] - 0s 2ms/step - loss: 721650384896.0000 - val_loss: 8462153023488.0000\n","Epoch 86/100\n","170/170 [==============================] - 0s 3ms/step - loss: 721594744832.0000 - val_loss: 8462344388608.0000\n","Epoch 87/100\n","170/170 [==============================] - 0s 2ms/step - loss: 721532813312.0000 - val_loss: 8462070710272.0000\n","Epoch 88/100\n","170/170 [==============================] - 0s 3ms/step - loss: 721478942720.0000 - val_loss: 8462011465728.0000\n","Epoch 89/100\n","170/170 [==============================] - 0s 2ms/step - loss: 721425661952.0000 - val_loss: 8461915521024.0000\n","Epoch 90/100\n","170/170 [==============================] - 0s 2ms/step - loss: 721370873856.0000 - val_loss: 8461851033600.0000\n","Epoch 91/100\n","170/170 [==============================] - 0s 2ms/step - loss: 721330438144.0000 - val_loss: 8461740408832.0000\n","Epoch 92/100\n","170/170 [==============================] - 0s 2ms/step - loss: 721293082624.0000 - val_loss: 8461608288256.0000\n","Epoch 93/100\n","170/170 [==============================] - 1s 4ms/step - loss: 721226170368.0000 - val_loss: 8461490847744.0000\n","Epoch 94/100\n","170/170 [==============================] - 1s 4ms/step - loss: 721174265856.0000 - val_loss: 8461227130880.0000\n","Epoch 95/100\n","170/170 [==============================] - 1s 4ms/step - loss: 721140449280.0000 - val_loss: 8461373931520.0000\n","Epoch 96/100\n","170/170 [==============================] - 1s 4ms/step - loss: 721095491584.0000 - val_loss: 8461332512768.0000\n","Epoch 97/100\n","170/170 [==============================] - 1s 4ms/step - loss: 721048961024.0000 - val_loss: 8461334085632.0000\n","Epoch 98/100\n","170/170 [==============================] - 1s 4ms/step - loss: 721004986368.0000 - val_loss: 8461434748928.0000\n","Epoch 99/100\n","170/170 [==============================] - 0s 3ms/step - loss: 720960880640.0000 - val_loss: 8461454671872.0000\n","Epoch 100/100\n","170/170 [==============================] - 0s 3ms/step - loss: 720917757952.0000 - val_loss: 8461697417216.0000\n","53/53 [==============================] - 0s 1ms/step\n","MAE: 56131.38422753394\n","Mean Squared Error: 22017649720.4084\n","RMSE: 148383.4550090016\n","R-squared: 0.8484006005056556\n"]}],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","scaler = StandardScaler()\n","x = scaler.fit_transform(X)\n","X=pd.DataFrame(x)\n","\n","# Assuming X and y are your datasets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Build the MLP model\n","model = Sequential([\n","    Dense(128, activation='relu', input_dim=X_train.shape[1]),\n","    Dense(64, activation='relu'),\n","    Dense(1)\n","])\n","\n","model.compile(optimizer='adam', loss='mse')\n","\n","# Train the model\n","model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n","\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l2o1JUxKJmEx"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"4rxWIjdTKCw1"},"source":["Convolutional Neural Networks (CNN)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36684,"status":"ok","timestamp":1719457275587,"user":{"displayName":"Shabana Yasmin","userId":"18302913856572837226"},"user_tz":-330},"id":"PalhZfzJJlrZ","outputId":"6db1edb5-c04a-4f68-dde9-a82a26c06b6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","212/212 [==============================] - 2s 3ms/step - loss: 2422535618560.0000\n","Epoch 2/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2398682873856.0000\n","Epoch 3/50\n","212/212 [==============================] - 1s 4ms/step - loss: 2353439703040.0000\n","Epoch 4/50\n","212/212 [==============================] - 1s 4ms/step - loss: 2339388522496.0000\n","Epoch 5/50\n","212/212 [==============================] - 1s 5ms/step - loss: 2335675514880.0000\n","Epoch 6/50\n","212/212 [==============================] - 1s 5ms/step - loss: 2333160243200.0000\n","Epoch 7/50\n","212/212 [==============================] - 1s 4ms/step - loss: 2330928349184.0000\n","Epoch 8/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2329098321920.0000\n","Epoch 9/50\n","212/212 [==============================] - 1s 2ms/step - loss: 2327164747776.0000\n","Epoch 10/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2325531066368.0000\n","Epoch 11/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2324079837184.0000\n","Epoch 12/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2322640142336.0000\n","Epoch 13/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2321190748160.0000\n","Epoch 14/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2319703867392.0000\n","Epoch 15/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2318490664960.0000\n","Epoch 16/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2317210091520.0000\n","Epoch 17/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2315874205696.0000\n","Epoch 18/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2314414325760.0000\n","Epoch 19/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2313071624192.0000\n","Epoch 20/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2311741767680.0000\n","Epoch 21/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2310256721920.0000\n","Epoch 22/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2308888330240.0000\n","Epoch 23/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2307300524032.0000\n","Epoch 24/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2305920073728.0000\n","Epoch 25/50\n","212/212 [==============================] - 1s 5ms/step - loss: 2304007733248.0000\n","Epoch 26/50\n","212/212 [==============================] - 1s 4ms/step - loss: 2302198677504.0000\n","Epoch 27/50\n","212/212 [==============================] - 1s 5ms/step - loss: 2300463808512.0000\n","Epoch 28/50\n","212/212 [==============================] - 1s 5ms/step - loss: 2298442940416.0000\n","Epoch 29/50\n","212/212 [==============================] - 1s 4ms/step - loss: 2296408440832.0000\n","Epoch 30/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2294480109568.0000\n","Epoch 31/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2292485193728.0000\n","Epoch 32/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2290581241856.0000\n","Epoch 33/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2288703504384.0000\n","Epoch 34/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2287199059968.0000\n","Epoch 35/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2285812842496.0000\n","Epoch 36/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2284738052096.0000\n","Epoch 37/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2283795382272.0000\n","Epoch 38/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2283124031488.0000\n","Epoch 39/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2282582441984.0000\n","Epoch 40/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2282022240256.0000\n","Epoch 41/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2281732571136.0000\n","Epoch 42/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2281243672576.0000\n","Epoch 43/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2280990179328.0000\n","Epoch 44/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2280685568000.0000\n","Epoch 45/50\n","212/212 [==============================] - 1s 3ms/step - loss: 2280433909760.0000\n","Epoch 46/50\n","212/212 [==============================] - 1s 4ms/step - loss: 2280180678656.0000\n","Epoch 47/50\n","212/212 [==============================] - 1s 5ms/step - loss: 2279965720576.0000\n","Epoch 48/50\n","212/212 [==============================] - 1s 4ms/step - loss: 2279814987776.0000\n","Epoch 49/50\n","212/212 [==============================] - 1s 5ms/step - loss: 2279575650304.0000\n","Epoch 50/50\n","212/212 [==============================] - 1s 5ms/step - loss: 2279392673792.0000\n","53/53 [==============================] - 0s 2ms/step\n","MAE: 103523.57926190831\n","Mean Squared Error: 36893891733.124\n","RMSE: 192077.8272813497\n","R-squared: 0.7459723493299715\n"]}],"source":["from tensorflow.keras.layers import Conv1D, Flatten, Dense\n","from tensorflow.keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import pandas as pd\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Assuming X and y are your datasets\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","# Example with 1D Convolutional layers for sequence data\n","model = Sequential([\n","    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n","    Conv1D(16, kernel_size=3, activation='relu'),\n","    Flatten(),\n","    Dense(64, activation='relu'),\n","    Dense(1)\n","])\n","\n","model.compile(optimizer='adam', loss='mse')\n","\n","# Note: Ensure X_train and X_test are reshaped for 1D convolutions\n","X_train = X_train.reshape(-1, X_train.shape[1], 1)\n","X_test = X_test.reshape(-1, X_test.shape[1], 1)\n","\n","model.fit(X_train, y_train, epochs=50, batch_size=32)\n","\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)"]},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"jScbmedHJl-g"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zIyjNkYfJlce","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720053292865,"user_tz":-330,"elapsed":58021,"user":{"displayName":"Shabana Yasmin","userId":"18302913856572837226"}},"outputId":"49684b97-00fa-42fe-e748-6280dc6fa175"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 39ms/step - loss: 646933315584.0000 - val_loss: 8672623722496.0000\n","Epoch 2/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1155562340352.0000 - val_loss: 8671101714432.0000\n","Epoch 3/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 359062274048.0000 - val_loss: 8663127293952.0000\n","Epoch 4/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 547647913984.0000 - val_loss: 8641339457536.0000\n","Epoch 5/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 545814446080.0000 - val_loss: 8618458480640.0000\n","Epoch 6/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 216448761856.0000 - val_loss: 8581448990720.0000\n","Epoch 7/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 846315847680.0000 - val_loss: 8561966448640.0000\n","Epoch 8/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 560371204096.0000 - val_loss: 8539006304256.0000\n","Epoch 9/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 431734259712.0000 - val_loss: 8523519361024.0000\n","Epoch 10/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 597839577088.0000 - val_loss: 8514338029568.0000\n","Epoch 11/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1876490584064.0000 - val_loss: 8510774444032.0000\n","Epoch 12/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2446450491392.0000 - val_loss: 8501999435776.0000\n","Epoch 13/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2436172611584.0000 - val_loss: 8502160392192.0000\n","Epoch 14/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 774089080832.0000 - val_loss: 8490212917248.0000\n","Epoch 15/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 489765502976.0000 - val_loss: 8490274783232.0000\n","Epoch 16/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 248673992704.0000 - val_loss: 8497972903936.0000\n","Epoch 17/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89677103104.0000 - val_loss: 8489157525504.0000\n","Epoch 18/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 431400222720.0000 - val_loss: 8482614935552.0000\n","Epoch 19/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 549883215872.0000 - val_loss: 8481928642560.0000\n","Epoch 20/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1753321046016.0000 - val_loss: 8471348510720.0000\n","Epoch 21/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 248956928000.0000 - val_loss: 8473707806720.0000\n","Epoch 22/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 125046947840.0000 - val_loss: 8473447235584.0000\n","Epoch 23/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 971146788864.0000 - val_loss: 8467504431104.0000\n","Epoch 24/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 199552794624.0000 - val_loss: 8461680115712.0000\n","Epoch 25/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 687191031808.0000 - val_loss: 8471476961280.0000\n","Epoch 26/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 254488690688.0000 - val_loss: 8466515099648.0000\n","Epoch 27/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 608572669952.0000 - val_loss: 8464716791808.0000\n","Epoch 28/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 251729182720.0000 - val_loss: 8466197905408.0000\n","Epoch 29/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3320313806848.0000 - val_loss: 8460763136000.0000\n","Epoch 30/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 427441422336.0000 - val_loss: 8457462218752.0000\n","Epoch 31/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 319544164352.0000 - val_loss: 8456274182144.0000\n","Epoch 32/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 329517301760.0000 - val_loss: 8458408558592.0000\n","Epoch 33/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1435532656640.0000 - val_loss: 8456850898944.0000\n","Epoch 34/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 79314329600.0000 - val_loss: 8468908998656.0000\n","Epoch 35/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 514139455488.0000 - val_loss: 8457011331072.0000\n","Epoch 36/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 420760518656.0000 - val_loss: 8455557480448.0000\n","Epoch 37/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 390981386240.0000 - val_loss: 8459144134656.0000\n","Epoch 38/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 245825781760.0000 - val_loss: 8452007002112.0000\n","Epoch 39/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 224116785152.0000 - val_loss: 8459446648832.0000\n","Epoch 40/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 869675302912.0000 - val_loss: 8456599764992.0000\n","Epoch 41/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 106791575552.0000 - val_loss: 8458610933760.0000\n","Epoch 42/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 724128628736.0000 - val_loss: 8456225423360.0000\n","Epoch 43/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 479787810816.0000 - val_loss: 8455260209152.0000\n","Epoch 44/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 289304182784.0000 - val_loss: 8455560101888.0000\n","Epoch 45/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 131544539136.0000 - val_loss: 8456959426560.0000\n","Epoch 46/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 115784409088.0000 - val_loss: 8446093557760.0000\n","Epoch 47/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 361103720448.0000 - val_loss: 8454118309888.0000\n","Epoch 48/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 744291893248.0000 - val_loss: 8451471179776.0000\n","Epoch 49/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1020566634496.0000 - val_loss: 8448838205440.0000\n","Epoch 50/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 259974856704.0000 - val_loss: 8452889378816.0000\n","Epoch 51/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 239976546304.0000 - val_loss: 8447648595968.0000\n","Epoch 52/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2282266820608.0000 - val_loss: 8451066953728.0000\n","Epoch 53/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 104547360768.0000 - val_loss: 8458261757952.0000\n","Epoch 54/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1267267010560.0000 - val_loss: 8450464022528.0000\n","Epoch 55/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 961206484992.0000 - val_loss: 8449251344384.0000\n","Epoch 56/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 228613734400.0000 - val_loss: 8438403825664.0000\n","Epoch 57/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 66707681280.0000 - val_loss: 8446181638144.0000\n","Epoch 58/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 768336134144.0000 - val_loss: 8444849422336.0000\n","Epoch 59/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 285808656384.0000 - val_loss: 8445602824192.0000\n","Epoch 60/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 95680348160.0000 - val_loss: 8440540299264.0000\n","Epoch 61/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 116696924160.0000 - val_loss: 8451377332224.0000\n","Epoch 62/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 606358994944.0000 - val_loss: 8448031850496.0000\n","Epoch 63/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1037918470144.0000 - val_loss: 8444005318656.0000\n","Epoch 64/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 548778082304.0000 - val_loss: 8447030984704.0000\n","Epoch 65/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 418638102528.0000 - val_loss: 8451336437760.0000\n","Epoch 66/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 492715999232.0000 - val_loss: 8447255904256.0000\n","\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n","MAE: 65218.92062024367\n","Mean Squared Error: 15686289935.715689\n","RMSE: 125244.91980002898\n","R-squared: 0.8919942789195864\n"]}],"source":["from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout, BatchNormalization\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","import pandas as pd\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import numpy as np\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Assuming X and y are your datasets\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","# Reshape data for Conv1D\n","X_train = X_train.reshape(-1, X_train.shape[1], 1)\n","X_test = X_test.reshape(-1, X_test.shape[1], 1)\n","\n","# Model with additional layers and dropout\n","model = Sequential([\n","    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n","    BatchNormalization(),\n","    Dropout(0.2),\n","    Conv1D(32, kernel_size=3, activation='relu'),\n","    BatchNormalization(),\n","    Dropout(0.2),\n","    Flatten(),\n","    Dense(128, activation='relu'),\n","    Dropout(0.3),\n","    Dense(1)\n","])\n","\n","# Compile model with Adam optimizer and learning rate schedule\n","optimizer = Adam(learning_rate=0.001)\n","model.compile(optimizer=optimizer, loss='mse')\n","\n","# Early stopping callback\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","\n","# Train the model with early stopping\n","history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n","\n","# Predict and evaluate the model\n","y_pred = model.predict(X_test).flatten()\n","\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Print results\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)\n"]},{"cell_type":"markdown","metadata":{"id":"OfSidaigLXoY"},"source":["Back-propagation Neural Network (BPNN)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51389,"status":"ok","timestamp":1719457411629,"user":{"displayName":"Shabana Yasmin","userId":"18302913856572837226"},"user_tz":-330},"id":"f907ltxjJvB8","outputId":"59912d20-1907-4916-d498-5a7511a10436"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","212/212 [==============================] - 2s 3ms/step - loss: 2424234311680.0000\n","Epoch 2/100\n","212/212 [==============================] - 1s 4ms/step - loss: 2423390994432.0000\n","Epoch 3/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2420536246272.0000\n","Epoch 4/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2415031222272.0000\n","Epoch 5/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2406655983616.0000\n","Epoch 6/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2395452211200.0000\n","Epoch 7/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2382117994496.0000\n","Epoch 8/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2367070142464.0000\n","Epoch 9/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2352255860736.0000\n","Epoch 10/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2338932654080.0000\n","Epoch 11/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2327617732608.0000\n","Epoch 12/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2318075166720.0000\n","Epoch 13/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2310708133888.0000\n","Epoch 14/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2305508769792.0000\n","Epoch 15/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2301654728704.0000\n","Epoch 16/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2298772455424.0000\n","Epoch 17/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2296611864576.0000\n","Epoch 18/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2295009902592.0000\n","Epoch 19/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2293638103040.0000\n","Epoch 20/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2292460290048.0000\n","Epoch 21/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2291400966144.0000\n","Epoch 22/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2290429984768.0000\n","Epoch 23/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2289530830848.0000\n","Epoch 24/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2288618569728.0000\n","Epoch 25/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2287810117632.0000\n","Epoch 26/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2286984626176.0000\n","Epoch 27/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2286236729344.0000\n","Epoch 28/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2285499318272.0000\n","Epoch 29/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2284731498496.0000\n","Epoch 30/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2283986747392.0000\n","Epoch 31/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2283278434304.0000\n","Epoch 32/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2282565664768.0000\n","Epoch 33/100\n","212/212 [==============================] - 1s 4ms/step - loss: 2281871769600.0000\n","Epoch 34/100\n","212/212 [==============================] - 1s 4ms/step - loss: 2281161883648.0000\n","Epoch 35/100\n","212/212 [==============================] - 1s 4ms/step - loss: 2280495775744.0000\n","Epoch 36/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2279895465984.0000\n","Epoch 37/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2279233028096.0000\n","Epoch 38/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2278612533248.0000\n","Epoch 39/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2278036078592.0000\n","Epoch 40/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2277394612224.0000\n","Epoch 41/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2276779360256.0000\n","Epoch 42/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2276196089856.0000\n","Epoch 43/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2275593945088.0000\n","Epoch 44/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2275052617728.0000\n","Epoch 45/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2274486124544.0000\n","Epoch 46/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2273937981440.0000\n","Epoch 47/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2273392459776.0000\n","Epoch 48/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2272889667584.0000\n","Epoch 49/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2272422002688.0000\n","Epoch 50/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2271933366272.0000\n","Epoch 51/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2271451545600.0000\n","Epoch 52/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2270951112704.0000\n","Epoch 53/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2270509662208.0000\n","Epoch 54/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2270032822272.0000\n","Epoch 55/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2269604741120.0000\n","Epoch 56/100\n","212/212 [==============================] - 1s 2ms/step - loss: 2269222535168.0000\n","Epoch 57/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2268774793216.0000\n","Epoch 58/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2268375547904.0000\n","Epoch 59/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2267999895552.0000\n","Epoch 60/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2267546910720.0000\n","Epoch 61/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2267210579968.0000\n","Epoch 62/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2266808451072.0000\n","Epoch 63/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2266492567552.0000\n","Epoch 64/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2266164363264.0000\n","Epoch 65/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2265836683264.0000\n","Epoch 66/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2265483575296.0000\n","Epoch 67/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2265140690944.0000\n","Epoch 68/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2264906072064.0000\n","Epoch 69/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2264547721216.0000\n","Epoch 70/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2264288198656.0000\n","Epoch 71/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2263986733056.0000\n","Epoch 72/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2263750803456.0000\n","Epoch 73/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2263453007872.0000\n","Epoch 74/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2263203708928.0000\n","Epoch 75/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2262966468608.0000\n","Epoch 76/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2262716645376.0000\n","Epoch 77/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2262493036544.0000\n","Epoch 78/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2262263922688.0000\n","Epoch 79/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2262063644672.0000\n","Epoch 80/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2261873328128.0000\n","Epoch 81/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2261631893504.0000\n","Epoch 82/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2261439741952.0000\n","Epoch 83/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2261201453056.0000\n","Epoch 84/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2261031845888.0000\n","Epoch 85/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2260872462336.0000\n","Epoch 86/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2260677951488.0000\n","Epoch 87/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2260555268096.0000\n","Epoch 88/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2260318814208.0000\n","Epoch 89/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2260209762304.0000\n","Epoch 90/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2260002406400.0000\n","Epoch 91/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2259863732224.0000\n","Epoch 92/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2259737640960.0000\n","Epoch 93/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2259565150208.0000\n","Epoch 94/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2259451379712.0000\n","Epoch 95/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2259261325312.0000\n","Epoch 96/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2259169050624.0000\n","Epoch 97/100\n","212/212 [==============================] - 1s 4ms/step - loss: 2259048726528.0000\n","Epoch 98/100\n","212/212 [==============================] - 1s 3ms/step - loss: 2258889867264.0000\n","Epoch 99/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2258764300288.0000\n","Epoch 100/100\n","212/212 [==============================] - 0s 2ms/step - loss: 2258679889920.0000\n","53/53 [==============================] - 0s 2ms/step\n","MAE: 85033.91371331707\n","Mean Squared Error: 29709248176.823895\n","RMSE: 172363.70899010004\n","R-squared: 0.7954411919424702\n"]}],"source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.optimizers import Adam\n","from sklearn.metrics import mean_squared_error\n","\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","scaler = StandardScaler()\n","x = scaler.fit_transform(X)\n","X=pd.DataFrame(x)\n","\n","# Assuming X and y are your datasets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Build the BPNN model\n","model = Sequential()\n","model.add(Dense(100, input_dim=X_train.shape[1], activation='relu'))\n","model.add(Dense(50, activation='relu'))\n","model.add(Dense(1))\n","\n","# Compile the model\n","model.compile(loss='mean_squared_error', optimizer=Adam())\n","\n","# Train the model\n","model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n","\n","# Make predictions\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJXPdKN0LbGd"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"bVvoSohygmQq"},"source":["Functional Link Artificial Neural Network (FLANN)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83105,"status":"ok","timestamp":1719457494728,"user":{"displayName":"Shabana Yasmin","userId":"18302913856572837226"},"user_tz":-330},"id":"h9y-Vk20UKFM","outputId":"f62b3683-17e0-44f7-e8c0-5970c0f12fe6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","678/678 [==============================] - 2s 2ms/step - loss: 2424237981696.0000\n","Epoch 2/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2423601233920.0000\n","Epoch 3/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2421616803840.0000\n","Epoch 4/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2418032771072.0000\n","Epoch 5/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2412843106304.0000\n","Epoch 6/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2405907038208.0000\n","Epoch 7/50\n","678/678 [==============================] - 2s 3ms/step - loss: 2397908762624.0000\n","Epoch 8/50\n","678/678 [==============================] - 2s 3ms/step - loss: 2389095481344.0000\n","Epoch 9/50\n","678/678 [==============================] - 2s 2ms/step - loss: 2379040948224.0000\n","Epoch 10/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2368357531648.0000\n","Epoch 11/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2358150692864.0000\n","Epoch 12/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2348368789504.0000\n","Epoch 13/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2339093086208.0000\n","Epoch 14/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2330595164160.0000\n","Epoch 15/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2323137167360.0000\n","Epoch 16/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2316863012864.0000\n","Epoch 17/50\n","678/678 [==============================] - 2s 2ms/step - loss: 2311549878272.0000\n","Epoch 18/50\n","678/678 [==============================] - 2s 3ms/step - loss: 2306896035840.0000\n","Epoch 19/50\n","678/678 [==============================] - 2s 3ms/step - loss: 2302894145536.0000\n","Epoch 20/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2299499118592.0000\n","Epoch 21/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2296833638400.0000\n","Epoch 22/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2294682484736.0000\n","Epoch 23/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2292587167744.0000\n","Epoch 24/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2290665914368.0000\n","Epoch 25/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2289044029440.0000\n","Epoch 26/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2287573139456.0000\n","Epoch 27/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2286166736896.0000\n","Epoch 28/50\n","678/678 [==============================] - 2s 3ms/step - loss: 2284792315904.0000\n","Epoch 29/50\n","678/678 [==============================] - 2s 3ms/step - loss: 2283450925056.0000\n","Epoch 30/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2282166681600.0000\n","Epoch 31/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2280974974976.0000\n","Epoch 32/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2279817347072.0000\n","Epoch 33/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2278618562560.0000\n","Epoch 34/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2277534597120.0000\n","Epoch 35/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2276493623296.0000\n","Epoch 36/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2275401793536.0000\n","Epoch 37/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2274397519872.0000\n","Epoch 38/50\n","678/678 [==============================] - 2s 3ms/step - loss: 2273348681728.0000\n","Epoch 39/50\n","678/678 [==============================] - 2s 3ms/step - loss: 2272363282432.0000\n","Epoch 40/50\n","678/678 [==============================] - 2s 3ms/step - loss: 2271364251648.0000\n","Epoch 41/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2270346346496.0000\n","Epoch 42/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2269411278848.0000\n","Epoch 43/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2268519727104.0000\n","Epoch 44/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2267594096640.0000\n","Epoch 45/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2266734526464.0000\n","Epoch 46/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2265861324800.0000\n","Epoch 47/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2265011453952.0000\n","Epoch 48/50\n","678/678 [==============================] - 1s 2ms/step - loss: 2264143495168.0000\n","Epoch 49/50\n","678/678 [==============================] - 2s 3ms/step - loss: 2263368859648.0000\n","Epoch 50/50\n","678/678 [==============================] - 2s 3ms/step - loss: 2262562504704.0000\n","53/53 [==============================] - 0s 2ms/step\n","MAE: 2865054387176.8467\n","Mean Squared Error: 1.2240659372085131e+26\n","RMSE: 11063751340338.924\n","R-squared: -842813212939288.0\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from sklearn.metrics import mean_squared_error\n","\n","# Define function to create augmented features\n","def create_augmented_features(X):\n","    # Assuming X is a NumPy array\n","    X_poly = np.power(X, 2)  # Add squared terms\n","    X_sin = np.sin(X)        # Add sine transformed terms\n","    return np.concatenate([X, X_poly, X_sin], axis=1)\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","# Augment features\n","X_augmented = create_augmented_features(X)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_augmented, y, test_size=0.2, random_state=42)\n","\n","# Scale the features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Create the model\n","model = Sequential()\n","model.add(Dense(20, input_dim=X_train_scaled.shape[1], activation='relu'))\n","model.add(Dense(10, activation='relu'))\n","model.add(Dense(1))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","# Train the model\n","model.fit(X_train_scaled, y_train, epochs=50, batch_size=10)\n","\n","# Make predictions\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ackIETTEUKC5"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"1IhdZyoX5NWp"},"source":["Wavelet Neural Network (WNN)"]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Dropout\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.utils import get_custom_objects\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","# Define the custom Morlet wavelet function\n","def morlet_wavelet(x):\n","    return tf.multiply(tf.cos(1.75 * x), tf.exp(-0.5 * tf.square(x)))\n","\n","# Register the custom activation function with Keras\n","get_custom_objects().update({'morlet_wavelet': morlet_wavelet})\n","\n","# Define the wavelet neural network model\n","def build_wavelet_model(input_shape, neurons=64, dropout_rate=0.2, learning_rate=0.001):\n","    inputs = Input(shape=(input_shape,))\n","    x = Dense(neurons, activation=morlet_wavelet)(inputs)\n","    x = Dropout(dropout_rate)(x)\n","    x = Dense(neurons, activation=morlet_wavelet)(x)\n","    x = Dropout(dropout_rate)(x)\n","    outputs = Dense(1)(x)\n","    model = Model(inputs, outputs)\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n","    return model\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","# Convert Series to numpy array if necessary\n","if isinstance(y, pd.Series):\n","    y = y.to_numpy()\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Scale inputs\n","scaler_x = StandardScaler()\n","X_train_scaled = scaler_x.fit_transform(X_train)\n","X_test_scaled = scaler_x.transform(X_test)\n","\n","# If y needs scaling:\n","scaler_y = StandardScaler()\n","y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))\n","y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))\n","\n","# Build and train the model\n","model = build_wavelet_model(input_shape=X_train_scaled.shape[1], neurons=128, dropout_rate=0.3, learning_rate=0.001)\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","model.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n","\n","# Make predictions\n","y_pred_scaled = model.predict(X_test_scaled)\n","\n","# Inverse transform the scaled predictions\n","y_pred = scaler_y.inverse_transform(y_pred_scaled)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UCB_W9OFNF7O","executionInfo":{"status":"ok","timestamp":1719534409788,"user_tz":-330,"elapsed":52162,"user":{"displayName":"Shabana Yasmin","userId":"02459626693949289733"}},"outputId":"f963b7f8-40c5-4067-80aa-6bbe86264166"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - loss: 0.9294 - mae: 0.5500 - val_loss: 3.6386 - val_mae: 0.2358\n","Epoch 2/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3389 - mae: 0.2584 - val_loss: 3.6095 - val_mae: 0.1613\n","Epoch 3/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3185 - mae: 0.1776 - val_loss: 3.6025 - val_mae: 0.1458\n","Epoch 4/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4007 - mae: 0.1456 - val_loss: 3.6060 - val_mae: 0.1618\n","Epoch 5/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0938 - mae: 0.1134 - val_loss: 3.5976 - val_mae: 0.1459\n","Epoch 6/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0868 - mae: 0.0999 - val_loss: 3.5952 - val_mae: 0.1542\n","Epoch 7/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0763 - mae: 0.0952 - val_loss: 3.5954 - val_mae: 0.1357\n","Epoch 8/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0766 - mae: 0.0909 - val_loss: 3.6001 - val_mae: 0.1410\n","Epoch 9/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4275 - mae: 0.1005 - val_loss: 3.5949 - val_mae: 0.1436\n","Epoch 10/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.4028 - mae: 0.1264 - val_loss: 3.5960 - val_mae: 0.1421\n","Epoch 11/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1078 - mae: 0.0794 - val_loss: 3.6031 - val_mae: 0.1603\n","Epoch 12/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3243 - mae: 0.0868 - val_loss: 3.5938 - val_mae: 0.1396\n","Epoch 13/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1285 - mae: 0.0756 - val_loss: 3.5929 - val_mae: 0.1401\n","Epoch 14/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0737 - mae: 0.1039 - val_loss: 3.5914 - val_mae: 0.1328\n","Epoch 15/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2632 - mae: 0.0757 - val_loss: 3.5872 - val_mae: 0.1292\n","Epoch 16/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2798 - mae: 0.0794 - val_loss: 3.5886 - val_mae: 0.1316\n","Epoch 17/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6143 - mae: 0.0897 - val_loss: 3.5853 - val_mae: 0.1303\n","Epoch 18/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0693 - mae: 0.0671 - val_loss: 3.5851 - val_mae: 0.1175\n","Epoch 19/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1024 - mae: 0.0689 - val_loss: 3.5874 - val_mae: 0.1265\n","Epoch 20/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5024 - mae: 0.0863 - val_loss: 3.5845 - val_mae: 0.1283\n","Epoch 21/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0863 - mae: 0.0648 - val_loss: 3.5937 - val_mae: 0.1344\n","Epoch 22/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4939 - mae: 0.0853 - val_loss: 3.5788 - val_mae: 0.1262\n","Epoch 23/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1212 - mae: 0.0686 - val_loss: 3.5900 - val_mae: 0.1325\n","Epoch 24/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0297 - mae: 0.0667 - val_loss: 3.5751 - val_mae: 0.1133\n","Epoch 25/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8181 - mae: 0.0937 - val_loss: 3.5801 - val_mae: 0.1213\n","Epoch 26/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.2294 - mae: 0.0989 - val_loss: 3.5757 - val_mae: 0.1157\n","Epoch 27/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0552 - mae: 0.0565 - val_loss: 3.5664 - val_mae: 0.1154\n","Epoch 28/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2952 - mae: 0.0711 - val_loss: 3.5758 - val_mae: 0.1121\n","Epoch 29/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2471 - mae: 0.0674 - val_loss: 3.5780 - val_mae: 0.1182\n","Epoch 30/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1622 - mae: 0.0656 - val_loss: 3.5767 - val_mae: 0.1153\n","Epoch 31/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3383 - mae: 0.0711 - val_loss: 3.5747 - val_mae: 0.1160\n","Epoch 32/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2756 - mae: 0.0678 - val_loss: 3.5720 - val_mae: 0.1160\n","Epoch 33/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1504 - mae: 0.0636 - val_loss: 3.5774 - val_mae: 0.1148\n","Epoch 34/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0143 - mae: 0.0601 - val_loss: 3.5714 - val_mae: 0.1083\n","Epoch 35/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3029 - mae: 0.0734 - val_loss: 3.5660 - val_mae: 0.1076\n","Epoch 36/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0868 - mae: 0.0583 - val_loss: 3.5739 - val_mae: 0.1123\n","Epoch 37/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2022 - mae: 0.0648 - val_loss: 3.5688 - val_mae: 0.1120\n","Epoch 38/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3350 - mae: 0.0672 - val_loss: 3.5733 - val_mae: 0.1073\n","Epoch 39/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7462 - mae: 0.0803 - val_loss: 3.5694 - val_mae: 0.1078\n","Epoch 40/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0959 - mae: 0.0555 - val_loss: 3.5621 - val_mae: 0.0985\n","Epoch 41/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0153 - mae: 0.0547 - val_loss: 3.5611 - val_mae: 0.0974\n","Epoch 42/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0625 - mae: 0.0901 - val_loss: 3.5641 - val_mae: 0.0999\n","Epoch 43/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6088 - mae: 0.0718 - val_loss: 3.5618 - val_mae: 0.1002\n","Epoch 44/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4223 - mae: 0.0655 - val_loss: 3.5622 - val_mae: 0.1042\n","Epoch 45/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0645 - mae: 0.0555 - val_loss: 3.5621 - val_mae: 0.1099\n","Epoch 46/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1610 - mae: 0.0614 - val_loss: 3.5666 - val_mae: 0.1047\n","Epoch 47/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2175 - mae: 0.0622 - val_loss: 3.5690 - val_mae: 0.1075\n","Epoch 48/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4370 - mae: 0.0651 - val_loss: 3.5608 - val_mae: 0.1024\n","Epoch 49/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3035 - mae: 0.0617 - val_loss: 3.5644 - val_mae: 0.1018\n","Epoch 50/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1743 - mae: 0.0554 - val_loss: 3.5594 - val_mae: 0.1001\n","Epoch 51/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7431 - mae: 0.0741 - val_loss: 3.5669 - val_mae: 0.1018\n","Epoch 52/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2111 - mae: 0.0566 - val_loss: 3.5645 - val_mae: 0.1009\n","Epoch 53/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0096 - mae: 0.0484 - val_loss: 3.5625 - val_mae: 0.0978\n","Epoch 54/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0381 - mae: 0.0590 - val_loss: 3.5638 - val_mae: 0.1183\n","Epoch 55/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5750 - mae: 0.0779 - val_loss: 3.5640 - val_mae: 0.1017\n","Epoch 56/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0438 - mae: 0.0490 - val_loss: 3.5640 - val_mae: 0.1093\n","Epoch 57/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3428 - mae: 0.0638 - val_loss: 3.5651 - val_mae: 0.1005\n","Epoch 58/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8544 - mae: 0.0744 - val_loss: 3.5624 - val_mae: 0.0980\n","Epoch 59/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4019 - mae: 0.0601 - val_loss: 3.5615 - val_mae: 0.1024\n","Epoch 60/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2743 - mae: 0.0568 - val_loss: 3.5591 - val_mae: 0.0976\n","Epoch 61/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0274 - mae: 0.0492 - val_loss: 3.5579 - val_mae: 0.0996\n","Epoch 62/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0542 - mae: 0.0536 - val_loss: 3.5487 - val_mae: 0.1004\n","Epoch 63/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9258 - mae: 0.0742 - val_loss: 3.5600 - val_mae: 0.0991\n","Epoch 64/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0354 - mae: 0.0469 - val_loss: 3.5624 - val_mae: 0.1018\n","Epoch 65/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4107 - mae: 0.0646 - val_loss: 3.5623 - val_mae: 0.0982\n","Epoch 66/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2003 - mae: 0.0512 - val_loss: 3.5606 - val_mae: 0.0956\n","Epoch 67/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0331 - mae: 0.0462 - val_loss: 3.5643 - val_mae: 0.1006\n","Epoch 68/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0240 - mae: 0.0512 - val_loss: 3.5595 - val_mae: 0.1000\n","Epoch 69/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0261 - mae: 0.0480 - val_loss: 3.5627 - val_mae: 0.1053\n","Epoch 70/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2560 - mae: 0.0582 - val_loss: 3.5569 - val_mae: 0.0973\n","Epoch 71/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5197 - mae: 0.0628 - val_loss: 3.5596 - val_mae: 0.0990\n","Epoch 72/100\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0681 - mae: 0.0454 - val_loss: 3.5584 - val_mae: 0.0975\n","\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n","MAE: 63331.11060467864\n","Mean Squared Error: 15441544012.37622\n","RMSE: 124264.00932038295\n","R-squared: 0.8936794422080441\n"]}]},{"cell_type":"code","source":["from scikeras.wrappers import KerasRegressor\n","from sklearn.model_selection import RandomizedSearchCV\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import Input, Dense, Dropout\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","\n","# Define the wavelet activation function\n","def morlet_wavelet(x):\n","    return tf.multiply(tf.cos(1.75 * x), tf.exp(-0.5 * tf.square(x)))\n","\n","# Register the custom activation with Keras\n","from tensorflow.keras.utils import get_custom_objects\n","get_custom_objects().update({'morlet_wavelet': morlet_wavelet})\n","\n","def build_model(neurons=64, dropout_rate=0.2, learning_rate=0.001):\n","    inputs = Input(shape=(X_train_scaled.shape[1],))\n","    x = Dense(neurons, activation=morlet_wavelet)(inputs)\n","    x = Dropout(dropout_rate)(x)\n","    x = Dense(neurons, activation=morlet_wavelet)(x)\n","    x = Dropout(dropout_rate)(x)\n","    outputs = Dense(1)(x)\n","    model = Model(inputs, outputs)\n","    optimizer = Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='mse')\n","    return model\n","\n","# Create KerasRegressor\n","model = KerasRegressor(\n","    model=build_model,\n","    verbose=0,\n","    neurons=64,\n","    dropout_rate=0.2,\n","    learning_rate=0.001,\n","    epochs=100,\n","    batch_size=32\n",")\n","\n","# Define the parameter grid\n","param_distribs = {\n","    'model__neurons': [64, 128, 256],\n","    'model__dropout_rate': [0.2, 0.3, 0.5],\n","    'model__learning_rate': [0.001, 0.01, 0.1],\n","    'batch_size': [16, 32, 64],\n","    'epochs': [100, 200, 300]\n","}\n","\n","# RandomizedSearchCV\n","rnd_search = RandomizedSearchCV(estimator=model, param_distributions=param_distribs, n_iter=20, cv=3, scoring='neg_mean_squared_error', verbose=2)\n","rnd_search.fit(X_train_scaled, y_train_scaled)\n","\n","# Best parameters\n","print(\"Best Hyperparameters:\", rnd_search.best_params_)\n","\n","# Train the best model with early stopping\n","best_model = build_model(\n","    neurons=rnd_search.best_params_['model__neurons'],\n","    dropout_rate=rnd_search.best_params_['model__dropout_rate'],\n","    learning_rate=rnd_search.best_params_['model__learning_rate']\n",")\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","best_model.fit(\n","    X_train_scaled, y_train_scaled,\n","    epochs=rnd_search.best_params_['epochs'],\n","    batch_size=rnd_search.best_params_['batch_size'],\n","    validation_split=0.2,\n","    callbacks=[early_stopping],\n","    verbose=1\n",")\n","\n","# Predict on the test set\n","y_pred = best_model.predict(X_test_scaled)\n","\n","# Inverse transform predictions and actual values if they were scaled\n","y_pred = scaler_y.inverse_transform(y_pred)\n","y_test = scaler_y.inverse_transform(y_test_scaled)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Print results\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IScfMNCCQSC_","executionInfo":{"status":"ok","timestamp":1719538645325,"user_tz":-330,"elapsed":3648712,"user":{"displayName":"Shabana Yasmin","userId":"02459626693949289733"}},"outputId":"5d580ba3-62ac-4a30-c9d9-35a32592da82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 3 folds for each of 20 candidates, totalling 60 fits\n","[CV] END batch_size=32, epochs=200, model__dropout_rate=0.5, model__learning_rate=0.001, model__neurons=64; total time= 1.1min\n","[CV] END batch_size=32, epochs=200, model__dropout_rate=0.5, model__learning_rate=0.001, model__neurons=64; total time= 1.1min\n","[CV] END batch_size=32, epochs=200, model__dropout_rate=0.5, model__learning_rate=0.001, model__neurons=64; total time= 1.1min\n","[CV] END batch_size=16, epochs=100, model__dropout_rate=0.5, model__learning_rate=0.1, model__neurons=256; total time= 1.0min\n","[CV] END batch_size=16, epochs=100, model__dropout_rate=0.5, model__learning_rate=0.1, model__neurons=256; total time= 1.1min\n","[CV] END batch_size=16, epochs=100, model__dropout_rate=0.5, model__learning_rate=0.1, model__neurons=256; total time= 1.1min\n","[CV] END batch_size=64, epochs=100, model__dropout_rate=0.5, model__learning_rate=0.01, model__neurons=256; total time=  25.8s\n","[CV] END batch_size=64, epochs=100, model__dropout_rate=0.5, model__learning_rate=0.01, model__neurons=256; total time=  27.2s\n","[CV] END batch_size=64, epochs=100, model__dropout_rate=0.5, model__learning_rate=0.01, model__neurons=256; total time=  22.7s\n","[CV] END batch_size=32, epochs=100, model__dropout_rate=0.5, model__learning_rate=0.01, model__neurons=128; total time=  33.7s\n","[CV] END batch_size=32, epochs=100, model__dropout_rate=0.5, model__learning_rate=0.01, model__neurons=128; total time=  35.6s\n","[CV] END batch_size=32, epochs=100, model__dropout_rate=0.5, model__learning_rate=0.01, model__neurons=128; total time=  39.5s\n","[CV] END batch_size=16, epochs=200, model__dropout_rate=0.3, model__learning_rate=0.001, model__neurons=64; total time= 2.0min\n","[CV] END batch_size=16, epochs=200, model__dropout_rate=0.3, model__learning_rate=0.001, model__neurons=64; total time= 2.0min\n","[CV] END batch_size=16, epochs=200, model__dropout_rate=0.3, model__learning_rate=0.001, model__neurons=64; total time= 1.9min\n","[CV] END batch_size=64, epochs=100, model__dropout_rate=0.5, model__learning_rate=0.1, model__neurons=256; total time=  22.3s\n","[CV] END batch_size=64, epochs=100, model__dropout_rate=0.5, model__learning_rate=0.1, model__neurons=256; total time=  22.7s\n","[CV] END batch_size=64, epochs=100, model__dropout_rate=0.5, model__learning_rate=0.1, model__neurons=256; total time=  25.8s\n","[CV] END batch_size=16, epochs=100, model__dropout_rate=0.5, model__learning_rate=0.001, model__neurons=64; total time= 1.0min\n","[CV] END batch_size=16, epochs=100, model__dropout_rate=0.5, model__learning_rate=0.001, model__neurons=64; total time= 1.0min\n","[CV] END batch_size=16, epochs=100, model__dropout_rate=0.5, model__learning_rate=0.001, model__neurons=64; total time= 1.0min\n","[CV] END batch_size=64, epochs=100, model__dropout_rate=0.3, model__learning_rate=0.1, model__neurons=256; total time=  23.3s\n","[CV] END batch_size=64, epochs=100, model__dropout_rate=0.3, model__learning_rate=0.1, model__neurons=256; total time=  22.5s\n","[CV] END batch_size=64, epochs=100, model__dropout_rate=0.3, model__learning_rate=0.1, model__neurons=256; total time=  23.8s\n","[CV] END batch_size=16, epochs=200, model__dropout_rate=0.3, model__learning_rate=0.01, model__neurons=128; total time= 1.8min\n","[CV] END batch_size=16, epochs=200, model__dropout_rate=0.3, model__learning_rate=0.01, model__neurons=128; total time= 1.8min\n","[CV] END batch_size=16, epochs=200, model__dropout_rate=0.3, model__learning_rate=0.01, model__neurons=128; total time= 1.8min\n","[CV] END batch_size=64, epochs=300, model__dropout_rate=0.3, model__learning_rate=0.01, model__neurons=256; total time=  50.2s\n","[CV] END batch_size=64, epochs=300, model__dropout_rate=0.3, model__learning_rate=0.01, model__neurons=256; total time=  51.5s\n","[CV] END batch_size=64, epochs=300, model__dropout_rate=0.3, model__learning_rate=0.01, model__neurons=256; total time=  49.6s\n","[CV] END batch_size=32, epochs=100, model__dropout_rate=0.3, model__learning_rate=0.001, model__neurons=128; total time=  37.2s\n","[CV] END batch_size=32, epochs=100, model__dropout_rate=0.3, model__learning_rate=0.001, model__neurons=128; total time=  35.0s\n","[CV] END batch_size=32, epochs=100, model__dropout_rate=0.3, model__learning_rate=0.001, model__neurons=128; total time=  34.9s\n","[CV] END batch_size=32, epochs=300, model__dropout_rate=0.3, model__learning_rate=0.001, model__neurons=256; total time= 1.6min\n","[CV] END batch_size=32, epochs=300, model__dropout_rate=0.3, model__learning_rate=0.001, model__neurons=256; total time= 1.6min\n","[CV] END batch_size=32, epochs=300, model__dropout_rate=0.3, model__learning_rate=0.001, model__neurons=256; total time= 1.6min\n","[CV] END batch_size=64, epochs=100, model__dropout_rate=0.2, model__learning_rate=0.01, model__neurons=256; total time=  22.7s\n","[CV] END batch_size=64, epochs=100, model__dropout_rate=0.2, model__learning_rate=0.01, model__neurons=256; total time=  23.1s\n","[CV] END batch_size=64, epochs=100, model__dropout_rate=0.2, model__learning_rate=0.01, model__neurons=256; total time=  25.0s\n","[CV] END batch_size=64, epochs=300, model__dropout_rate=0.2, model__learning_rate=0.001, model__neurons=128; total time=  51.2s\n","[CV] END batch_size=64, epochs=300, model__dropout_rate=0.2, model__learning_rate=0.001, model__neurons=128; total time=  50.5s\n","[CV] END batch_size=64, epochs=300, model__dropout_rate=0.2, model__learning_rate=0.001, model__neurons=128; total time=  51.0s\n","[CV] END batch_size=16, epochs=200, model__dropout_rate=0.2, model__learning_rate=0.1, model__neurons=64; total time= 1.8min\n","[CV] END batch_size=16, epochs=200, model__dropout_rate=0.2, model__learning_rate=0.1, model__neurons=64; total time= 1.9min\n","[CV] END batch_size=16, epochs=200, model__dropout_rate=0.2, model__learning_rate=0.1, model__neurons=64; total time= 1.9min\n","[CV] END batch_size=16, epochs=100, model__dropout_rate=0.5, model__learning_rate=0.01, model__neurons=64; total time= 1.1min\n","[CV] END batch_size=16, epochs=100, model__dropout_rate=0.5, model__learning_rate=0.01, model__neurons=64; total time=  59.4s\n","[CV] END batch_size=16, epochs=100, model__dropout_rate=0.5, model__learning_rate=0.01, model__neurons=64; total time=  59.0s\n","[CV] END batch_size=64, epochs=200, model__dropout_rate=0.5, model__learning_rate=0.1, model__neurons=128; total time=  34.9s\n","[CV] END batch_size=64, epochs=200, model__dropout_rate=0.5, model__learning_rate=0.1, model__neurons=128; total time=  36.8s\n","[CV] END batch_size=64, epochs=200, model__dropout_rate=0.5, model__learning_rate=0.1, model__neurons=128; total time=  35.4s\n","[CV] END batch_size=64, epochs=300, model__dropout_rate=0.3, model__learning_rate=0.001, model__neurons=128; total time=  54.8s\n","[CV] END batch_size=64, epochs=300, model__dropout_rate=0.3, model__learning_rate=0.001, model__neurons=128; total time=  49.9s\n","[CV] END batch_size=64, epochs=300, model__dropout_rate=0.3, model__learning_rate=0.001, model__neurons=128; total time=  49.2s\n","[CV] END batch_size=32, epochs=300, model__dropout_rate=0.3, model__learning_rate=0.1, model__neurons=128; total time= 1.5min\n","[CV] END batch_size=32, epochs=300, model__dropout_rate=0.3, model__learning_rate=0.1, model__neurons=128; total time= 1.4min\n","[CV] END batch_size=32, epochs=300, model__dropout_rate=0.3, model__learning_rate=0.1, model__neurons=128; total time= 1.6min\n","[CV] END batch_size=64, epochs=200, model__dropout_rate=0.2, model__learning_rate=0.01, model__neurons=256; total time=  36.7s\n","[CV] END batch_size=64, epochs=200, model__dropout_rate=0.2, model__learning_rate=0.01, model__neurons=256; total time=  39.4s\n","[CV] END batch_size=64, epochs=200, model__dropout_rate=0.2, model__learning_rate=0.01, model__neurons=256; total time=  35.3s\n","Best Hyperparameters: {'model__neurons': 256, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.3, 'epochs': 300, 'batch_size': 32}\n","Epoch 1/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - loss: 0.4326 - val_loss: 3.6097\n","Epoch 2/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1414 - val_loss: 3.6024\n","Epoch 3/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3278 - val_loss: 3.6077\n","Epoch 4/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6466 - val_loss: 3.6017\n","Epoch 5/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4205 - val_loss: 3.6024\n","Epoch 6/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0754 - val_loss: 3.5924\n","Epoch 7/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0574 - val_loss: 3.5891\n","Epoch 8/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1155 - val_loss: 3.5910\n","Epoch 9/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2328 - val_loss: 3.5878\n","Epoch 10/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1848 - val_loss: 3.5910\n","Epoch 11/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2735 - val_loss: 3.5855\n","Epoch 12/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2509 - val_loss: 3.5873\n","Epoch 13/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0399 - val_loss: 3.5821\n","Epoch 14/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1574 - val_loss: 3.5822\n","Epoch 15/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0167 - val_loss: 3.5802\n","Epoch 16/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5438 - val_loss: 3.5785\n","Epoch 17/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1849 - val_loss: 3.5774\n","Epoch 18/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496 - val_loss: 3.5737\n","Epoch 19/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3127 - val_loss: 3.5759\n","Epoch 20/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2956 - val_loss: 3.5721\n","Epoch 21/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7783 - val_loss: 3.5745\n","Epoch 22/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1905 - val_loss: 3.5712\n","Epoch 23/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7561 - val_loss: 3.5725\n","Epoch 24/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4704 - val_loss: 3.5705\n","Epoch 25/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2450 - val_loss: 3.5674\n","Epoch 26/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3968 - val_loss: 3.5624\n","Epoch 27/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6984 - val_loss: 3.5684\n","Epoch 28/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5680 - val_loss: 3.5609\n","Epoch 29/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1063 - val_loss: 3.5618\n","Epoch 30/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0560 - val_loss: 3.5632\n","Epoch 31/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4960 - val_loss: 3.5608\n","Epoch 32/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4957 - val_loss: 3.5564\n","Epoch 33/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2261 - val_loss: 3.5602\n","Epoch 34/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0385 - val_loss: 3.5645\n","Epoch 35/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5690 - val_loss: 3.5576\n","Epoch 36/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0442 - val_loss: 3.5573\n","Epoch 37/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0617 - val_loss: 3.5660\n","Epoch 38/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2907 - val_loss: 3.5575\n","Epoch 39/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0716 - val_loss: 3.5574\n","Epoch 40/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1289 - val_loss: 3.5704\n","Epoch 41/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3232 - val_loss: 3.5608\n","Epoch 42/300\n","\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2493 - val_loss: 3.5610\n","\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n","MAE: 66161.603962902\n","Mean Squared Error: 13076332574.801191\n","RMSE: 114351.79305459619\n","R-squared: 0.9099647695779846\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iYnhUHyfWNsb"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"0yVjElUbVenL"},"source":["Generalized Regression Neural Networks (GRNN)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6243,"status":"ok","timestamp":1719558117478,"user":{"displayName":"Shabana Yasmin","userId":"13381702277896287900"},"user_tz":-330},"id":"qqYcAjyWVkI7","outputId":"e682b02a-b381-4e83-ddef-0c2bf4a6ec6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyGRNN\n","  Downloading pyGRNN-0.1.2-py3-none-any.whl (12 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pyGRNN) (2.0.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyGRNN) (1.25.2)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from pyGRNN) (0.13.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pyGRNN) (1.5.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from pyGRNN) (3.7.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyGRNN) (1.11.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyGRNN) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyGRNN) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyGRNN) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyGRNN) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyGRNN) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyGRNN) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyGRNN) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyGRNN) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pyGRNN) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pyGRNN) (2024.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyGRNN) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyGRNN) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->pyGRNN) (1.16.0)\n","Installing collected packages: pyGRNN\n","Successfully installed pyGRNN-0.1.2\n"]}],"source":["!pip install pyGRNN"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bh9U7mC4VxoB","outputId":"abf68a31-32a8-4bee-b7e4-c08b7dec4ab9","executionInfo":{"status":"ok","timestamp":1719563123040,"user_tz":-330,"elapsed":5005568,"user":{"displayName":"Shabana Yasmin","userId":"13381702277896287900"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Executing warm start...\n","Warm start concluded. The optimum isotropic sigma is [1.32948663]\n","Gradient search concluded. The optimum sigma is [3.63538044 1.80309565 4.67103031 1.65347581 6.27462607 5.68535867\n"," 4.37694962 0.05898241 8.49173485 0.19032693 0.21216142 2.59337011\n"," 4.16006863 1.66269481 0.09752468 1.39792372 1.329     ]\n","MAE: 51216.34727360432\n","Mean Squared Error: 23242833837.446236\n","RMSE: 152456.00623604908\n","R-squared: 0.8399647693078879\n"]}],"source":["import numpy as np\n","from pyGRNN import GRNN\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","scaler = StandardScaler()\n","x = scaler.fit_transform(X)\n","X=pd.DataFrame(x)\n","\n","# Assuming X and y are your datasets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create and train GRNN model\n","grnn = GRNN()\n","grnn.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = grnn.predict(X_test)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)"]},{"cell_type":"markdown","metadata":{"id":"LNkVR8PCryKc"},"source":["OLSRNN (Output Layer Self-Recurrent Neural Network)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDo1C0mKUJ9-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"908cbc60-0d70-4964-ca74-b9859598ff47"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Training loss (for one batch) at step 666: 620142133248.0000\n","Training loss (for one batch) at step 667: 35356934144.0000\n","Training loss (for one batch) at step 668: 27899721728.0000\n","Training loss (for one batch) at step 669: 20161576960.0000\n","Training loss (for one batch) at step 670: 1441821440.0000\n","Training loss (for one batch) at step 671: 26617556992.0000\n","Training loss (for one batch) at step 672: 164070948864.0000\n","Training loss (for one batch) at step 673: 4900618240.0000\n","Training loss (for one batch) at step 674: 10829395968.0000\n","Training loss (for one batch) at step 675: 54399176704.0000\n","Training loss (for one batch) at step 676: 70174892032.0000\n","Training loss (for one batch) at step 677: 10124705792.0000\n","Training loss (for one batch) at step 678: 26612633600.0000\n","Training loss (for one batch) at step 679: 14348090368.0000\n","Training loss (for one batch) at step 680: 2751237120.0000\n","Training loss (for one batch) at step 681: 4914039296.0000\n","Training loss (for one batch) at step 682: 8811465728.0000\n","Training loss (for one batch) at step 683: 3721777664.0000\n","Training loss (for one batch) at step 684: 110536507392.0000\n","Training loss (for one batch) at step 685: 5166583808.0000\n","Training loss (for one batch) at step 686: 185247137792.0000\n","Training loss (for one batch) at step 687: 195359670272.0000\n","Training loss (for one batch) at step 688: 70522978304.0000\n","Training loss (for one batch) at step 689: 180562427904.0000\n","Training loss (for one batch) at step 690: 13600325632.0000\n","Training loss (for one batch) at step 691: 22881867776.0000\n","Training loss (for one batch) at step 692: 10369728512.0000\n","Training loss (for one batch) at step 693: 80754515968.0000\n","Training loss (for one batch) at step 694: 1146711703552.0000\n","Training loss (for one batch) at step 695: 19336769536.0000\n","Training loss (for one batch) at step 696: 4488995840.0000\n","Training loss (for one batch) at step 697: 2471163854848.0000\n","Training loss (for one batch) at step 698: 170834460672.0000\n","Training loss (for one batch) at step 699: 10539667456.0000\n","Training loss (for one batch) at step 700: 932184588288.0000\n","Training loss (for one batch) at step 701: 2915165184.0000\n","Training loss (for one batch) at step 702: 5657397760.0000\n","Training loss (for one batch) at step 703: 315831517184.0000\n","Training loss (for one batch) at step 704: 30256967680.0000\n","Training loss (for one batch) at step 705: 4205575424.0000\n","Training loss (for one batch) at step 706: 203458691072.0000\n","Training loss (for one batch) at step 707: 48503001088.0000\n","Training loss (for one batch) at step 708: 20309774336.0000\n","Training loss (for one batch) at step 709: 25067786240.0000\n","Training loss (for one batch) at step 710: 38756638720.0000\n","Training loss (for one batch) at step 711: 67748347904.0000\n","Training loss (for one batch) at step 712: 240794009600.0000\n","Training loss (for one batch) at step 713: 118121881600.0000\n","Training loss (for one batch) at step 714: 11645152256.0000\n","Training loss (for one batch) at step 715: 223178178560.0000\n","Training loss (for one batch) at step 716: 144761176064.0000\n","Training loss (for one batch) at step 717: 104037998592.0000\n","Training loss (for one batch) at step 718: 465057087488.0000\n","Training loss (for one batch) at step 719: 72055455744.0000\n","Training loss (for one batch) at step 720: 11846506496.0000\n","Training loss (for one batch) at step 721: 8795985920.0000\n","Training loss (for one batch) at step 722: 28317208576.0000\n","Training loss (for one batch) at step 723: 108265234432.0000\n","Training loss (for one batch) at step 724: 12788770816.0000\n","Training loss (for one batch) at step 725: 4900617728.0000\n","Training loss (for one batch) at step 726: 7030463488.0000\n","Training loss (for one batch) at step 727: 55147298816.0000\n","Training loss (for one batch) at step 728: 6587170816.0000\n","Training loss (for one batch) at step 729: 83143507968.0000\n","Training loss (for one batch) at step 730: 11232425984.0000\n","Training loss (for one batch) at step 731: 62169448448.0000\n","Training loss (for one batch) at step 732: 12254863360.0000\n","Training loss (for one batch) at step 733: 8515371008.0000\n","Training loss (for one batch) at step 734: 18333345792.0000\n","Training loss (for one batch) at step 735: 300350832640.0000\n","Training loss (for one batch) at step 736: 5675552768.0000\n","Training loss (for one batch) at step 737: 19855454208.0000\n","Training loss (for one batch) at step 738: 81707483136.0000\n","Training loss (for one batch) at step 739: 16210758656.0000\n","Training loss (for one batch) at step 740: 27277053952.0000\n","Training loss (for one batch) at step 741: 274109988864.0000\n","Training loss (for one batch) at step 742: 1335778344960.0000\n","Training loss (for one batch) at step 743: 292603232256.0000\n","Training loss (for one batch) at step 744: 152852725760.0000\n","Training loss (for one batch) at step 745: 198703677440.0000\n","Training loss (for one batch) at step 746: 8197069824.0000\n","Training loss (for one batch) at step 747: 4513776640.0000\n","Training loss (for one batch) at step 748: 28012832768.0000\n","Training loss (for one batch) at step 749: 25702770688.0000\n","Training loss (for one batch) at step 750: 76132343808.0000\n","Training loss (for one batch) at step 751: 154364559360.0000\n","Training loss (for one batch) at step 752: 4075252736.0000\n","Training loss (for one batch) at step 753: 6721643520.0000\n","Training loss (for one batch) at step 754: 17153126400.0000\n","Training loss (for one batch) at step 755: 14449718272.0000\n","Training loss (for one batch) at step 756: 9600399360.0000\n","Training loss (for one batch) at step 757: 801170849792.0000\n","Training loss (for one batch) at step 758: 62889844736.0000\n","Training loss (for one batch) at step 759: 38302068736.0000\n","Training loss (for one batch) at step 760: 141819625472.0000\n","Training loss (for one batch) at step 761: 12454046720.0000\n","Training loss (for one batch) at step 762: 10683883520.0000\n","Training loss (for one batch) at step 763: 22091765760.0000\n","Training loss (for one batch) at step 764: 5391370240.0000\n","Training loss (for one batch) at step 765: 30884321280.0000\n","Training loss (for one batch) at step 766: 18251620352.0000\n","Training loss (for one batch) at step 767: 570712832.0000\n","Training loss (for one batch) at step 768: 28009943040.0000\n","Training loss (for one batch) at step 769: 4552454144.0000\n","Training loss (for one batch) at step 770: 9746947072.0000\n","Training loss (for one batch) at step 771: 136845328384.0000\n","Training loss (for one batch) at step 772: 34332643328.0000\n","Training loss (for one batch) at step 773: 61509820416.0000\n","Training loss (for one batch) at step 774: 118432956416.0000\n","Training loss (for one batch) at step 775: 6947732480.0000\n","Training loss (for one batch) at step 776: 1351503970304.0000\n","Training loss (for one batch) at step 777: 24518414336.0000\n","Training loss (for one batch) at step 778: 1643135744.0000\n","Training loss (for one batch) at step 779: 202677256192.0000\n","Training loss (for one batch) at step 780: 76095856640.0000\n","Training loss (for one batch) at step 781: 319961595904.0000\n","Training loss (for one batch) at step 782: 3612788224.0000\n","Training loss (for one batch) at step 783: 5989071872.0000\n","Training loss (for one batch) at step 784: 16243622912.0000\n","Training loss (for one batch) at step 785: 11510942720.0000\n","Training loss (for one batch) at step 786: 218918682624.0000\n","Training loss (for one batch) at step 787: 48622411776.0000\n","Training loss (for one batch) at step 788: 85801091072.0000\n","Training loss (for one batch) at step 789: 209696243712.0000\n","Training loss (for one batch) at step 790: 211976093696.0000\n","Training loss (for one batch) at step 791: 9289074688.0000\n","Training loss (for one batch) at step 792: 52808933376.0000\n","Training loss (for one batch) at step 793: 30979710976.0000\n","Training loss (for one batch) at step 794: 5070469988352.0000\n","Training loss (for one batch) at step 795: 9788851200.0000\n","Training loss (for one batch) at step 796: 143774941184.0000\n","Training loss (for one batch) at step 797: 16688588800.0000\n","Training loss (for one batch) at step 798: 40012374016.0000\n","Training loss (for one batch) at step 799: 9545392128.0000\n","Training loss (for one batch) at step 800: 178337759232.0000\n","Training loss (for one batch) at step 801: 217216221184.0000\n","Training loss (for one batch) at step 802: 9813852160.0000\n","Training loss (for one batch) at step 803: 11798854656.0000\n","Training loss (for one batch) at step 804: 37546868736.0000\n","Training loss (for one batch) at step 805: 40043806720.0000\n","Training loss (for one batch) at step 806: 3730975744.0000\n","Training loss (for one batch) at step 807: 232977022976.0000\n","Training loss (for one batch) at step 808: 19194427392.0000\n","Training loss (for one batch) at step 809: 71554924544.0000\n","Training loss (for one batch) at step 810: 2965035008.0000\n","Training loss (for one batch) at step 811: 2209706672128.0000\n","Training loss (for one batch) at step 812: 38943399936.0000\n","Training loss (for one batch) at step 813: 3803447040.0000\n","Training loss (for one batch) at step 814: 5807588352.0000\n","Training loss (for one batch) at step 815: 9025357824.0000\n","Training loss (for one batch) at step 816: 1363122432.0000\n","Training loss (for one batch) at step 817: 212979957760.0000\n","Training loss (for one batch) at step 818: 40299634688.0000\n","Training loss (for one batch) at step 819: 15145524224.0000\n","Training loss (for one batch) at step 820: 2865425408.0000\n","Training loss (for one batch) at step 821: 24585752576.0000\n","Training loss (for one batch) at step 822: 4799831040.0000\n","Training loss (for one batch) at step 823: 184897028096.0000\n","Training loss (for one batch) at step 824: 29973610496.0000\n","Training loss (for one batch) at step 825: 3510522368.0000\n","Training loss (for one batch) at step 826: 314972241920.0000\n","Training loss (for one batch) at step 827: 17851080704.0000\n","Training loss (for one batch) at step 828: 30005239808.0000\n","Training loss (for one batch) at step 829: 6419397632.0000\n","Training loss (for one batch) at step 830: 341567176704.0000\n","Training loss (for one batch) at step 831: 1917914368.0000\n","Training loss (for one batch) at step 832: 85151866880.0000\n","Training loss (for one batch) at step 833: 7163702272.0000\n","Training loss (for one batch) at step 834: 5543363584.0000\n","Training loss (for one batch) at step 835: 157824532480.0000\n","Training loss (for one batch) at step 836: 41080082432.0000\n","Training loss (for one batch) at step 837: 403636551680.0000\n","Training loss (for one batch) at step 838: 6356455936.0000\n","Training loss (for one batch) at step 839: 52997603328.0000\n","Training loss (for one batch) at step 840: 7490162688.0000\n","Training loss (for one batch) at step 841: 37634949120.0000\n","Training loss (for one batch) at step 842: 47863517184.0000\n","Training loss (for one batch) at step 843: 60740034560.0000\n","Training loss (for one batch) at step 844: 226894561280.0000\n","Training loss (for one batch) at step 845: 168852078592.0000\n","Training loss (for one batch) at step 846: 14928104448.0000\n","Training loss (for one batch) at step 847: 530795724800.0000\n","Training loss (for one batch) at step 848: 3792033792.0000\n","Training loss (for one batch) at step 849: 113813962752.0000\n","Training loss (for one batch) at step 850: 225357676544.0000\n","Training loss (for one batch) at step 851: 1154728722432.0000\n","Training loss (for one batch) at step 852: 12456464384.0000\n","Training loss (for one batch) at step 853: 12098375680.0000\n","Training loss (for one batch) at step 854: 1068801916928.0000\n","Training loss (for one batch) at step 855: 9462447104.0000\n","Training loss (for one batch) at step 856: 221351215104.0000\n","Training loss (for one batch) at step 857: 2244666880.0000\n","Training loss (for one batch) at step 858: 4419507200.0000\n","Training loss (for one batch) at step 859: 13890833408.0000\n","Training loss (for one batch) at step 860: 162431893504.0000\n","Training loss (for one batch) at step 861: 280295964672.0000\n","Training loss (for one batch) at step 862: 7306577920.0000\n","Training loss (for one batch) at step 863: 342557229056.0000\n","Training loss (for one batch) at step 864: 284742156288.0000\n","Training loss (for one batch) at step 865: 77667614720.0000\n","Training loss (for one batch) at step 866: 295948124160.0000\n","Training loss (for one batch) at step 867: 12113660928.0000\n","Training loss (for one batch) at step 868: 139779817472.0000\n","Training loss (for one batch) at step 869: 25803882496.0000\n","Training loss (for one batch) at step 870: 442456375296.0000\n","Training loss (for one batch) at step 871: 577357873152.0000\n","Training loss (for one batch) at step 872: 174379384832.0000\n","Training loss (for one batch) at step 873: 1269762176.0000\n","Training loss (for one batch) at step 874: 86689906688.0000\n","Training loss (for one batch) at step 875: 175804383232.0000\n","Training loss (for one batch) at step 876: 7577249280.0000\n","Training loss (for one batch) at step 877: 183572774912.0000\n","Training loss (for one batch) at step 878: 700244754432.0000\n","Training loss (for one batch) at step 879: 67734585344.0000\n","Training loss (for one batch) at step 880: 3148091904.0000\n","Training loss (for one batch) at step 881: 18423312384.0000\n","Training loss (for one batch) at step 882: 4394203136.0000\n","Training loss (for one batch) at step 883: 10756829184.0000\n","Training loss (for one batch) at step 884: 43174346752.0000\n","Training loss (for one batch) at step 885: 7279607296.0000\n","Training loss (for one batch) at step 886: 16015719424.0000\n","Training loss (for one batch) at step 887: 169552510976.0000\n","Training loss (for one batch) at step 888: 1874131288064.0000\n","Training loss (for one batch) at step 889: 25224204288.0000\n","Training loss (for one batch) at step 890: 240731439104.0000\n","Training loss (for one batch) at step 891: 353614266368.0000\n","Training loss (for one batch) at step 892: 96962838528.0000\n","Training loss (for one batch) at step 893: 3442432278528.0000\n","Training loss (for one batch) at step 894: 8201632768.0000\n","Training loss (for one batch) at step 895: 364288081920.0000\n","Training loss (for one batch) at step 896: 113833631744.0000\n","Training loss (for one batch) at step 897: 299779588096.0000\n","Training loss (for one batch) at step 898: 941117210624.0000\n","Training loss (for one batch) at step 899: 382946836480.0000\n","Training loss (for one batch) at step 900: 416577257472.0000\n","Training loss (for one batch) at step 901: 197697880064.0000\n","Training loss (for one batch) at step 902: 8109393920.0000\n","Training loss (for one batch) at step 903: 27131295744.0000\n","Training loss (for one batch) at step 904: 16703750144.0000\n","Training loss (for one batch) at step 905: 2953676032.0000\n","Training loss (for one batch) at step 906: 2716114432.0000\n","Training loss (for one batch) at step 907: 64036765696.0000\n","Training loss (for one batch) at step 908: 225804615680.0000\n","Training loss (for one batch) at step 909: 45651673088.0000\n","Training loss (for one batch) at step 910: 6148509696.0000\n","Training loss (for one batch) at step 911: 13050983424.0000\n","Training loss (for one batch) at step 912: 19213772800.0000\n","Training loss (for one batch) at step 913: 92516966400.0000\n","Training loss (for one batch) at step 914: 18690981888.0000\n","Training loss (for one batch) at step 915: 160956235776.0000\n","Training loss (for one batch) at step 916: 9185916928.0000\n","Training loss (for one batch) at step 917: 577346142208.0000\n","Training loss (for one batch) at step 918: 19695185920.0000\n","Training loss (for one batch) at step 919: 3108432896.0000\n","Training loss (for one batch) at step 920: 10917794816.0000\n","Training loss (for one batch) at step 921: 151307337728.0000\n","Training loss (for one batch) at step 922: 7701917696.0000\n","Training loss (for one batch) at step 923: 799578718208.0000\n","Training loss (for one batch) at step 924: 4160832256.0000\n","Training loss (for one batch) at step 925: 14119329792.0000\n","Training loss (for one batch) at step 926: 18116009984.0000\n","Training loss (for one batch) at step 927: 93925163008.0000\n","Training loss (for one batch) at step 928: 7393106432.0000\n","Training loss (for one batch) at step 929: 28793915392.0000\n","Training loss (for one batch) at step 930: 6228023808.0000\n","Training loss (for one batch) at step 931: 23892637696.0000\n","Training loss (for one batch) at step 932: 6135026176.0000\n","Training loss (for one batch) at step 933: 2432660992.0000\n","Training loss (for one batch) at step 934: 19157983232.0000\n","Training loss (for one batch) at step 935: 15194088448.0000\n","Training loss (for one batch) at step 936: 4888013312.0000\n","Training loss (for one batch) at step 937: 779948392448.0000\n","Training loss (for one batch) at step 938: 310142894080.0000\n","Training loss (for one batch) at step 939: 533294972928.0000\n","Training loss (for one batch) at step 940: 2174857728.0000\n","Training loss (for one batch) at step 941: 28954077184.0000\n","Training loss (for one batch) at step 942: 9672513536.0000\n","Training loss (for one batch) at step 943: 11301954560.0000\n","Training loss (for one batch) at step 944: 534530293760.0000\n","Training loss (for one batch) at step 945: 44588695552.0000\n","Training loss (for one batch) at step 946: 101984722944.0000\n","Training loss (for one batch) at step 947: 261874679808.0000\n","Training loss (for one batch) at step 948: 262685540352.0000\n","Training loss (for one batch) at step 949: 89679765504.0000\n","Training loss (for one batch) at step 950: 94875836416.0000\n","Training loss (for one batch) at step 951: 4918465024.0000\n","Training loss (for one batch) at step 952: 6128995328.0000\n","Training loss (for one batch) at step 953: 245346877440.0000\n","Training loss (for one batch) at step 954: 154594738176.0000\n","Training loss (for one batch) at step 955: 4331674624.0000\n","Training loss (for one batch) at step 956: 4151839488.0000\n","Training loss (for one batch) at step 957: 253150871552.0000\n","Training loss (for one batch) at step 958: 12422860800.0000\n","Training loss (for one batch) at step 959: 189728571392.0000\n","Training loss (for one batch) at step 960: 75034894336.0000\n","Training loss (for one batch) at step 961: 15874302976.0000\n","Training loss (for one batch) at step 962: 191338987520.0000\n","Training loss (for one batch) at step 963: 102956351488.0000\n","Training loss (for one batch) at step 964: 4850732032.0000\n","Training loss (for one batch) at step 965: 59746983936.0000\n","Training loss (for one batch) at step 966: 205104840704.0000\n","Training loss (for one batch) at step 967: 32504598528.0000\n","Training loss (for one batch) at step 968: 45958189056.0000\n","Training loss (for one batch) at step 969: 18451560448.0000\n","Training loss (for one batch) at step 970: 3048156928.0000\n","Training loss (for one batch) at step 971: 98522906624.0000\n","Training loss (for one batch) at step 972: 28079468544.0000\n","Training loss (for one batch) at step 973: 3267342592.0000\n","Training loss (for one batch) at step 974: 1841272832.0000\n","Training loss (for one batch) at step 975: 40117239808.0000\n","Training loss (for one batch) at step 976: 16300590080.0000\n","Training loss (for one batch) at step 977: 777147056128.0000\n","Training loss (for one batch) at step 978: 20202684416.0000\n","Training loss (for one batch) at step 979: 11646734336.0000\n","Training loss (for one batch) at step 980: 73794191360.0000\n","Training loss (for one batch) at step 981: 1692429312.0000\n","Training loss (for one batch) at step 982: 81256595456.0000\n","Training loss (for one batch) at step 983: 2256912640.0000\n","Training loss (for one batch) at step 984: 5390941184.0000\n","Training loss (for one batch) at step 985: 3630240768.0000\n","Training loss (for one batch) at step 986: 43423006720.0000\n","Training loss (for one batch) at step 987: 433572773888.0000\n","Training loss (for one batch) at step 988: 16002452480.0000\n","Training loss (for one batch) at step 989: 211543425024.0000\n","Training loss (for one batch) at step 990: 217667452928.0000\n","Training loss (for one batch) at step 991: 105006153728.0000\n","Training loss (for one batch) at step 992: 55898533888.0000\n","Training loss (for one batch) at step 993: 4347435008.0000\n","Training loss (for one batch) at step 994: 7921624064.0000\n","Training loss (for one batch) at step 995: 3206693376.0000\n","Training loss (for one batch) at step 996: 128201252864.0000\n","Training loss (for one batch) at step 997: 7732074496.0000\n","Training loss (for one batch) at step 998: 408589271040.0000\n","Training loss (for one batch) at step 999: 14885274624.0000\n","Training loss (for one batch) at step 1000: 16671408128.0000\n","Training loss (for one batch) at step 1001: 6342899200.0000\n","Training loss (for one batch) at step 1002: 12412007424.0000\n","Training loss (for one batch) at step 1003: 1179079802880.0000\n","Training loss (for one batch) at step 1004: 7369911808.0000\n","Training loss (for one batch) at step 1005: 14355662848.0000\n","Training loss (for one batch) at step 1006: 46654320640.0000\n","Training loss (for one batch) at step 1007: 26290593792.0000\n","Training loss (for one batch) at step 1008: 18343518208.0000\n","Training loss (for one batch) at step 1009: 54318137344.0000\n","Training loss (for one batch) at step 1010: 15865448448.0000\n","Training loss (for one batch) at step 1011: 101399379968.0000\n","Training loss (for one batch) at step 1012: 12873016320.0000\n","Training loss (for one batch) at step 1013: 18532024320.0000\n","Training loss (for one batch) at step 1014: 26027108352.0000\n","Training loss (for one batch) at step 1015: 21014315008.0000\n","Training loss (for one batch) at step 1016: 61451378688.0000\n","Training loss (for one batch) at step 1017: 3607262208.0000\n","Training loss (for one batch) at step 1018: 70550945792.0000\n","Training loss (for one batch) at step 1019: 301757431808.0000\n","Training loss (for one batch) at step 1020: 186217234432.0000\n","Training loss (for one batch) at step 1021: 4713723904.0000\n","Training loss (for one batch) at step 1022: 438561046528.0000\n","Training loss (for one batch) at step 1023: 11746205696.0000\n","Training loss (for one batch) at step 1024: 3537532928.0000\n","Training loss (for one batch) at step 1025: 5453411328.0000\n","Training loss (for one batch) at step 1026: 23239786496.0000\n","Training loss (for one batch) at step 1027: 414717771776.0000\n","Training loss (for one batch) at step 1028: 112493821952.0000\n","Training loss (for one batch) at step 1029: 161566162944.0000\n","Training loss (for one batch) at step 1030: 37335556096.0000\n","Training loss (for one batch) at step 1031: 131210723328.0000\n","Training loss (for one batch) at step 1032: 4005657344.0000\n","Training loss (for one batch) at step 1033: 16459456512.0000\n","Training loss (for one batch) at step 1034: 8564272128.0000\n","Training loss (for one batch) at step 1035: 68922490880.0000\n","Training loss (for one batch) at step 1036: 48642019328.0000\n","Training loss (for one batch) at step 1037: 58240212992.0000\n","Training loss (for one batch) at step 1038: 301515931648.0000\n","Training loss (for one batch) at step 1039: 1256719515648.0000\n","Training loss (for one batch) at step 1040: 89859497984.0000\n","Training loss (for one batch) at step 1041: 14474222592.0000\n","Training loss (for one batch) at step 1042: 59791347712.0000\n","Training loss (for one batch) at step 1043: 315223506944.0000\n","Training loss (for one batch) at step 1044: 41179385856.0000\n","Training loss (for one batch) at step 1045: 2107392512.0000\n","Training loss (for one batch) at step 1046: 312372822016.0000\n","Training loss (for one batch) at step 1047: 61119229952.0000\n","Training loss (for one batch) at step 1048: 253050257408.0000\n","Training loss (for one batch) at step 1049: 54542929920.0000\n","Training loss (for one batch) at step 1050: 235192008704.0000\n","Training loss (for one batch) at step 1051: 8631088128.0000\n","Training loss (for one batch) at step 1052: 17853521920.0000\n","Training loss (for one batch) at step 1053: 1453575552.0000\n","Training loss (for one batch) at step 1054: 106135289856.0000\n","Training loss (for one batch) at step 1055: 15762806784.0000\n","Training loss (for one batch) at step 1056: 152656510976.0000\n","Training loss (for one batch) at step 1057: 177784520704.0000\n","Training loss (for one batch) at step 1058: 3565995520.0000\n","Training loss (for one batch) at step 1059: 18807779328.0000\n","Training loss (for one batch) at step 1060: 67962015744.0000\n","Training loss (for one batch) at step 1061: 146119819264.0000\n","Training loss (for one batch) at step 1062: 56699981824.0000\n","Training loss (for one batch) at step 1063: 20417789952.0000\n","Training loss (for one batch) at step 1064: 363357011968.0000\n","Training loss (for one batch) at step 1065: 1575980544.0000\n","Training loss (for one batch) at step 1066: 148623360000.0000\n","Training loss (for one batch) at step 1067: 114745147392.0000\n","Training loss (for one batch) at step 1068: 3233467858944.0000\n","Training loss (for one batch) at step 1069: 6527907328.0000\n","Training loss (for one batch) at step 1070: 30837041152.0000\n","Training loss (for one batch) at step 1071: 734911135744.0000\n","Training loss (for one batch) at step 1072: 135386365952.0000\n","Training loss (for one batch) at step 1073: 8501219840.0000\n","Training loss (for one batch) at step 1074: 41567490048.0000\n","Training loss (for one batch) at step 1075: 90682425344.0000\n","Training loss (for one batch) at step 1076: 4248286976.0000\n","Training loss (for one batch) at step 1077: 7906671104.0000\n","Training loss (for one batch) at step 1078: 6734109696.0000\n","Training loss (for one batch) at step 1079: 64443351040.0000\n","Training loss (for one batch) at step 1080: 604026372096.0000\n","Training loss (for one batch) at step 1081: 100568326144.0000\n","Training loss (for one batch) at step 1082: 1453187712.0000\n","Training loss (for one batch) at step 1083: 145631428608.0000\n","Training loss (for one batch) at step 1084: 51684323328.0000\n","Training loss (for one batch) at step 1085: 445963648.0000\n","Training loss (for one batch) at step 1086: 31095848960.0000\n","Training loss (for one batch) at step 1087: 111618097152.0000\n","Training loss (for one batch) at step 1088: 1762449536.0000\n","Training loss (for one batch) at step 1089: 231559905280.0000\n","Training loss (for one batch) at step 1090: 242541658112.0000\n","Training loss (for one batch) at step 1091: 20707479552.0000\n","Training loss (for one batch) at step 1092: 56622137344.0000\n","Training loss (for one batch) at step 1093: 205891846144.0000\n","Training loss (for one batch) at step 1094: 599322525696.0000\n","Training loss (for one batch) at step 1095: 75790032896.0000\n","Training loss (for one batch) at step 1096: 20269178.0000\n","Training loss (for one batch) at step 1097: 11915384832.0000\n","Training loss (for one batch) at step 1098: 30540206080.0000\n","Training loss (for one batch) at step 1099: 315412119552.0000\n","Training loss (for one batch) at step 1100: 21305225216.0000\n","Training loss (for one batch) at step 1101: 61910167552.0000\n","Training loss (for one batch) at step 1102: 35189944320.0000\n","Training loss (for one batch) at step 1103: 4795500032.0000\n","Training loss (for one batch) at step 1104: 13402712064.0000\n","Training loss (for one batch) at step 1105: 238189674496.0000\n","Training loss (for one batch) at step 1106: 14734497792.0000\n","Training loss (for one batch) at step 1107: 429994377216.0000\n","Training loss (for one batch) at step 1108: 22302941184.0000\n","Training loss (for one batch) at step 1109: 11635931136.0000\n","Training loss (for one batch) at step 1110: 13922125824.0000\n","Training loss (for one batch) at step 1111: 141077479424.0000\n","Training loss (for one batch) at step 1112: 26250272768.0000\n","Training loss (for one batch) at step 1113: 19812132864.0000\n","Training loss (for one batch) at step 1114: 131231784960.0000\n","Training loss (for one batch) at step 1115: 179665174528.0000\n","Training loss (for one batch) at step 1116: 2349438861312.0000\n","Training loss (for one batch) at step 1117: 11491289088.0000\n","Training loss (for one batch) at step 1118: 14038495232.0000\n","Training loss (for one batch) at step 1119: 5283615232.0000\n","Training loss (for one batch) at step 1120: 2196482293760.0000\n","Training loss (for one batch) at step 1121: 12228955136.0000\n","Training loss (for one batch) at step 1122: 3429470720.0000\n","Training loss (for one batch) at step 1123: 81671266304.0000\n","Training loss (for one batch) at step 1124: 13161919488.0000\n","Training loss (for one batch) at step 1125: 4752267264.0000\n","Training loss (for one batch) at step 1126: 8653123584.0000\n","Training loss (for one batch) at step 1127: 214894788608.0000\n","Training loss (for one batch) at step 1128: 3371525376.0000\n","Training loss (for one batch) at step 1129: 160803667968.0000\n","Training loss (for one batch) at step 1130: 95383863296.0000\n","Training loss (for one batch) at step 1131: 5722347520.0000\n","Training loss (for one batch) at step 1132: 146950619136.0000\n","Training loss (for one batch) at step 1133: 4905135616.0000\n","Training loss (for one batch) at step 1134: 820144046080.0000\n","Training loss (for one batch) at step 1135: 85964890112.0000\n","Training loss (for one batch) at step 1136: 1671616768.0000\n","Training loss (for one batch) at step 1137: 552300380160.0000\n","Training loss (for one batch) at step 1138: 96997548032.0000\n","Training loss (for one batch) at step 1139: 1523901399040.0000\n","Training loss (for one batch) at step 1140: 148030521344.0000\n","Training loss (for one batch) at step 1141: 22711894016.0000\n","Training loss (for one batch) at step 1142: 342104145920.0000\n","Training loss (for one batch) at step 1143: 9851871232.0000\n","Training loss (for one batch) at step 1144: 11886927872.0000\n","Training loss (for one batch) at step 1145: 298917036032.0000\n","Training loss (for one batch) at step 1146: 2857254912.0000\n","Training loss (for one batch) at step 1147: 20190228480.0000\n","Training loss (for one batch) at step 1148: 37101674496.0000\n","Training loss (for one batch) at step 1149: 800673536.0000\n","Training loss (for one batch) at step 1150: 28889718784.0000\n","Training loss (for one batch) at step 1151: 73213018112.0000\n","Training loss (for one batch) at step 1152: 115220824064.0000\n","Training loss (for one batch) at step 1153: 208747429888.0000\n","Training loss (for one batch) at step 1154: 61957939200.0000\n","Training loss (for one batch) at step 1155: 9359845376.0000\n","Training loss (for one batch) at step 1156: 11827281920.0000\n","Training loss (for one batch) at step 1157: 31201863680.0000\n","Training loss (for one batch) at step 1158: 5076847104.0000\n","Training loss (for one batch) at step 1159: 45291659264.0000\n","Training loss (for one batch) at step 1160: 263874379776.0000\n","Training loss (for one batch) at step 1161: 11152526336.0000\n","Training loss (for one batch) at step 1162: 67016896512.0000\n","Training loss (for one batch) at step 1163: 33337260032.0000\n","Training loss (for one batch) at step 1164: 27217317888.0000\n","Training loss (for one batch) at step 1165: 533901705216.0000\n","Training loss (for one batch) at step 1166: 1432521146368.0000\n","Training loss (for one batch) at step 1167: 26108192768.0000\n","Training loss (for one batch) at step 1168: 7202491904.0000\n","Training loss (for one batch) at step 1169: 35881922560.0000\n","Training loss (for one batch) at step 1170: 3101340928.0000\n","Training loss (for one batch) at step 1171: 66154917888.0000\n","Training loss (for one batch) at step 1172: 409533568.0000\n","Training loss (for one batch) at step 1173: 40320036864.0000\n","Training loss (for one batch) at step 1174: 53953130496.0000\n","Training loss (for one batch) at step 1175: 118152724480.0000\n","Training loss (for one batch) at step 1176: 1201622745088.0000\n","Training loss (for one batch) at step 1177: 162669760.0000\n","Training loss (for one batch) at step 1178: 6943688704.0000\n","Training loss (for one batch) at step 1179: 513787559936.0000\n","Training loss (for one batch) at step 1180: 57244516352.0000\n","Training loss (for one batch) at step 1181: 33426034688.0000\n","Training loss (for one batch) at step 1182: 25375191040.0000\n","Training loss (for one batch) at step 1183: 3694026752.0000\n","Training loss (for one batch) at step 1184: 247504224256.0000\n","Training loss (for one batch) at step 1185: 72601501696.0000\n","Training loss (for one batch) at step 1186: 271885811712.0000\n","Training loss (for one batch) at step 1187: 24052391936.0000\n","Training loss (for one batch) at step 1188: 96789217280.0000\n","Training loss (for one batch) at step 1189: 23867932672.0000\n","Training loss (for one batch) at step 1190: 358076317696.0000\n","Training loss (for one batch) at step 1191: 9770608640.0000\n","Training loss (for one batch) at step 1192: 106746249216.0000\n","Training loss (for one batch) at step 1193: 19133532160.0000\n","Training loss (for one batch) at step 1194: 6379544064.0000\n","Training loss (for one batch) at step 1195: 5449419587584.0000\n","Training loss (for one batch) at step 1196: 4347674112.0000\n","Training loss (for one batch) at step 1197: 77335650304.0000\n","Training loss (for one batch) at step 1198: 3249613312.0000\n","Training loss (for one batch) at step 1199: 230450282496.0000\n","Training loss (for one batch) at step 1200: 3720007936.0000\n","Training loss (for one batch) at step 1201: 22871601152.0000\n","Training loss (for one batch) at step 1202: 115479969792.0000\n","Training loss (for one batch) at step 1203: 492853854208.0000\n","Training loss (for one batch) at step 1204: 75734941696.0000\n","Training loss (for one batch) at step 1205: 6803213312.0000\n","Training loss (for one batch) at step 1206: 104475451392.0000\n","Training loss (for one batch) at step 1207: 10534649856.0000\n","Training loss (for one batch) at step 1208: 1152213760.0000\n","Training loss (for one batch) at step 1209: 154315292672.0000\n","Training loss (for one batch) at step 1210: 2513635840.0000\n","Training loss (for one batch) at step 1211: 67349696512.0000\n","Training loss (for one batch) at step 1212: 18380115968.0000\n","Training loss (for one batch) at step 1213: 246322790400.0000\n","Training loss (for one batch) at step 1214: 90093690880.0000\n","Training loss (for one batch) at step 1215: 123370881024.0000\n","Training loss (for one batch) at step 1216: 273469243392.0000\n","Training loss (for one batch) at step 1217: 19163918336.0000\n","Training loss (for one batch) at step 1218: 146592497664.0000\n","Training loss (for one batch) at step 1219: 19536076800.0000\n","Training loss (for one batch) at step 1220: 358864027648.0000\n","Training loss (for one batch) at step 1221: 7567273984.0000\n","Training loss (for one batch) at step 1222: 2940004352.0000\n","Training loss (for one batch) at step 1223: 302396243968.0000\n","Training loss (for one batch) at step 1224: 14306991104.0000\n","Training loss (for one batch) at step 1225: 6636579840.0000\n","Training loss (for one batch) at step 1226: 44084801536.0000\n","Training loss (for one batch) at step 1227: 6825369600.0000\n","Training loss (for one batch) at step 1228: 143217377280.0000\n","Training loss (for one batch) at step 1229: 14000205824.0000\n","Training loss (for one batch) at step 1230: 42238423040.0000\n","Training loss (for one batch) at step 1231: 1948549120.0000\n","Training loss (for one batch) at step 1232: 19151015936.0000\n","Training loss (for one batch) at step 1233: 13795745792.0000\n","Training loss (for one batch) at step 1234: 36057321472.0000\n","Training loss (for one batch) at step 1235: 20590964736.0000\n","Training loss (for one batch) at step 1236: 140946014208.0000\n","Training loss (for one batch) at step 1237: 7543417856.0000\n","Training loss (for one batch) at step 1238: 7211661824.0000\n","Training loss (for one batch) at step 1239: 9894981632.0000\n","Training loss (for one batch) at step 1240: 24228433920.0000\n","Training loss (for one batch) at step 1241: 134837149696.0000\n","Training loss (for one batch) at step 1242: 5527995904.0000\n","Training loss (for one batch) at step 1243: 96305774592.0000\n","Training loss (for one batch) at step 1244: 17520848896.0000\n","Training loss (for one batch) at step 1245: 46608355328.0000\n","Training loss (for one batch) at step 1246: 165715722240.0000\n","Training loss (for one batch) at step 1247: 68646236160.0000\n","Training loss (for one batch) at step 1248: 9063495680.0000\n","Training loss (for one batch) at step 1249: 32998928384.0000\n","Training loss (for one batch) at step 1250: 39798689792.0000\n","Training loss (for one batch) at step 1251: 3501441024.0000\n","Training loss (for one batch) at step 1252: 138319036416.0000\n","Training loss (for one batch) at step 1253: 8784781312.0000\n","Training loss (for one batch) at step 1254: 27367313408.0000\n","Training loss (for one batch) at step 1255: 4882038272.0000\n","Training loss (for one batch) at step 1256: 5637239808.0000\n","Training loss (for one batch) at step 1257: 86028025856.0000\n","Training loss (for one batch) at step 1258: 26382346240.0000\n","Training loss (for one batch) at step 1259: 1421240576.0000\n","Training loss (for one batch) at step 1260: 49891110912.0000\n","Training loss (for one batch) at step 1261: 30758189056.0000\n","Training loss (for one batch) at step 1262: 14833193984.0000\n","Training loss (for one batch) at step 1263: 60017700864.0000\n","Training loss (for one batch) at step 1264: 265150365696.0000\n","Training loss (for one batch) at step 1265: 155924119552.0000\n","Training loss (for one batch) at step 1266: 298370465792.0000\n","Training loss (for one batch) at step 1267: 17356029952.0000\n","Training loss (for one batch) at step 1268: 14213903360.0000\n","Training loss (for one batch) at step 1269: 7505083392.0000\n","Training loss (for one batch) at step 1270: 124761653248.0000\n","Training loss (for one batch) at step 1271: 185082576896.0000\n","Training loss (for one batch) at step 1272: 8081779712.0000\n","Training loss (for one batch) at step 1273: 7428814336.0000\n","Training loss (for one batch) at step 1274: 66349596672.0000\n","Training loss (for one batch) at step 1275: 69611520000.0000\n","Training loss (for one batch) at step 1276: 5610566656.0000\n","Training loss (for one batch) at step 1277: 3658197248.0000\n","Training loss (for one batch) at step 1278: 6436948992.0000\n","Training loss (for one batch) at step 1279: 12656398336.0000\n","Training loss (for one batch) at step 1280: 19836817408.0000\n","Training loss (for one batch) at step 1281: 11145286656.0000\n","Training loss (for one batch) at step 1282: 27984441344.0000\n","Training loss (for one batch) at step 1283: 4642193920.0000\n","Training loss (for one batch) at step 1284: 119001980928.0000\n","Training loss (for one batch) at step 1285: 194086027264.0000\n","Training loss (for one batch) at step 1286: 97358962688.0000\n","Training loss (for one batch) at step 1287: 213836185600.0000\n","Training loss (for one batch) at step 1288: 10267924480.0000\n","Training loss (for one batch) at step 1289: 158755307520.0000\n","Training loss (for one batch) at step 1290: 2204141879296.0000\n","Training loss (for one batch) at step 1291: 9378368512.0000\n","Training loss (for one batch) at step 1292: 128217161728.0000\n","Training loss (for one batch) at step 1293: 27621728256.0000\n","Training loss (for one batch) at step 1294: 5666903552.0000\n","Training loss (for one batch) at step 1295: 354235088896.0000\n","Training loss (for one batch) at step 1296: 8326488576.0000\n","Training loss (for one batch) at step 1297: 11142091776.0000\n","Training loss (for one batch) at step 1298: 1793517312.0000\n","Training loss (for one batch) at step 1299: 71068540928.0000\n","Training loss (for one batch) at step 1300: 231193673728.0000\n","Training loss (for one batch) at step 1301: 23267909632.0000\n","Training loss (for one batch) at step 1302: 53600182272.0000\n","Training loss (for one batch) at step 1303: 1377125531648.0000\n","Training loss (for one batch) at step 1304: 554117760.0000\n","Training loss (for one batch) at step 1305: 110749392896.0000\n","Training loss (for one batch) at step 1306: 13500235776.0000\n","Training loss (for one batch) at step 1307: 2272530176.0000\n","Training loss (for one batch) at step 1308: 9120609280.0000\n","Training loss (for one batch) at step 1309: 15481548800.0000\n","Training loss (for one batch) at step 1310: 16560000000.0000\n","Training loss (for one batch) at step 1311: 23257118720.0000\n","Training loss (for one batch) at step 1312: 19920166912.0000\n","Training loss (for one batch) at step 1313: 23589683200.0000\n","Training loss (for one batch) at step 1314: 65801252864.0000\n","Training loss (for one batch) at step 1315: 4296770560.0000\n","Training loss (for one batch) at step 1316: 27883057152.0000\n","Training loss (for one batch) at step 1317: 36470075392.0000\n","Training loss (for one batch) at step 1318: 660320026624.0000\n","Training loss (for one batch) at step 1319: 138521182208.0000\n","Training loss (for one batch) at step 1320: 3166088200192.0000\n","Training loss (for one batch) at step 1321: 6728836096.0000\n","Training loss (for one batch) at step 1322: 51135963136.0000\n","Training loss (for one batch) at step 1323: 29715658752.0000\n","Training loss (for one batch) at step 1324: 33626642432.0000\n","Training loss (for one batch) at step 1325: 4015474688.0000\n","Training loss (for one batch) at step 1326: 3555614208.0000\n","Training loss (for one batch) at step 1327: 31651950592.0000\n","Training loss (for one batch) at step 1328: 358614269952.0000\n","Training loss (for one batch) at step 1329: 42991751168.0000\n","Training loss (for one batch) at step 1330: 70845431808.0000\n","Training loss (for one batch) at step 1331: 17968148480.0000\n","Training loss (for one batch) at step 1332: 21557045248.0000\n","Training loss (for one batch) at step 1333: 2586405632.0000\n","Training loss (for one batch) at step 1334: 9625928704.0000\n","Training loss (for one batch) at step 1335: 1870707456.0000\n","Training loss (for one batch) at step 1336: 3302644224.0000\n","Training loss (for one batch) at step 1337: 18348109824.0000\n","Training loss (for one batch) at step 1338: 21452193792.0000\n","Training loss (for one batch) at step 1339: 13726808064.0000\n","Training loss (for one batch) at step 1340: 26643488768.0000\n","Training loss (for one batch) at step 1341: 57704808448.0000\n","Training loss (for one batch) at step 1342: 26314223616.0000\n","Training loss (for one batch) at step 1343: 45971857408.0000\n","Training loss (for one batch) at step 1344: 403973636096.0000\n","Training loss (for one batch) at step 1345: 6486612992.0000\n","Training loss (for one batch) at step 1346: 3481878016.0000\n","Training loss (for one batch) at step 1347: 150383820800.0000\n","Training loss (for one batch) at step 1348: 107891474432.0000\n","Training loss (for one batch) at step 1349: 74920017920.0000\n","Training loss (for one batch) at step 1350: 11939426304.0000\n","Training loss (for one batch) at step 1351: 86129950720.0000\n","Training loss (for one batch) at step 1352: 16019761152.0000\n","Training loss (for one batch) at step 1353: 5006007296.0000\n","Training loss (for one batch) at step 1354: 3226432768.0000\n","Training loss (for one batch) at step 1355: 8836737024.0000\n","Training loss (for one batch) at step 1356: 80386949120.0000\n","Training loss (for one batch) at step 1357: 6450796544.0000\n","Training loss (for one batch) at step 1358: 19008212992.0000\n","Training loss (for one batch) at step 1359: 6143258624.0000\n","Training loss (for one batch) at step 1360: 107149713408.0000\n","Training loss (for one batch) at step 1361: 5299388928.0000\n","Training loss (for one batch) at step 1362: 44961021952.0000\n","Training loss (for one batch) at step 1363: 118486368256.0000\n","Training loss (for one batch) at step 1364: 8232066560.0000\n","Training loss (for one batch) at step 1365: 36512931840.0000\n","Training loss (for one batch) at step 1366: 3872025088.0000\n","Training loss (for one batch) at step 1367: 24850753536.0000\n","Training loss (for one batch) at step 1368: 5955542016.0000\n","Training loss (for one batch) at step 1369: 18729508864.0000\n","Training loss (for one batch) at step 1370: 2507681024.0000\n","Training loss (for one batch) at step 1371: 6033967104.0000\n","Training loss (for one batch) at step 1372: 98995003392.0000\n","Training loss (for one batch) at step 1373: 170807164928.0000\n","Training loss (for one batch) at step 1374: 33227110400.0000\n","Training loss (for one batch) at step 1375: 158333075456.0000\n","Training loss (for one batch) at step 1376: 767957991424.0000\n","Training loss (for one batch) at step 1377: 34326286336.0000\n","Training loss (for one batch) at step 1378: 112303513600.0000\n","Training loss (for one batch) at step 1379: 3920851712.0000\n","Training loss (for one batch) at step 1380: 2267365120.0000\n","Training loss (for one batch) at step 1381: 62294851584.0000\n","Training loss (for one batch) at step 1382: 25870923776.0000\n","Training loss (for one batch) at step 1383: 1009636802560.0000\n","Training loss (for one batch) at step 1384: 138655481856.0000\n","Training loss (for one batch) at step 1385: 27390640128.0000\n","Training loss (for one batch) at step 1386: 267246092288.0000\n","Training loss (for one batch) at step 1387: 95156248576.0000\n","Training loss (for one batch) at step 1388: 3394219540480.0000\n","Training loss (for one batch) at step 1389: 38948769792.0000\n","Training loss (for one batch) at step 1390: 14481512448.0000\n","Training loss (for one batch) at step 1391: 41682415616.0000\n","Training loss (for one batch) at step 1392: 76225896448.0000\n","Training loss (for one batch) at step 1393: 468226637824.0000\n","Training loss (for one batch) at step 1394: 4905343909888.0000\n","Training loss (for one batch) at step 1395: 22730706944.0000\n","Training loss (for one batch) at step 1396: 14417895424.0000\n","Training loss (for one batch) at step 1397: 34197168128.0000\n","Training loss (for one batch) at step 1398: 388403036160.0000\n","Training loss (for one batch) at step 1399: 7002469376.0000\n","Training loss (for one batch) at step 1400: 146348949504.0000\n","Training loss (for one batch) at step 1401: 3605428224.0000\n","Training loss (for one batch) at step 1402: 388687101952.0000\n","Training loss (for one batch) at step 1403: 121729613824.0000\n","Training loss (for one batch) at step 1404: 286867587072.0000\n","Training loss (for one batch) at step 1405: 138478174208.0000\n","Training loss (for one batch) at step 1406: 63254450176.0000\n","Training loss (for one batch) at step 1407: 110086397952.0000\n","Training loss (for one batch) at step 1408: 22817101824.0000\n","Training loss (for one batch) at step 1409: 279764664320.0000\n","Training loss (for one batch) at step 1410: 1981624832.0000\n","Training loss (for one batch) at step 1411: 16386877440.0000\n","Training loss (for one batch) at step 1412: 48178216960.0000\n","Training loss (for one batch) at step 1413: 136652488704.0000\n","Training loss (for one batch) at step 1414: 215443488768.0000\n","Training loss (for one batch) at step 1415: 347234402304.0000\n","Training loss (for one batch) at step 1416: 6817963520.0000\n","Training loss (for one batch) at step 1417: 3734018048.0000\n","Training loss (for one batch) at step 1418: 14030716928.0000\n","Training loss (for one batch) at step 1419: 10889521152.0000\n","Training loss (for one batch) at step 1420: 298675208192.0000\n","Training loss (for one batch) at step 1421: 33505677312.0000\n","Training loss (for one batch) at step 1422: 42790199296.0000\n","Training loss (for one batch) at step 1423: 331107434496.0000\n","Training loss (for one batch) at step 1424: 2214132992.0000\n","Training loss (for one batch) at step 1425: 1595348736.0000\n","Training loss (for one batch) at step 1426: 593191698432.0000\n","Training loss (for one batch) at step 1427: 9458560000.0000\n","Training loss (for one batch) at step 1428: 76040208384.0000\n","Training loss (for one batch) at step 1429: 285356982272.0000\n","Training loss (for one batch) at step 1430: 70611378176.0000\n","Training loss (for one batch) at step 1431: 3696450048.0000\n","Training loss (for one batch) at step 1432: 8185177088.0000\n","Training loss (for one batch) at step 1433: 18250768384.0000\n","Training loss (for one batch) at step 1434: 12520879104.0000\n","Training loss (for one batch) at step 1435: 5871873536.0000\n","Training loss (for one batch) at step 1436: 35285262336.0000\n","Training loss (for one batch) at step 1437: 11690273792.0000\n","Training loss (for one batch) at step 1438: 8567165440.0000\n","Training loss (for one batch) at step 1439: 137293045760.0000\n","Training loss (for one batch) at step 1440: 93809573888.0000\n","Training loss (for one batch) at step 1441: 6672882176.0000\n","Training loss (for one batch) at step 1442: 161582399488.0000\n","Training loss (for one batch) at step 1443: 265393799168.0000\n","Training loss (for one batch) at step 1444: 2812598784.0000\n","Training loss (for one batch) at step 1445: 460460228608.0000\n","Training loss (for one batch) at step 1446: 58116104192.0000\n","Training loss (for one batch) at step 1447: 64212275200.0000\n","Training loss (for one batch) at step 1448: 3380405760.0000\n","Training loss (for one batch) at step 1449: 15094410240.0000\n","Training loss (for one batch) at step 1450: 5888115200.0000\n","Training loss (for one batch) at step 1451: 4982508032.0000\n","Training loss (for one batch) at step 1452: 57714462720.0000\n","Training loss (for one batch) at step 1453: 75256265965568.0000\n","Training loss (for one batch) at step 1454: 70601244672.0000\n","Training loss (for one batch) at step 1455: 142917386240.0000\n","Training loss (for one batch) at step 1456: 336153411584.0000\n","Training loss (for one batch) at step 1457: 13015342080.0000\n","Training loss (for one batch) at step 1458: 10038517760.0000\n","Training loss (for one batch) at step 1459: 1404366080.0000\n","Training loss (for one batch) at step 1460: 2705517824.0000\n","Training loss (for one batch) at step 1461: 6569316864.0000\n","Training loss (for one batch) at step 1462: 10133825536.0000\n","Training loss (for one batch) at step 1463: 7780009984.0000\n","Training loss (for one batch) at step 1464: 373025144832.0000\n","Training loss (for one batch) at step 1465: 92973522944.0000\n","Training loss (for one batch) at step 1466: 12080795648.0000\n","Training loss (for one batch) at step 1467: 59381346304.0000\n","Training loss (for one batch) at step 1468: 14881089536.0000\n","Training loss (for one batch) at step 1469: 3233001984.0000\n","Training loss (for one batch) at step 1470: 6849853440.0000\n","Training loss (for one batch) at step 1471: 13613353984.0000\n","Training loss (for one batch) at step 1472: 8496390656.0000\n","Training loss (for one batch) at step 1473: 3503200000.0000\n","Training loss (for one batch) at step 1474: 591359770624.0000\n","Training loss (for one batch) at step 1475: 17219219456.0000\n","Training loss (for one batch) at step 1476: 1817035264.0000\n","Training loss (for one batch) at step 1477: 17518657536.0000\n","Training loss (for one batch) at step 1478: 12183500800.0000\n","Training loss (for one batch) at step 1479: 11051449344.0000\n","Training loss (for one batch) at step 1480: 232921546752.0000\n","Training loss (for one batch) at step 1481: 167793999872.0000\n","Training loss (for one batch) at step 1482: 86759686144.0000\n","Training loss (for one batch) at step 1483: 13101802979328.0000\n","Training loss (for one batch) at step 1484: 12018513920.0000\n","Training loss (for one batch) at step 1485: 15857278976.0000\n","Training loss (for one batch) at step 1486: 5632816128.0000\n","Training loss (for one batch) at step 1487: 95204261888.0000\n","Training loss (for one batch) at step 1488: 13342808064.0000\n","Training loss (for one batch) at step 1489: 42921586688.0000\n","Training loss (for one batch) at step 1490: 4839845888.0000\n","Training loss (for one batch) at step 1491: 9865486336.0000\n","Training loss (for one batch) at step 1492: 36444717056.0000\n","Training loss (for one batch) at step 1493: 28207005696.0000\n","Training loss (for one batch) at step 1494: 328703279104.0000\n","Training loss (for one batch) at step 1495: 48512278528.0000\n","Training loss (for one batch) at step 1496: 1093871872.0000\n","Training loss (for one batch) at step 1497: 17874317312.0000\n","Training loss (for one batch) at step 1498: 122123616256.0000\n","Training loss (for one batch) at step 1499: 219258355712.0000\n","Training loss (for one batch) at step 1500: 81089028096.0000\n","Training loss (for one batch) at step 1501: 34951680000.0000\n","Training loss (for one batch) at step 1502: 18396860416.0000\n","Training loss (for one batch) at step 1503: 3949524224.0000\n","Training loss (for one batch) at step 1504: 2048274944.0000\n","Training loss (for one batch) at step 1505: 638462656512.0000\n","Training loss (for one batch) at step 1506: 151124541440.0000\n","Training loss (for one batch) at step 1507: 3643938304.0000\n","Training loss (for one batch) at step 1508: 4868927488.0000\n","Training loss (for one batch) at step 1509: 33248811008.0000\n","Training loss (for one batch) at step 1510: 5033726976.0000\n","Training loss (for one batch) at step 1511: 55894663168.0000\n","Training loss (for one batch) at step 1512: 17587724288.0000\n","Training loss (for one batch) at step 1513: 40637308928.0000\n","Training loss (for one batch) at step 1514: 15786878976.0000\n","Training loss (for one batch) at step 1515: 32634304512.0000\n","Training loss (for one batch) at step 1516: 90589896704.0000\n","Training loss (for one batch) at step 1517: 27576614912.0000\n","Training loss (for one batch) at step 1518: 37766995968.0000\n","Training loss (for one batch) at step 1519: 290866102272.0000\n","Training loss (for one batch) at step 1520: 3767462400.0000\n","Training loss (for one batch) at step 1521: 46879051776.0000\n","Training loss (for one batch) at step 1522: 1654744960.0000\n","Training loss (for one batch) at step 1523: 935204093952.0000\n","Training loss (for one batch) at step 1524: 8316477440.0000\n","Training loss (for one batch) at step 1525: 124555075584.0000\n","Training loss (for one batch) at step 1526: 7200351744.0000\n","Training loss (for one batch) at step 1527: 18722713600.0000\n","Training loss (for one batch) at step 1528: 152139972608.0000\n","Training loss (for one batch) at step 1529: 25850163200.0000\n","Training loss (for one batch) at step 1530: 95604310016.0000\n","Training loss (for one batch) at step 1531: 18166190080.0000\n","Training loss (for one batch) at step 1532: 8077996032.0000\n","Training loss (for one batch) at step 1533: 5415409664.0000\n","Training loss (for one batch) at step 1534: 102193250304.0000\n","Training loss (for one batch) at step 1535: 1364823900160.0000\n","Training loss (for one batch) at step 1536: 1498206080.0000\n","Training loss (for one batch) at step 1537: 21349457920.0000\n","Training loss (for one batch) at step 1538: 6416565248.0000\n","Training loss (for one batch) at step 1539: 28923267072.0000\n","Training loss (for one batch) at step 1540: 40884150272.0000\n","Training loss (for one batch) at step 1541: 56263557120.0000\n","Training loss (for one batch) at step 1542: 15404498944.0000\n","Training loss (for one batch) at step 1543: 37534978048.0000\n","Training loss (for one batch) at step 1544: 2244137216.0000\n","Training loss (for one batch) at step 1545: 3297224448.0000\n","Training loss (for one batch) at step 1546: 3674470912.0000\n","Training loss (for one batch) at step 1547: 98299846656.0000\n","Training loss (for one batch) at step 1548: 19294853120.0000\n","Training loss (for one batch) at step 1549: 19941957632.0000\n","Training loss (for one batch) at step 1550: 87654998016.0000\n","Training loss (for one batch) at step 1551: 256615104512.0000\n","Training loss (for one batch) at step 1552: 30238232576.0000\n","Training loss (for one batch) at step 1553: 122278699008.0000\n","Training loss (for one batch) at step 1554: 204448219136.0000\n","Training loss (for one batch) at step 1555: 25021669376.0000\n","Training loss (for one batch) at step 1556: 17461262336.0000\n","Training loss (for one batch) at step 1557: 12216940544.0000\n","Training loss (for one batch) at step 1558: 4582303232.0000\n","Training loss (for one batch) at step 1559: 34098857984.0000\n","Training loss (for one batch) at step 1560: 9612783616.0000\n","Training loss (for one batch) at step 1561: 32181942272.0000\n","Training loss (for one batch) at step 1562: 305345495040.0000\n","Training loss (for one batch) at step 1563: 392422555648.0000\n","Training loss (for one batch) at step 1564: 41049444352.0000\n","Training loss (for one batch) at step 1565: 16761564160.0000\n","Training loss (for one batch) at step 1566: 28358850560.0000\n","Training loss (for one batch) at step 1567: 19059085312.0000\n","Training loss (for one batch) at step 1568: 460563218432.0000\n","Training loss (for one batch) at step 1569: 16751712256.0000\n","Training loss (for one batch) at step 1570: 824588435456.0000\n","Training loss (for one batch) at step 1571: 2192453795840.0000\n","Training loss (for one batch) at step 1572: 13385068544.0000\n","Training loss (for one batch) at step 1573: 141716799488.0000\n","Training loss (for one batch) at step 1574: 16066483200.0000\n","Training loss (for one batch) at step 1575: 420094214144.0000\n","Training loss (for one batch) at step 1576: 11130792960.0000\n","Training loss (for one batch) at step 1577: 12763866112.0000\n","Training loss (for one batch) at step 1578: 6624897024.0000\n","Training loss (for one batch) at step 1579: 275194839040.0000\n","Training loss (for one batch) at step 1580: 14625057792.0000\n","Training loss (for one batch) at step 1581: 47416999936.0000\n","Training loss (for one batch) at step 1582: 2800612613816320.0000\n","Training loss (for one batch) at step 1583: 8058494976.0000\n","Training loss (for one batch) at step 1584: 17557463040.0000\n","Training loss (for one batch) at step 1585: 46865465344.0000\n","Training loss (for one batch) at step 1586: 3923788800.0000\n","Training loss (for one batch) at step 1587: 7574990848.0000\n","Training loss (for one batch) at step 1588: 121760759808.0000\n","Training loss (for one batch) at step 1589: 7079108096.0000\n","Training loss (for one batch) at step 1590: 22715789312.0000\n","Training loss (for one batch) at step 1591: 97131200512.0000\n","Training loss (for one batch) at step 1592: 15393865728.0000\n","Training loss (for one batch) at step 1593: 4844491776.0000\n","Training loss (for one batch) at step 1594: 15484169216.0000\n","Training loss (for one batch) at step 1595: 37499543552.0000\n","Training loss (for one batch) at step 1596: 23870717952.0000\n","Training loss (for one batch) at step 1597: 1527039616.0000\n","Training loss (for one batch) at step 1598: 360567472128.0000\n","Training loss (for one batch) at step 1599: 3393412864.0000\n","Training loss (for one batch) at step 1600: 158447878144.0000\n","Training loss (for one batch) at step 1601: 149643132928.0000\n","Training loss (for one batch) at step 1602: 2189769216.0000\n","Training loss (for one batch) at step 1603: 7167437824.0000\n","Training loss (for one batch) at step 1604: 2954880256.0000\n","Training loss (for one batch) at step 1605: 16728242176.0000\n","Training loss (for one batch) at step 1606: 38507847680.0000\n","Training loss (for one batch) at step 1607: 27297216512.0000\n","Training loss (for one batch) at step 1608: 185407012864.0000\n","Training loss (for one batch) at step 1609: 11622199296.0000\n","Training loss (for one batch) at step 1610: 209928224768.0000\n","Training loss (for one batch) at step 1611: 224052674560.0000\n","Training loss (for one batch) at step 1612: 170800840704.0000\n","Training loss (for one batch) at step 1613: 5096549376.0000\n","Training loss (for one batch) at step 1614: 59949211648.0000\n","Training loss (for one batch) at step 1615: 3663341056.0000\n","Training loss (for one batch) at step 1616: 127237128192.0000\n","Training loss (for one batch) at step 1617: 2700665683968.0000\n","Training loss (for one batch) at step 1618: 8657193984.0000\n","Training loss (for one batch) at step 1619: 28585801728.0000\n","Training loss (for one batch) at step 1620: 9811614720.0000\n","Training loss (for one batch) at step 1621: 8480838656.0000\n","Training loss (for one batch) at step 1622: 3070830592.0000\n","Training loss (for one batch) at step 1623: 1973196544.0000\n","Training loss (for one batch) at step 1624: 2932478208.0000\n","Training loss (for one batch) at step 1625: 21615585280.0000\n","Training loss (for one batch) at step 1626: 5503619072.0000\n","Training loss (for one batch) at step 1627: 44144631808.0000\n","Training loss (for one batch) at step 1628: 80054190080.0000\n","Training loss (for one batch) at step 1629: 24893863936.0000\n","Training loss (for one batch) at step 1630: 3546757632.0000\n","Training loss (for one batch) at step 1631: 88373002240.0000\n","Training loss (for one batch) at step 1632: 138698899456.0000\n","Training loss (for one batch) at step 1633: 274084560896.0000\n","Training loss (for one batch) at step 1634: 6178849280.0000\n","Training loss (for one batch) at step 1635: 17561718784.0000\n","Training loss (for one batch) at step 1636: 10599763968.0000\n","Training loss (for one batch) at step 1637: 6824591360.0000\n","Training loss (for one batch) at step 1638: 72663941120.0000\n","Training loss (for one batch) at step 1639: 3898263040.0000\n","Training loss (for one batch) at step 1640: 46423572480.0000\n","Training loss (for one batch) at step 1641: 73720594432.0000\n","Training loss (for one batch) at step 1642: 199552647168.0000\n","Training loss (for one batch) at step 1643: 385425604608.0000\n","Training loss (for one batch) at step 1644: 234593812480.0000\n","Training loss (for one batch) at step 1645: 20431845376.0000\n","Training loss (for one batch) at step 1646: 148139524096.0000\n","Training loss (for one batch) at step 1647: 131440041984.0000\n","Training loss (for one batch) at step 1648: 5269419982848.0000\n","Training loss (for one batch) at step 1649: 24105347072.0000\n","Training loss (for one batch) at step 1650: 22295015424.0000\n","Training loss (for one batch) at step 1651: 5155159040.0000\n","Training loss (for one batch) at step 1652: 17804234752.0000\n","Training loss (for one batch) at step 1653: 4818441216.0000\n","Training loss (for one batch) at step 1654: 71894614016.0000\n","Training loss (for one batch) at step 1655: 2335670528.0000\n","Training loss (for one batch) at step 1656: 13615110144.0000\n","Training loss (for one batch) at step 1657: 26004035584.0000\n","Training loss (for one batch) at step 1658: 41631539200.0000\n","Training loss (for one batch) at step 1659: 96988004352.0000\n","Training loss (for one batch) at step 1660: 205525090304.0000\n","Training loss (for one batch) at step 1661: 3032089856.0000\n","Training loss (for one batch) at step 1662: 44794146816.0000\n","Training loss (for one batch) at step 1663: 8230966272.0000\n","Training loss (for one batch) at step 1664: 30661783552.0000\n","Training loss (for one batch) at step 1665: 79338553344.0000\n","Training loss (for one batch) at step 1666: 720558161920.0000\n","Training loss (for one batch) at step 1667: 24279764992.0000\n","Training loss (for one batch) at step 1668: 615039959040.0000\n","Training loss (for one batch) at step 1669: 7245097984.0000\n","Training loss (for one batch) at step 1670: 10853249024.0000\n","Training loss (for one batch) at step 1671: 46016638976.0000\n","Training loss (for one batch) at step 1672: 51638382592.0000\n","Training loss (for one batch) at step 1673: 22606610432.0000\n","Training loss (for one batch) at step 1674: 26097528832.0000\n","Training loss (for one batch) at step 1675: 6479580160.0000\n","Training loss (for one batch) at step 1676: 2099427584.0000\n","Training loss (for one batch) at step 1677: 165546393600.0000\n","Training loss (for one batch) at step 1678: 60989566976.0000\n","Training loss (for one batch) at step 1679: 45130477568.0000\n","Training loss (for one batch) at step 1680: 442104938496.0000\n","Training loss (for one batch) at step 1681: 3396348416.0000\n","Training loss (for one batch) at step 1682: 4911579648.0000\n","Training loss (for one batch) at step 1683: 230422806528.0000\n","Training loss (for one batch) at step 1684: 45410115584.0000\n","Training loss (for one batch) at step 1685: 15881126912.0000\n","Training loss (for one batch) at step 1686: 505974915072.0000\n","Training loss (for one batch) at step 1687: 76093054976.0000\n","Training loss (for one batch) at step 1688: 5092201472.0000\n","Training loss (for one batch) at step 1689: 3480952576.0000\n","Training loss (for one batch) at step 1690: 1715255040.0000\n","Training loss (for one batch) at step 1691: 14068291584.0000\n","Training loss (for one batch) at step 1692: 222624088064.0000\n","Training loss (for one batch) at step 1693: 2012267413504.0000\n","\n","Start of epoch 47\n","Training loss (for one batch) at step 0: 38505881600.0000\n","Training loss (for one batch) at step 1: 32023355392.0000\n","Training loss (for one batch) at step 2: 48922267648.0000\n","Training loss (for one batch) at step 3: 19964786688.0000\n","Training loss (for one batch) at step 4: 5180250624.0000\n","Training loss (for one batch) at step 5: 358014386176.0000\n","Training loss (for one batch) at step 6: 18009589760.0000\n","Training loss (for one batch) at step 7: 275750289408.0000\n","Training loss (for one batch) at step 8: 5603191808.0000\n","Training loss (for one batch) at step 9: 8055739392.0000\n","Training loss (for one batch) at step 10: 54342529024.0000\n","Training loss (for one batch) at step 11: 49176473600.0000\n","Training loss (for one batch) at step 12: 5871377408.0000\n","Training loss (for one batch) at step 13: 5159552512.0000\n","Training loss (for one batch) at step 14: 35395158016.0000\n","Training loss (for one batch) at step 15: 19316682752.0000\n","Training loss (for one batch) at step 16: 30632347648.0000\n","Training loss (for one batch) at step 17: 5133726208.0000\n","Training loss (for one batch) at step 18: 197584945152.0000\n","Training loss (for one batch) at step 19: 171294310400.0000\n","Training loss (for one batch) at step 20: 18112710656.0000\n","Training loss (for one batch) at step 21: 3477987840.0000\n","Training loss (for one batch) at step 22: 294617841664.0000\n","Training loss (for one batch) at step 23: 20388536320.0000\n","Training loss (for one batch) at step 24: 85941231616.0000\n","Training loss (for one batch) at step 25: 4155348736.0000\n","Training loss (for one batch) at step 26: 988927885312.0000\n","Training loss (for one batch) at step 27: 98552029184.0000\n","Training loss (for one batch) at step 28: 134947020800.0000\n","Training loss (for one batch) at step 29: 16718826496.0000\n","Training loss (for one batch) at step 30: 128179568640.0000\n","Training loss (for one batch) at step 31: 125552050176.0000\n","Training loss (for one batch) at step 32: 451533537280.0000\n","Training loss (for one batch) at step 33: 3461523701760.0000\n","Training loss (for one batch) at step 34: 28837900288.0000\n","Training loss (for one batch) at step 35: 4628152832.0000\n","Training loss (for one batch) at step 36: 56697835520.0000\n","Training loss (for one batch) at step 37: 20649476096.0000\n","Training loss (for one batch) at step 38: 3201801728.0000\n","Training loss (for one batch) at step 39: 75980972032.0000\n","Training loss (for one batch) at step 40: 3460900864.0000\n","Training loss (for one batch) at step 41: 6030956544.0000\n","Training loss (for one batch) at step 42: 163378839552.0000\n","Training loss (for one batch) at step 43: 1360873088.0000\n","Training loss (for one batch) at step 44: 10667675648.0000\n","Training loss (for one batch) at step 45: 10175667200.0000\n","Training loss (for one batch) at step 46: 611366600704.0000\n","Training loss (for one batch) at step 47: 259441623040.0000\n","Training loss (for one batch) at step 48: 35490336768.0000\n","Training loss (for one batch) at step 49: 16628009984.0000\n","Training loss (for one batch) at step 50: 811558371328.0000\n","Training loss (for one batch) at step 51: 39466827776.0000\n","Training loss (for one batch) at step 52: 16849430528.0000\n","Training loss (for one batch) at step 53: 216469045248.0000\n","Training loss (for one batch) at step 54: 202765828096.0000\n","Training loss (for one batch) at step 55: 35527131136.0000\n","Training loss (for one batch) at step 56: 221350379520.0000\n","Training loss (for one batch) at step 57: 5218505728.0000\n","Training loss (for one batch) at step 58: 10774120448.0000\n","Training loss (for one batch) at step 59: 4215313408.0000\n","Training loss (for one batch) at step 60: 42478616576.0000\n","Training loss (for one batch) at step 61: 92873572352.0000\n","Training loss (for one batch) at step 62: 89125560320.0000\n","Training loss (for one batch) at step 63: 21493438464.0000\n","Training loss (for one batch) at step 64: 7632557056.0000\n","Training loss (for one batch) at step 65: 22193467392.0000\n","Training loss (for one batch) at step 66: 511032480.0000\n","Training loss (for one batch) at step 67: 359772028928.0000\n","Training loss (for one batch) at step 68: 202788667392.0000\n","Training loss (for one batch) at step 69: 195709485056.0000\n","Training loss (for one batch) at step 70: 822407856128.0000\n","Training loss (for one batch) at step 71: 955132215296.0000\n","Training loss (for one batch) at step 72: 6455595520.0000\n","Training loss (for one batch) at step 73: 24531658752.0000\n","Training loss (for one batch) at step 74: 22017560576.0000\n","Training loss (for one batch) at step 75: 70676021248.0000\n","Training loss (for one batch) at step 76: 126046527488.0000\n","Training loss (for one batch) at step 77: 5809105408.0000\n","Training loss (for one batch) at step 78: 11011721216.0000\n","Training loss (for one batch) at step 79: 39393411072.0000\n","Training loss (for one batch) at step 80: 728396800.0000\n","Training loss (for one batch) at step 81: 3636306432.0000\n","Training loss (for one batch) at step 82: 33834151936.0000\n","Training loss (for one batch) at step 83: 400214523904.0000\n","Training loss (for one batch) at step 84: 14278393856.0000\n","Training loss (for one batch) at step 85: 5966790144.0000\n","Training loss (for one batch) at step 86: 772232314880.0000\n","Training loss (for one batch) at step 87: 1619989376.0000\n","Training loss (for one batch) at step 88: 192153812992.0000\n","Training loss (for one batch) at step 89: 183130193920.0000\n","Training loss (for one batch) at step 90: 208204496896.0000\n","Training loss (for one batch) at step 91: 11183428608.0000\n","Training loss (for one batch) at step 92: 16752937984.0000\n","Training loss (for one batch) at step 93: 145033101312.0000\n","Training loss (for one batch) at step 94: 6546263552.0000\n","Training loss (for one batch) at step 95: 279627661312.0000\n","Training loss (for one batch) at step 96: 213419245568.0000\n","Training loss (for one batch) at step 97: 692724170752.0000\n","Training loss (for one batch) at step 98: 12211991552.0000\n","Training loss (for one batch) at step 99: 12281272320.0000\n","Training loss (for one batch) at step 100: 58890047488.0000\n","Training loss (for one batch) at step 101: 60898951168.0000\n","Training loss (for one batch) at step 102: 13415030784.0000\n","Training loss (for one batch) at step 103: 37517471744.0000\n","Training loss (for one batch) at step 104: 109036830720.0000\n","Training loss (for one batch) at step 105: 5203990016.0000\n","Training loss (for one batch) at step 106: 957391898148864.0000\n","Training loss (for one batch) at step 107: 9967201280.0000\n","Training loss (for one batch) at step 108: 26708713472.0000\n","Training loss (for one batch) at step 109: 13590781952.0000\n","Training loss (for one batch) at step 110: 20966688768.0000\n","Training loss (for one batch) at step 111: 87367630848.0000\n","Training loss (for one batch) at step 112: 192441008128.0000\n","Training loss (for one batch) at step 113: 45613105152.0000\n","Training loss (for one batch) at step 114: 9282384896.0000\n","Training loss (for one batch) at step 115: 158278385664.0000\n","Training loss (for one batch) at step 116: 153340051456.0000\n","Training loss (for one batch) at step 117: 17540214784.0000\n","Training loss (for one batch) at step 118: 8658185216.0000\n","Training loss (for one batch) at step 119: 11330231296.0000\n","Training loss (for one batch) at step 120: 12761106432.0000\n","Training loss (for one batch) at step 121: 130064891904.0000\n","Training loss (for one batch) at step 122: 200325627904.0000\n","Training loss (for one batch) at step 123: 57439559680.0000\n","Training loss (for one batch) at step 124: 18032046080.0000\n","Training loss (for one batch) at step 125: 18596904960.0000\n","Training loss (for one batch) at step 126: 271143206912.0000\n","Training loss (for one batch) at step 127: 28673638400.0000\n","Training loss (for one batch) at step 128: 155983642624.0000\n","Training loss (for one batch) at step 129: 5848253952.0000\n","Training loss (for one batch) at step 130: 10100277248.0000\n","Training loss (for one batch) at step 131: 5639738368.0000\n","Training loss (for one batch) at step 132: 92594864128.0000\n","Training loss (for one batch) at step 133: 10751191040.0000\n","Training loss (for one batch) at step 134: 33675364352.0000\n","Training loss (for one batch) at step 135: 590073495552.0000\n","Training loss (for one batch) at step 136: 54021963776.0000\n","Training loss (for one batch) at step 137: 126074912768.0000\n","Training loss (for one batch) at step 138: 2165410365440.0000\n","Training loss (for one batch) at step 139: 33457469440.0000\n","Training loss (for one batch) at step 140: 6296136704.0000\n","Training loss (for one batch) at step 141: 504792809472.0000\n","Training loss (for one batch) at step 142: 11491427328.0000\n","Training loss (for one batch) at step 143: 12465508352.0000\n","Training loss (for one batch) at step 144: 72810774528.0000\n","Training loss (for one batch) at step 145: 17728970752.0000\n","Training loss (for one batch) at step 146: 8799843328.0000\n","Training loss (for one batch) at step 147: 155695104000.0000\n","Training loss (for one batch) at step 148: 199164575744.0000\n","Training loss (for one batch) at step 149: 23351164928.0000\n","Training loss (for one batch) at step 150: 16738349056.0000\n","Training loss (for one batch) at step 151: 11958069248.0000\n","Training loss (for one batch) at step 152: 8995600384.0000\n","Training loss (for one batch) at step 153: 7687695360.0000\n","Training loss (for one batch) at step 154: 39980642304.0000\n","Training loss (for one batch) at step 155: 9066919936.0000\n","Training loss (for one batch) at step 156: 55848378368.0000\n","Training loss (for one batch) at step 157: 2895464960.0000\n","Training loss (for one batch) at step 158: 91288674304.0000\n","Training loss (for one batch) at step 159: 4681788416.0000\n","Training loss (for one batch) at step 160: 167441416192.0000\n","Training loss (for one batch) at step 161: 13552722944.0000\n","Training loss (for one batch) at step 162: 122517585920.0000\n","Training loss (for one batch) at step 163: 55079194624.0000\n","Training loss (for one batch) at step 164: 23395913728.0000\n","Training loss (for one batch) at step 165: 111122055168.0000\n","Training loss (for one batch) at step 166: 236625362944.0000\n","Training loss (for one batch) at step 167: 11026319360.0000\n","Training loss (for one batch) at step 168: 91448082432.0000\n","Training loss (for one batch) at step 169: 309217624064.0000\n","Training loss (for one batch) at step 170: 191228870656.0000\n","Training loss (for one batch) at step 171: 10334666752.0000\n","Training loss (for one batch) at step 172: 436119437312.0000\n","Training loss (for one batch) at step 173: 555971641344.0000\n","Training loss (for one batch) at step 174: 4130171136.0000\n","Training loss (for one batch) at step 175: 21779914752.0000\n","Training loss (for one batch) at step 176: 18685360128.0000\n","Training loss (for one batch) at step 177: 60539461632.0000\n","Training loss (for one batch) at step 178: 1432728576.0000\n","Training loss (for one batch) at step 179: 1306362624.0000\n","Training loss (for one batch) at step 180: 110429618176.0000\n","Training loss (for one batch) at step 181: 58836234240.0000\n","Training loss (for one batch) at step 182: 4087644160.0000\n","Training loss (for one batch) at step 183: 117102018560.0000\n","Training loss (for one batch) at step 184: 9831446528.0000\n","Training loss (for one batch) at step 185: 79178317824.0000\n","Training loss (for one batch) at step 186: 332949651456.0000\n","Training loss (for one batch) at step 187: 69342715904.0000\n","Training loss (for one batch) at step 188: 326705414144.0000\n","Training loss (for one batch) at step 189: 35932745728.0000\n","Training loss (for one batch) at step 190: 208862363648.0000\n","Training loss (for one batch) at step 191: 4194513408.0000\n","Training loss (for one batch) at step 192: 181317451776.0000\n","Training loss (for one batch) at step 193: 3395024640.0000\n","Training loss (for one batch) at step 194: 841887318016.0000\n","Training loss (for one batch) at step 195: 267373281280.0000\n","Training loss (for one batch) at step 196: 10314948608.0000\n","Training loss (for one batch) at step 197: 73409552384.0000\n","Training loss (for one batch) at step 198: 2811350272.0000\n","Training loss (for one batch) at step 199: 272799531008.0000\n","Training loss (for one batch) at step 200: 13351139328.0000\n","Training loss (for one batch) at step 201: 39947931648.0000\n","Training loss (for one batch) at step 202: 47730860032.0000\n","Training loss (for one batch) at step 203: 14108265472.0000\n","Training loss (for one batch) at step 204: 11424135168.0000\n","Training loss (for one batch) at step 205: 726895493120.0000\n","Training loss (for one batch) at step 206: 27631345664.0000\n","Training loss (for one batch) at step 207: 21208023040.0000\n","Training loss (for one batch) at step 208: 68105666560.0000\n","Training loss (for one batch) at step 209: 6699896832.0000\n","Training loss (for one batch) at step 210: 244876705792.0000\n","Training loss (for one batch) at step 211: 3928050176.0000\n","Training loss (for one batch) at step 212: 50603618304.0000\n","Training loss (for one batch) at step 213: 26443345920.0000\n","Training loss (for one batch) at step 214: 6502315520.0000\n","Training loss (for one batch) at step 215: 3185520148480.0000\n","Training loss (for one batch) at step 216: 3830474240.0000\n","Training loss (for one batch) at step 217: 1176947328.0000\n","Training loss (for one batch) at step 218: 3734962432.0000\n","Training loss (for one batch) at step 219: 30627549184.0000\n","Training loss (for one batch) at step 220: 858880540672.0000\n","Training loss (for one batch) at step 221: 3583351552.0000\n","Training loss (for one batch) at step 222: 15325188096.0000\n","Training loss (for one batch) at step 223: 19152244736.0000\n","Training loss (for one batch) at step 224: 35645526016.0000\n","Training loss (for one batch) at step 225: 262682787840.0000\n","Training loss (for one batch) at step 226: 4544425472.0000\n","Training loss (for one batch) at step 227: 256037109760.0000\n","Training loss (for one batch) at step 228: 2800890112.0000\n","Training loss (for one batch) at step 229: 18487613440.0000\n","Training loss (for one batch) at step 230: 6013437952.0000\n","Training loss (for one batch) at step 231: 79845703680.0000\n","Training loss (for one batch) at step 232: 21777772544.0000\n","Training loss (for one batch) at step 233: 218500890624.0000\n","Training loss (for one batch) at step 234: 38538928128.0000\n","Training loss (for one batch) at step 235: 6463339520.0000\n","Training loss (for one batch) at step 236: 5805617664.0000\n","Training loss (for one batch) at step 237: 183879024640.0000\n","Training loss (for one batch) at step 238: 182408904704.0000\n","Training loss (for one batch) at step 239: 87334289408.0000\n","Training loss (for one batch) at step 240: 528926441472.0000\n","Training loss (for one batch) at step 241: 12974313472.0000\n","Training loss (for one batch) at step 242: 30607935488.0000\n","Training loss (for one batch) at step 243: 3782213632.0000\n","Training loss (for one batch) at step 244: 1417940608.0000\n","Training loss (for one batch) at step 245: 29926502400.0000\n","Training loss (for one batch) at step 246: 4842899968.0000\n","Training loss (for one batch) at step 247: 183407050752.0000\n","Training loss (for one batch) at step 248: 120901672960.0000\n","Training loss (for one batch) at step 249: 125765861376.0000\n","Training loss (for one batch) at step 250: 128622493696.0000\n","Training loss (for one batch) at step 251: 39782572032.0000\n","Training loss (for one batch) at step 252: 13433910272.0000\n","Training loss (for one batch) at step 253: 27987171328.0000\n","Training loss (for one batch) at step 254: 16117405696.0000\n","Training loss (for one batch) at step 255: 38557335552.0000\n","Training loss (for one batch) at step 256: 106171514880.0000\n","Training loss (for one batch) at step 257: 18127009792.0000\n","Training loss (for one batch) at step 258: 14305654784.0000\n","Training loss (for one batch) at step 259: 15761724416.0000\n","Training loss (for one batch) at step 260: 25148133376.0000\n","Training loss (for one batch) at step 261: 150709485568.0000\n","Training loss (for one batch) at step 262: 12204524544.0000\n","Training loss (for one batch) at step 263: 392926298112.0000\n","Training loss (for one batch) at step 264: 310809952256.0000\n","Training loss (for one batch) at step 265: 15234195456.0000\n","Training loss (for one batch) at step 266: 49441828864.0000\n","Training loss (for one batch) at step 267: 16737016832.0000\n","Training loss (for one batch) at step 268: 25401962496.0000\n","Training loss (for one batch) at step 269: 70213222400.0000\n","Training loss (for one batch) at step 270: 5812187136.0000\n","Training loss (for one batch) at step 271: 202536402944.0000\n","Training loss (for one batch) at step 272: 4510138368.0000\n","Training loss (for one batch) at step 273: 163675865088.0000\n","Training loss (for one batch) at step 274: 41198682112.0000\n","Training loss (for one batch) at step 275: 39747842048.0000\n","Training loss (for one batch) at step 276: 123233574912.0000\n","Training loss (for one batch) at step 277: 17662881792.0000\n","Training loss (for one batch) at step 278: 132090232832.0000\n","Training loss (for one batch) at step 279: 154514522112.0000\n","Training loss (for one batch) at step 280: 19587332096.0000\n","Training loss (for one batch) at step 281: 10857289728.0000\n","Training loss (for one batch) at step 282: 27340525568.0000\n","Training loss (for one batch) at step 283: 286988664832.0000\n","Training loss (for one batch) at step 284: 28789530624.0000\n","Training loss (for one batch) at step 285: 13485576192.0000\n","Training loss (for one batch) at step 286: 374355427328.0000\n","Training loss (for one batch) at step 287: 6079268352.0000\n","Training loss (for one batch) at step 288: 131761913856.0000\n","Training loss (for one batch) at step 289: 490884399104.0000\n","Training loss (for one batch) at step 290: 10251307008.0000\n","Training loss (for one batch) at step 291: 322847801344.0000\n","Training loss (for one batch) at step 292: 35191812096.0000\n","Training loss (for one batch) at step 293: 1360131915776.0000\n","Training loss (for one batch) at step 294: 3551705600.0000\n","Training loss (for one batch) at step 295: 375954112512.0000\n","Training loss (for one batch) at step 296: 39622303744.0000\n","Training loss (for one batch) at step 297: 285340958720.0000\n","Training loss (for one batch) at step 298: 87013072896.0000\n","Training loss (for one batch) at step 299: 10158673920.0000\n","Training loss (for one batch) at step 300: 532470988800.0000\n","Training loss (for one batch) at step 301: 39151230976.0000\n","Training loss (for one batch) at step 302: 36786860032.0000\n","Training loss (for one batch) at step 303: 22778159104.0000\n","Training loss (for one batch) at step 304: 11430061056.0000\n","Training loss (for one batch) at step 305: 4352569344.0000\n","Training loss (for one batch) at step 306: 506539376640.0000\n","Training loss (for one batch) at step 307: 17866129408.0000\n","Training loss (for one batch) at step 308: 24797405184.0000\n","Training loss (for one batch) at step 309: 16212568064.0000\n","Training loss (for one batch) at step 310: 3106422016.0000\n","Training loss (for one batch) at step 311: 3235808256.0000\n","Training loss (for one batch) at step 312: 43356135424.0000\n","Training loss (for one batch) at step 313: 67896274944.0000\n","Training loss (for one batch) at step 314: 3894336552960.0000\n","Training loss (for one batch) at step 315: 516407394304.0000\n","Training loss (for one batch) at step 316: 3218488688640.0000\n","Training loss (for one batch) at step 317: 30693685248.0000\n","Training loss (for one batch) at step 318: 8010177024.0000\n","Training loss (for one batch) at step 319: 190419501056.0000\n","Training loss (for one batch) at step 320: 2820036096.0000\n","Training loss (for one batch) at step 321: 11368047616.0000\n","Training loss (for one batch) at step 322: 2269104896.0000\n","Training loss (for one batch) at step 323: 11320764416.0000\n","Training loss (for one batch) at step 324: 10465047552.0000\n","Training loss (for one batch) at step 325: 12275863552.0000\n","Training loss (for one batch) at step 326: 6040995840.0000\n","Training loss (for one batch) at step 327: 8134096896.0000\n","Training loss (for one batch) at step 328: 1914735616.0000\n","Training loss (for one batch) at step 329: 33015183360.0000\n","Training loss (for one batch) at step 330: 335179677696.0000\n","Training loss (for one batch) at step 331: 9343778816.0000\n","Training loss (for one batch) at step 332: 156640477184.0000\n","Training loss (for one batch) at step 333: 179915866112.0000\n","Training loss (for one batch) at step 334: 315834105856.0000\n","Training loss (for one batch) at step 335: 271659024384.0000\n","Training loss (for one batch) at step 336: 2869942026240.0000\n","Training loss (for one batch) at step 337: 59169386496.0000\n","Training loss (for one batch) at step 338: 43709583360.0000\n","Training loss (for one batch) at step 339: 34000091136.0000\n","Training loss (for one batch) at step 340: 31250851840.0000\n","Training loss (for one batch) at step 341: 42267516928.0000\n","Training loss (for one batch) at step 342: 21654581248.0000\n","Training loss (for one batch) at step 343: 5250938880.0000\n","Training loss (for one batch) at step 344: 5398732800.0000\n","Training loss (for one batch) at step 345: 59249037312.0000\n","Training loss (for one batch) at step 346: 11708003328.0000\n","Training loss (for one batch) at step 347: 102825541632.0000\n","Training loss (for one batch) at step 348: 16020867072.0000\n","Training loss (for one batch) at step 349: 42688946176.0000\n","Training loss (for one batch) at step 350: 899969792.0000\n","Training loss (for one batch) at step 351: 137738518528.0000\n","Training loss (for one batch) at step 352: 53667151872.0000\n","Training loss (for one batch) at step 353: 665402081280.0000\n","Training loss (for one batch) at step 354: 4472504320.0000\n","Training loss (for one batch) at step 355: 20258809856.0000\n","Training loss (for one batch) at step 356: 14766240768.0000\n","Training loss (for one batch) at step 357: 10810708992.0000\n","Training loss (for one batch) at step 358: 249416859648.0000\n","Training loss (for one batch) at step 359: 12600348672.0000\n","Training loss (for one batch) at step 360: 1017438080.0000\n","Training loss (for one batch) at step 361: 19548411904.0000\n","Training loss (for one batch) at step 362: 14805467136.0000\n","Training loss (for one batch) at step 363: 263829159936.0000\n","Training loss (for one batch) at step 364: 396832047104.0000\n","Training loss (for one batch) at step 365: 24167108608.0000\n","Training loss (for one batch) at step 366: 29162713088.0000\n","Training loss (for one batch) at step 367: 18135093248.0000\n","Training loss (for one batch) at step 368: 24515512320.0000\n","Training loss (for one batch) at step 369: 925073216.0000\n","Training loss (for one batch) at step 370: 7164474368.0000\n","Training loss (for one batch) at step 371: 29186967552.0000\n","Training loss (for one batch) at step 372: 72712445952.0000\n","Training loss (for one batch) at step 373: 6338332672.0000\n","Training loss (for one batch) at step 374: 3448061440.0000\n","Training loss (for one batch) at step 375: 263885357056.0000\n","Training loss (for one batch) at step 376: 22338881536.0000\n","Training loss (for one batch) at step 377: 6692361216.0000\n","Training loss (for one batch) at step 378: 38837977088.0000\n","Training loss (for one batch) at step 379: 69342576640.0000\n","Training loss (for one batch) at step 380: 8694387712.0000\n","Training loss (for one batch) at step 381: 8709369856.0000\n","Training loss (for one batch) at step 382: 132860469248.0000\n","Training loss (for one batch) at step 383: 12502779904.0000\n","Training loss (for one batch) at step 384: 3032872648704.0000\n","Training loss (for one batch) at step 385: 6424205312.0000\n","Training loss (for one batch) at step 386: 411694563328.0000\n","Training loss (for one batch) at step 387: 5574481920.0000\n","Training loss (for one batch) at step 388: 50594074624.0000\n","Training loss (for one batch) at step 389: 58699079680.0000\n","Training loss (for one batch) at step 390: 188104392704.0000\n","Training loss (for one batch) at step 391: 3106377728.0000\n","Training loss (for one batch) at step 392: 53141340160.0000\n","Training loss (for one batch) at step 393: 18595932160.0000\n","Training loss (for one batch) at step 394: 11357673472.0000\n","Training loss (for one batch) at step 395: 45245407232.0000\n","Training loss (for one batch) at step 396: 58819350528.0000\n","Training loss (for one batch) at step 397: 4872631808.0000\n","Training loss (for one batch) at step 398: 2840982016.0000\n","Training loss (for one batch) at step 399: 58078945280.0000\n","Training loss (for one batch) at step 400: 11268416512.0000\n","Training loss (for one batch) at step 401: 39419158528.0000\n","Training loss (for one batch) at step 402: 91741110272.0000\n","Training loss (for one batch) at step 403: 455047053312.0000\n","Training loss (for one batch) at step 404: 10114652160.0000\n","Training loss (for one batch) at step 405: 118954950656.0000\n","Training loss (for one batch) at step 406: 69379112960.0000\n","Training loss (for one batch) at step 407: 3721309696.0000\n","Training loss (for one batch) at step 408: 274016632832.0000\n","Training loss (for one batch) at step 409: 301110001664.0000\n","Training loss (for one batch) at step 410: 7511950848.0000\n","Training loss (for one batch) at step 411: 62950313984.0000\n","Training loss (for one batch) at step 412: 95404720128.0000\n","Training loss (for one batch) at step 413: 306027823104.0000\n","Training loss (for one batch) at step 414: 6620657664.0000\n","Training loss (for one batch) at step 415: 19328376832.0000\n","Training loss (for one batch) at step 416: 2723911424.0000\n","Training loss (for one batch) at step 417: 24182476800.0000\n","Training loss (for one batch) at step 418: 941244481536.0000\n","Training loss (for one batch) at step 419: 87107649536.0000\n","Training loss (for one batch) at step 420: 67579977728.0000\n","Training loss (for one batch) at step 421: 9186582528.0000\n","Training loss (for one batch) at step 422: 9596904448.0000\n","Training loss (for one batch) at step 423: 23815913472.0000\n","Training loss (for one batch) at step 424: 59805155328.0000\n","Training loss (for one batch) at step 425: 9260702720.0000\n","Training loss (for one batch) at step 426: 118882902016.0000\n","Training loss (for one batch) at step 427: 179320029184.0000\n","Training loss (for one batch) at step 428: 286706237440.0000\n","Training loss (for one batch) at step 429: 306693963776.0000\n","Training loss (for one batch) at step 430: 62786174976.0000\n","Training loss (for one batch) at step 431: 10745357312.0000\n","Training loss (for one batch) at step 432: 361483698176.0000\n","Training loss (for one batch) at step 433: 6997927424.0000\n","Training loss (for one batch) at step 434: 13041530880.0000\n","Training loss (for one batch) at step 435: 260622204928.0000\n","Training loss (for one batch) at step 436: 26671450112.0000\n","Training loss (for one batch) at step 437: 13818375168.0000\n","Training loss (for one batch) at step 438: 549548720128.0000\n","Training loss (for one batch) at step 439: 802034417664.0000\n","Training loss (for one batch) at step 440: 85760327680.0000\n","Training loss (for one batch) at step 441: 184220729344.0000\n","Training loss (for one batch) at step 442: 68824457216.0000\n","Training loss (for one batch) at step 443: 39113814016.0000\n","Training loss (for one batch) at step 444: 3707619328.0000\n","Training loss (for one batch) at step 445: 85894701056.0000\n","Training loss (for one batch) at step 446: 194184708096.0000\n","Training loss (for one batch) at step 447: 23474552832.0000\n","Training loss (for one batch) at step 448: 5716677632.0000\n","Training loss (for one batch) at step 449: 7354180608.0000\n","Training loss (for one batch) at step 450: 265454534656.0000\n","Training loss (for one batch) at step 451: 385670053888.0000\n","Training loss (for one batch) at step 452: 633062162432.0000\n","Training loss (for one batch) at step 453: 50042003456.0000\n","Training loss (for one batch) at step 454: 2638536769536.0000\n","Training loss (for one batch) at step 455: 70650896384.0000\n","Training loss (for one batch) at step 456: 24252395520.0000\n","Training loss (for one batch) at step 457: 77035421696.0000\n","Training loss (for one batch) at step 458: 29946525696.0000\n","Training loss (for one batch) at step 459: 32954083328.0000\n","Training loss (for one batch) at step 460: 292907057152.0000\n","Training loss (for one batch) at step 461: 4121719296.0000\n","Training loss (for one batch) at step 462: 79060115456.0000\n","Training loss (for one batch) at step 463: 16360987648.0000\n","Training loss (for one batch) at step 464: 9121746944.0000\n","Training loss (for one batch) at step 465: 18998337536.0000\n","Training loss (for one batch) at step 466: 6793194496.0000\n","Training loss (for one batch) at step 467: 5603673088.0000\n","Training loss (for one batch) at step 468: 195075227648.0000\n","Training loss (for one batch) at step 469: 49899773952.0000\n","Training loss (for one batch) at step 470: 46138011648.0000\n","Training loss (for one batch) at step 471: 92006432768.0000\n","Training loss (for one batch) at step 472: 9978225664.0000\n","Training loss (for one batch) at step 473: 7401142272.0000\n","Training loss (for one batch) at step 474: 55659610112.0000\n","Training loss (for one batch) at step 475: 557214990336.0000\n","Training loss (for one batch) at step 476: 15146127360.0000\n","Training loss (for one batch) at step 477: 29801857024.0000\n","Training loss (for one batch) at step 478: 22142732288.0000\n","Training loss (for one batch) at step 479: 336200990720.0000\n","Training loss (for one batch) at step 480: 7897748992.0000\n","Training loss (for one batch) at step 481: 27723411456.0000\n","Training loss (for one batch) at step 482: 20489261056.0000\n","Training loss (for one batch) at step 483: 48982888448.0000\n","Training loss (for one batch) at step 484: 216397103104.0000\n","Training loss (for one batch) at step 485: 1485251840.0000\n","Training loss (for one batch) at step 486: 2241498624.0000\n","Training loss (for one batch) at step 487: 164095737856.0000\n","Training loss (for one batch) at step 488: 5735104512.0000\n","Training loss (for one batch) at step 489: 73727508480.0000\n","Training loss (for one batch) at step 490: 85696446464.0000\n","Training loss (for one batch) at step 491: 82954993664.0000\n","Training loss (for one batch) at step 492: 4991032832.0000\n","Training loss (for one batch) at step 493: 10638029824.0000\n","Training loss (for one batch) at step 494: 7832990208.0000\n","Training loss (for one batch) at step 495: 27896969216.0000\n","Training loss (for one batch) at step 496: 282982744064.0000\n","Training loss (for one batch) at step 497: 48454307840.0000\n","Training loss (for one batch) at step 498: 10682363904.0000\n","Training loss (for one batch) at step 499: 92410216448.0000\n","Training loss (for one batch) at step 500: 8827374592.0000\n","Training loss (for one batch) at step 501: 3763168256.0000\n","Training loss (for one batch) at step 502: 34135932928.0000\n","Training loss (for one batch) at step 503: 20468916224.0000\n","Training loss (for one batch) at step 504: 3679855837184.0000\n","Training loss (for one batch) at step 505: 29222262784.0000\n","Training loss (for one batch) at step 506: 287283380224.0000\n","Training loss (for one batch) at step 507: 25300871168.0000\n","Training loss (for one batch) at step 508: 40539512832.0000\n","Training loss (for one batch) at step 509: 4382738432.0000\n","Training loss (for one batch) at step 510: 4230059520.0000\n","Training loss (for one batch) at step 511: 13743386624.0000\n","Training loss (for one batch) at step 512: 21167865856.0000\n","Training loss (for one batch) at step 513: 564918026240.0000\n","Training loss (for one batch) at step 514: 161770307584.0000\n","Training loss (for one batch) at step 515: 28039643136.0000\n","Training loss (for one batch) at step 516: 18405619712.0000\n","Training loss (for one batch) at step 517: 3099114496.0000\n","Training loss (for one batch) at step 518: 9054049280.0000\n","Training loss (for one batch) at step 519: 3615409664.0000\n","Training loss (for one batch) at step 520: 59733729280.0000\n","Training loss (for one batch) at step 521: 29934835712.0000\n","Training loss (for one batch) at step 522: 15288070144.0000\n","Training loss (for one batch) at step 523: 9493425152.0000\n","Training loss (for one batch) at step 524: 1909905536.0000\n","Training loss (for one batch) at step 525: 7939358720.0000\n","Training loss (for one batch) at step 526: 31559350272.0000\n","Training loss (for one batch) at step 527: 13132930048.0000\n","Training loss (for one batch) at step 528: 339856523264.0000\n","Training loss (for one batch) at step 529: 21609619456.0000\n","Training loss (for one batch) at step 530: 304217063424.0000\n","Training loss (for one batch) at step 531: 548270702592.0000\n","Training loss (for one batch) at step 532: 23172151296.0000\n","Training loss (for one batch) at step 533: 10922493952.0000\n","Training loss (for one batch) at step 534: 26532106240.0000\n","Training loss (for one batch) at step 535: 97978580992.0000\n","Training loss (for one batch) at step 536: 367367225344.0000\n","Training loss (for one batch) at step 537: 9305602048.0000\n","Training loss (for one batch) at step 538: 62201364480.0000\n","Training loss (for one batch) at step 539: 2858785280.0000\n","Training loss (for one batch) at step 540: 3685116416.0000\n","Training loss (for one batch) at step 541: 17103907840.0000\n","Training loss (for one batch) at step 542: 59535323136.0000\n","Training loss (for one batch) at step 543: 261770379264.0000\n","Training loss (for one batch) at step 544: 124287205376.0000\n","Training loss (for one batch) at step 545: 66747289600.0000\n","Training loss (for one batch) at step 546: 283366064128.0000\n","Training loss (for one batch) at step 547: 22807775232.0000\n","Training loss (for one batch) at step 548: 331229822976.0000\n","Training loss (for one batch) at step 549: 207562883072.0000\n","Training loss (for one batch) at step 550: 11232683008.0000\n","Training loss (for one batch) at step 551: 1729278208.0000\n","Training loss (for one batch) at step 552: 1023029120.0000\n","Training loss (for one batch) at step 553: 34529390592.0000\n","Training loss (for one batch) at step 554: 200637153280.0000\n","Training loss (for one batch) at step 555: 84869259264.0000\n","Training loss (for one batch) at step 556: 64736522240.0000\n","Training loss (for one batch) at step 557: 26464466944.0000\n","Training loss (for one batch) at step 558: 11941837824.0000\n","Training loss (for one batch) at step 559: 6000386048.0000\n","Training loss (for one batch) at step 560: 20129728512.0000\n","Training loss (for one batch) at step 561: 2375115776.0000\n","Training loss (for one batch) at step 562: 32837339136.0000\n","Training loss (for one batch) at step 563: 26794766336.0000\n","Training loss (for one batch) at step 564: 8480961536.0000\n","Training loss (for one batch) at step 565: 159396855808.0000\n","Training loss (for one batch) at step 566: 12352003072.0000\n","Training loss (for one batch) at step 567: 1187344547840.0000\n","Training loss (for one batch) at step 568: 6117160448.0000\n","Training loss (for one batch) at step 569: 22720923648.0000\n","Training loss (for one batch) at step 570: 6517913600.0000\n","Training loss (for one batch) at step 571: 222393335808.0000\n","Training loss (for one batch) at step 572: 264680177664.0000\n","Training loss (for one batch) at step 573: 2536272640.0000\n","Training loss (for one batch) at step 574: 35139407872.0000\n","Training loss (for one batch) at step 575: 11977180160.0000\n","Training loss (for one batch) at step 576: 61578903552.0000\n","Training loss (for one batch) at step 577: 119799726080.0000\n","Training loss (for one batch) at step 578: 12410778624.0000\n","Training loss (for one batch) at step 579: 81087045632.0000\n","Training loss (for one batch) at step 580: 81919574016.0000\n","Training loss (for one batch) at step 581: 65468944384.0000\n","Training loss (for one batch) at step 582: 16001853440.0000\n","Training loss (for one batch) at step 583: 13526116352.0000\n","Training loss (for one batch) at step 584: 18243538944.0000\n","Training loss (for one batch) at step 585: 44404547584.0000\n","Training loss (for one batch) at step 586: 589205209088.0000\n","Training loss (for one batch) at step 587: 20874000384.0000\n","Training loss (for one batch) at step 588: 180587872256.0000\n","Training loss (for one batch) at step 589: 20340123648.0000\n","Training loss (for one batch) at step 590: 249587302400.0000\n","Training loss (for one batch) at step 591: 36946677760.0000\n","Training loss (for one batch) at step 592: 243817332736.0000\n","Training loss (for one batch) at step 593: 7108673536.0000\n","Training loss (for one batch) at step 594: 32976957440.0000\n","Training loss (for one batch) at step 595: 493222461440.0000\n","Training loss (for one batch) at step 596: 1727844188160.0000\n","Training loss (for one batch) at step 597: 76630007808.0000\n","Training loss (for one batch) at step 598: 3656397824.0000\n","Training loss (for one batch) at step 599: 76443918336.0000\n","Training loss (for one batch) at step 600: 26074664960.0000\n","Training loss (for one batch) at step 601: 13960641536.0000\n","Training loss (for one batch) at step 602: 29497315328.0000\n","Training loss (for one batch) at step 603: 163605610496.0000\n","Training loss (for one batch) at step 604: 22814978048.0000\n","Training loss (for one batch) at step 605: 315264303104.0000\n","Training loss (for one batch) at step 606: 370968068096.0000\n","Training loss (for one batch) at step 607: 541885235200.0000\n","Training loss (for one batch) at step 608: 69593776128.0000\n","Training loss (for one batch) at step 609: 4883850752.0000\n","Training loss (for one batch) at step 610: 8861360128.0000\n","Training loss (for one batch) at step 611: 153092046848.0000\n","Training loss (for one batch) at step 612: 29030092800.0000\n","Training loss (for one batch) at step 613: 14346491904.0000\n","Training loss (for one batch) at step 614: 108568035328.0000\n","Training loss (for one batch) at step 615: 5933255680.0000\n","Training loss (for one batch) at step 616: 1288181383168.0000\n","Training loss (for one batch) at step 617: 158973001728.0000\n","Training loss (for one batch) at step 618: 669630857216.0000\n","Training loss (for one batch) at step 619: 28943437824.0000\n","Training loss (for one batch) at step 620: 17892474880.0000\n","Training loss (for one batch) at step 621: 736969228288.0000\n","Training loss (for one batch) at step 622: 6571976704.0000\n","Training loss (for one batch) at step 623: 3799458048.0000\n","Training loss (for one batch) at step 624: 2959650304.0000\n","Training loss (for one batch) at step 625: 21635475456.0000\n","Training loss (for one batch) at step 626: 58675331072.0000\n","Training loss (for one batch) at step 627: 2040527232.0000\n","Training loss (for one batch) at step 628: 4264424448.0000\n","Training loss (for one batch) at step 629: 4636324864.0000\n","Training loss (for one batch) at step 630: 8427619840.0000\n","Training loss (for one batch) at step 631: 92409896960.0000\n","Training loss (for one batch) at step 632: 483067756544.0000\n","Training loss (for one batch) at step 633: 776033796096.0000\n","Training loss (for one batch) at step 634: 115685834752.0000\n","Training loss (for one batch) at step 635: 12527860736.0000\n","Training loss (for one batch) at step 636: 138487889920.0000\n","Training loss (for one batch) at step 637: 3988436992.0000\n","Training loss (for one batch) at step 638: 24376295424.0000\n","Training loss (for one batch) at step 639: 15165262848.0000\n","Training loss (for one batch) at step 640: 95099756544.0000\n","Training loss (for one batch) at step 641: 961205888.0000\n","Training loss (for one batch) at step 642: 4389589504.0000\n","Training loss (for one batch) at step 643: 80959168512.0000\n","Training loss (for one batch) at step 644: 71396073472.0000\n","Training loss (for one batch) at step 645: 7607616000.0000\n","Training loss (for one batch) at step 646: 11131844608.0000\n","Training loss (for one batch) at step 647: 12894496768.0000\n","Training loss (for one batch) at step 648: 34172923904.0000\n","Training loss (for one batch) at step 649: 710504153088.0000\n","Training loss (for one batch) at step 650: 15708070912.0000\n","Training loss (for one batch) at step 651: 20943695872.0000\n","Training loss (for one batch) at step 652: 16730356736.0000\n","Training loss (for one batch) at step 653: 33013264384.0000\n","Training loss (for one batch) at step 654: 5916408320.0000\n","Training loss (for one batch) at step 655: 248810504192.0000\n","Training loss (for one batch) at step 656: 5353505280.0000\n","Training loss (for one batch) at step 657: 536750391296.0000\n","Training loss (for one batch) at step 658: 3228469248.0000\n","Training loss (for one batch) at step 659: 1841475200.0000\n","Training loss (for one batch) at step 660: 2949745920.0000\n","Training loss (for one batch) at step 661: 151059070976.0000\n","Training loss (for one batch) at step 662: 4212358400.0000\n","Training loss (for one batch) at step 663: 141474594816.0000\n","Training loss (for one batch) at step 664: 10287431680.0000\n","Training loss (for one batch) at step 665: 163329441792.0000\n","Training loss (for one batch) at step 666: 620142133248.0000\n","Training loss (for one batch) at step 667: 35356934144.0000\n","Training loss (for one batch) at step 668: 27899721728.0000\n","Training loss (for one batch) at step 669: 20161576960.0000\n","Training loss (for one batch) at step 670: 1441821184.0000\n","Training loss (for one batch) at step 671: 26617556992.0000\n","Training loss (for one batch) at step 672: 164070948864.0000\n","Training loss (for one batch) at step 673: 4900617728.0000\n","Training loss (for one batch) at step 674: 10829394944.0000\n","Training loss (for one batch) at step 675: 54399176704.0000\n","Training loss (for one batch) at step 676: 70174892032.0000\n","Training loss (for one batch) at step 677: 10124704768.0000\n","Training loss (for one batch) at step 678: 26612633600.0000\n","Training loss (for one batch) at step 679: 14348088320.0000\n","Training loss (for one batch) at step 680: 2751236864.0000\n","Training loss (for one batch) at step 681: 4914038784.0000\n","Training loss (for one batch) at step 682: 8811465728.0000\n","Training loss (for one batch) at step 683: 3721776896.0000\n","Training loss (for one batch) at step 684: 110536507392.0000\n","Training loss (for one batch) at step 685: 5166583296.0000\n","Training loss (for one batch) at step 686: 185247137792.0000\n","Training loss (for one batch) at step 687: 195359670272.0000\n","Training loss (for one batch) at step 688: 70522978304.0000\n","Training loss (for one batch) at step 689: 180562427904.0000\n","Training loss (for one batch) at step 690: 13600325632.0000\n","Training loss (for one batch) at step 691: 22881865728.0000\n","Training loss (for one batch) at step 692: 10369728512.0000\n","Training loss (for one batch) at step 693: 80754515968.0000\n","Training loss (for one batch) at step 694: 1146711703552.0000\n","Training loss (for one batch) at step 695: 19336769536.0000\n","Training loss (for one batch) at step 696: 4488995840.0000\n","Training loss (for one batch) at step 697: 2471163854848.0000\n","Training loss (for one batch) at step 698: 170834460672.0000\n","Training loss (for one batch) at step 699: 10539666432.0000\n","Training loss (for one batch) at step 700: 932184588288.0000\n","Training loss (for one batch) at step 701: 2915164928.0000\n","Training loss (for one batch) at step 702: 5657396736.0000\n","Training loss (for one batch) at step 703: 315831517184.0000\n","Training loss (for one batch) at step 704: 30256967680.0000\n","Training loss (for one batch) at step 705: 4205574656.0000\n","Training loss (for one batch) at step 706: 203458691072.0000\n","Training loss (for one batch) at step 707: 48503001088.0000\n","Training loss (for one batch) at step 708: 20309774336.0000\n","Training loss (for one batch) at step 709: 25067786240.0000\n","Training loss (for one batch) at step 710: 38756638720.0000\n","Training loss (for one batch) at step 711: 67748347904.0000\n","Training loss (for one batch) at step 712: 240794009600.0000\n","Training loss (for one batch) at step 713: 118121881600.0000\n","Training loss (for one batch) at step 714: 11645152256.0000\n","Training loss (for one batch) at step 715: 223178178560.0000\n","Training loss (for one batch) at step 716: 144761176064.0000\n","Training loss (for one batch) at step 717: 104037998592.0000\n","Training loss (for one batch) at step 718: 465057087488.0000\n","Training loss (for one batch) at step 719: 72055455744.0000\n","Training loss (for one batch) at step 720: 11846505472.0000\n","Training loss (for one batch) at step 721: 8795985920.0000\n","Training loss (for one batch) at step 722: 28317204480.0000\n","Training loss (for one batch) at step 723: 108265234432.0000\n","Training loss (for one batch) at step 724: 12788770816.0000\n","Training loss (for one batch) at step 725: 4900617728.0000\n","Training loss (for one batch) at step 726: 7030463488.0000\n","Training loss (for one batch) at step 727: 55147298816.0000\n","Training loss (for one batch) at step 728: 6587169280.0000\n","Training loss (for one batch) at step 729: 83143499776.0000\n","Training loss (for one batch) at step 730: 11232425984.0000\n","Training loss (for one batch) at step 731: 62169448448.0000\n","Training loss (for one batch) at step 732: 12254863360.0000\n","Training loss (for one batch) at step 733: 8515371008.0000\n","Training loss (for one batch) at step 734: 18333345792.0000\n","Training loss (for one batch) at step 735: 300350832640.0000\n","Training loss (for one batch) at step 736: 5675552768.0000\n","Training loss (for one batch) at step 737: 19855454208.0000\n","Training loss (for one batch) at step 738: 81707483136.0000\n","Training loss (for one batch) at step 739: 16210757632.0000\n","Training loss (for one batch) at step 740: 27277053952.0000\n","Training loss (for one batch) at step 741: 274109988864.0000\n","Training loss (for one batch) at step 742: 1335778344960.0000\n","Training loss (for one batch) at step 743: 292603232256.0000\n","Training loss (for one batch) at step 744: 152852725760.0000\n","Training loss (for one batch) at step 745: 198703677440.0000\n","Training loss (for one batch) at step 746: 8197069824.0000\n","Training loss (for one batch) at step 747: 4513776128.0000\n","Training loss (for one batch) at step 748: 28012832768.0000\n","Training loss (for one batch) at step 749: 25702768640.0000\n","Training loss (for one batch) at step 750: 76132343808.0000\n","Training loss (for one batch) at step 751: 154364559360.0000\n","Training loss (for one batch) at step 752: 4075252224.0000\n","Training loss (for one batch) at step 753: 6721643520.0000\n","Training loss (for one batch) at step 754: 17153126400.0000\n","Training loss (for one batch) at step 755: 14449718272.0000\n","Training loss (for one batch) at step 756: 9600399360.0000\n","Training loss (for one batch) at step 757: 801170849792.0000\n","Training loss (for one batch) at step 758: 62889844736.0000\n","Training loss (for one batch) at step 759: 38302068736.0000\n","Training loss (for one batch) at step 760: 141819625472.0000\n","Training loss (for one batch) at step 761: 12454045696.0000\n","Training loss (for one batch) at step 762: 10683883520.0000\n","Training loss (for one batch) at step 763: 22091763712.0000\n","Training loss (for one batch) at step 764: 5391370240.0000\n","Training loss (for one batch) at step 765: 30884321280.0000\n","Training loss (for one batch) at step 766: 18251620352.0000\n","Training loss (for one batch) at step 767: 570712768.0000\n","Training loss (for one batch) at step 768: 28009943040.0000\n","Training loss (for one batch) at step 769: 4552453632.0000\n","Training loss (for one batch) at step 770: 9746946048.0000\n","Training loss (for one batch) at step 771: 136845328384.0000\n","Training loss (for one batch) at step 772: 34332643328.0000\n","Training loss (for one batch) at step 773: 61509820416.0000\n","Training loss (for one batch) at step 774: 118432956416.0000\n","Training loss (for one batch) at step 775: 6947732480.0000\n","Training loss (for one batch) at step 776: 1351503970304.0000\n","Training loss (for one batch) at step 777: 24518414336.0000\n","Training loss (for one batch) at step 778: 1643135744.0000\n","Training loss (for one batch) at step 779: 202677256192.0000\n","Training loss (for one batch) at step 780: 76095856640.0000\n","Training loss (for one batch) at step 781: 319961595904.0000\n","Training loss (for one batch) at step 782: 3612787968.0000\n","Training loss (for one batch) at step 783: 5989071872.0000\n","Training loss (for one batch) at step 784: 16243621888.0000\n","Training loss (for one batch) at step 785: 11510942720.0000\n","Training loss (for one batch) at step 786: 218918682624.0000\n","Training loss (for one batch) at step 787: 48622411776.0000\n","Training loss (for one batch) at step 788: 85801091072.0000\n","Training loss (for one batch) at step 789: 209696243712.0000\n","Training loss (for one batch) at step 790: 211976093696.0000\n","Training loss (for one batch) at step 791: 9289074688.0000\n","Training loss (for one batch) at step 792: 52808933376.0000\n","Training loss (for one batch) at step 793: 30979708928.0000\n","Training loss (for one batch) at step 794: 5070469988352.0000\n","Training loss (for one batch) at step 795: 9788849152.0000\n","Training loss (for one batch) at step 796: 143774941184.0000\n","Training loss (for one batch) at step 797: 16688588800.0000\n","Training loss (for one batch) at step 798: 40012374016.0000\n","Training loss (for one batch) at step 799: 9545392128.0000\n","Training loss (for one batch) at step 800: 178337759232.0000\n","Training loss (for one batch) at step 801: 217216221184.0000\n","Training loss (for one batch) at step 802: 9813852160.0000\n","Training loss (for one batch) at step 803: 11798853632.0000\n","Training loss (for one batch) at step 804: 37546868736.0000\n","Training loss (for one batch) at step 805: 40043802624.0000\n","Training loss (for one batch) at step 806: 3730975232.0000\n","Training loss (for one batch) at step 807: 232977022976.0000\n","Training loss (for one batch) at step 808: 19194427392.0000\n","Training loss (for one batch) at step 809: 71554924544.0000\n","Training loss (for one batch) at step 810: 2965035008.0000\n","Training loss (for one batch) at step 811: 2209706672128.0000\n","Training loss (for one batch) at step 812: 38943399936.0000\n","Training loss (for one batch) at step 813: 3803446784.0000\n","Training loss (for one batch) at step 814: 5807588352.0000\n","Training loss (for one batch) at step 815: 9025357824.0000\n","Training loss (for one batch) at step 816: 1363122432.0000\n","Training loss (for one batch) at step 817: 212979957760.0000\n","Training loss (for one batch) at step 818: 40299634688.0000\n","Training loss (for one batch) at step 819: 15145524224.0000\n","Training loss (for one batch) at step 820: 2865425408.0000\n","Training loss (for one batch) at step 821: 24585752576.0000\n","Training loss (for one batch) at step 822: 4799831040.0000\n","Training loss (for one batch) at step 823: 184897028096.0000\n","Training loss (for one batch) at step 824: 29973610496.0000\n","Training loss (for one batch) at step 825: 3510522112.0000\n","Training loss (for one batch) at step 826: 314972241920.0000\n","Training loss (for one batch) at step 827: 17851080704.0000\n","Training loss (for one batch) at step 828: 30005239808.0000\n","Training loss (for one batch) at step 829: 6419397120.0000\n","Training loss (for one batch) at step 830: 341567176704.0000\n","Training loss (for one batch) at step 831: 1917914112.0000\n","Training loss (for one batch) at step 832: 85151866880.0000\n","Training loss (for one batch) at step 833: 7163701248.0000\n","Training loss (for one batch) at step 834: 5543363584.0000\n","Training loss (for one batch) at step 835: 157824532480.0000\n","Training loss (for one batch) at step 836: 41080082432.0000\n","Training loss (for one batch) at step 837: 403636551680.0000\n","Training loss (for one batch) at step 838: 6356455936.0000\n","Training loss (for one batch) at step 839: 52997603328.0000\n","Training loss (for one batch) at step 840: 7490162688.0000\n","Training loss (for one batch) at step 841: 37634949120.0000\n","Training loss (for one batch) at step 842: 47863517184.0000\n","Training loss (for one batch) at step 843: 60740026368.0000\n","Training loss (for one batch) at step 844: 226894561280.0000\n","Training loss (for one batch) at step 845: 168852078592.0000\n","Training loss (for one batch) at step 846: 14928102400.0000\n","Training loss (for one batch) at step 847: 530795724800.0000\n","Training loss (for one batch) at step 848: 3792033536.0000\n","Training loss (for one batch) at step 849: 113813962752.0000\n","Training loss (for one batch) at step 850: 225357660160.0000\n","Training loss (for one batch) at step 851: 1154728722432.0000\n","Training loss (for one batch) at step 852: 12456464384.0000\n","Training loss (for one batch) at step 853: 12098375680.0000\n","Training loss (for one batch) at step 854: 1068801916928.0000\n","Training loss (for one batch) at step 855: 9462447104.0000\n","Training loss (for one batch) at step 856: 221351198720.0000\n","Training loss (for one batch) at step 857: 2244666624.0000\n","Training loss (for one batch) at step 858: 4419507200.0000\n","Training loss (for one batch) at step 859: 13890833408.0000\n","Training loss (for one batch) at step 860: 162431893504.0000\n","Training loss (for one batch) at step 861: 280295964672.0000\n","Training loss (for one batch) at step 862: 7306577920.0000\n","Training loss (for one batch) at step 863: 342557229056.0000\n","Training loss (for one batch) at step 864: 284742156288.0000\n","Training loss (for one batch) at step 865: 77667614720.0000\n","Training loss (for one batch) at step 866: 295948124160.0000\n","Training loss (for one batch) at step 867: 12113659904.0000\n","Training loss (for one batch) at step 868: 139779817472.0000\n","Training loss (for one batch) at step 869: 25803880448.0000\n","Training loss (for one batch) at step 870: 442456309760.0000\n","Training loss (for one batch) at step 871: 577357873152.0000\n","Training loss (for one batch) at step 872: 174379384832.0000\n","Training loss (for one batch) at step 873: 1269762048.0000\n","Training loss (for one batch) at step 874: 86689906688.0000\n","Training loss (for one batch) at step 875: 175804383232.0000\n","Training loss (for one batch) at step 876: 7577249280.0000\n","Training loss (for one batch) at step 877: 183572774912.0000\n","Training loss (for one batch) at step 878: 700244754432.0000\n","Training loss (for one batch) at step 879: 67734585344.0000\n","Training loss (for one batch) at step 880: 3148091904.0000\n","Training loss (for one batch) at step 881: 18423312384.0000\n","Training loss (for one batch) at step 882: 4394202112.0000\n","Training loss (for one batch) at step 883: 10756829184.0000\n","Training loss (for one batch) at step 884: 43174346752.0000\n","Training loss (for one batch) at step 885: 7279606784.0000\n","Training loss (for one batch) at step 886: 16015719424.0000\n","Training loss (for one batch) at step 887: 169552510976.0000\n","Training loss (for one batch) at step 888: 1874131025920.0000\n","Training loss (for one batch) at step 889: 25224204288.0000\n","Training loss (for one batch) at step 890: 240731439104.0000\n","Training loss (for one batch) at step 891: 353614266368.0000\n","Training loss (for one batch) at step 892: 96962838528.0000\n","Training loss (for one batch) at step 893: 3442432278528.0000\n","Training loss (for one batch) at step 894: 8201632768.0000\n","Training loss (for one batch) at step 895: 364288081920.0000\n","Training loss (for one batch) at step 896: 113833623552.0000\n","Training loss (for one batch) at step 897: 299779588096.0000\n","Training loss (for one batch) at step 898: 941117210624.0000\n","Training loss (for one batch) at step 899: 382946836480.0000\n","Training loss (for one batch) at step 900: 416577257472.0000\n","Training loss (for one batch) at step 901: 197697880064.0000\n","Training loss (for one batch) at step 902: 8109392384.0000\n","Training loss (for one batch) at step 903: 27131295744.0000\n","Training loss (for one batch) at step 904: 16703748096.0000\n","Training loss (for one batch) at step 905: 2953675776.0000\n","Training loss (for one batch) at step 906: 2716114176.0000\n","Training loss (for one batch) at step 907: 64036765696.0000\n","Training loss (for one batch) at step 908: 225804615680.0000\n","Training loss (for one batch) at step 909: 45651673088.0000\n","Training loss (for one batch) at step 910: 6148509696.0000\n","Training loss (for one batch) at step 911: 13050983424.0000\n","Training loss (for one batch) at step 912: 19213772800.0000\n","Training loss (for one batch) at step 913: 92516966400.0000\n","Training loss (for one batch) at step 914: 18690981888.0000\n","Training loss (for one batch) at step 915: 160956235776.0000\n","Training loss (for one batch) at step 916: 9185915904.0000\n","Training loss (for one batch) at step 917: 577346142208.0000\n","Training loss (for one batch) at step 918: 19695185920.0000\n","Training loss (for one batch) at step 919: 3108432640.0000\n","Training loss (for one batch) at step 920: 10917794816.0000\n","Training loss (for one batch) at step 921: 151307337728.0000\n","Training loss (for one batch) at step 922: 7701916160.0000\n","Training loss (for one batch) at step 923: 799578718208.0000\n","Training loss (for one batch) at step 924: 4160832000.0000\n","Training loss (for one batch) at step 925: 14119329792.0000\n","Training loss (for one batch) at step 926: 18116009984.0000\n","Training loss (for one batch) at step 927: 93925163008.0000\n","Training loss (for one batch) at step 928: 7393104384.0000\n","Training loss (for one batch) at step 929: 28793915392.0000\n","Training loss (for one batch) at step 930: 6228023808.0000\n","Training loss (for one batch) at step 931: 23892631552.0000\n","Training loss (for one batch) at step 932: 6135025664.0000\n","Training loss (for one batch) at step 933: 2432660224.0000\n","Training loss (for one batch) at step 934: 19157983232.0000\n","Training loss (for one batch) at step 935: 15194088448.0000\n","Training loss (for one batch) at step 936: 4888012800.0000\n","Training loss (for one batch) at step 937: 779948392448.0000\n","Training loss (for one batch) at step 938: 310142894080.0000\n","Training loss (for one batch) at step 939: 533294972928.0000\n","Training loss (for one batch) at step 940: 2174857472.0000\n","Training loss (for one batch) at step 941: 28954077184.0000\n","Training loss (for one batch) at step 942: 9672513536.0000\n","Training loss (for one batch) at step 943: 11301954560.0000\n","Training loss (for one batch) at step 944: 534530293760.0000\n","Training loss (for one batch) at step 945: 44588695552.0000\n","Training loss (for one batch) at step 946: 101984722944.0000\n","Training loss (for one batch) at step 947: 261874679808.0000\n","Training loss (for one batch) at step 948: 262685540352.0000\n","Training loss (for one batch) at step 949: 89679765504.0000\n","Training loss (for one batch) at step 950: 94875836416.0000\n","Training loss (for one batch) at step 951: 4918464512.0000\n","Training loss (for one batch) at step 952: 6128995328.0000\n","Training loss (for one batch) at step 953: 245346877440.0000\n","Training loss (for one batch) at step 954: 154594738176.0000\n","Training loss (for one batch) at step 955: 4331673600.0000\n","Training loss (for one batch) at step 956: 4151839488.0000\n","Training loss (for one batch) at step 957: 253150871552.0000\n","Training loss (for one batch) at step 958: 12422860800.0000\n","Training loss (for one batch) at step 959: 189728571392.0000\n","Training loss (for one batch) at step 960: 75034894336.0000\n","Training loss (for one batch) at step 961: 15874302976.0000\n","Training loss (for one batch) at step 962: 191338987520.0000\n","Training loss (for one batch) at step 963: 102956351488.0000\n","Training loss (for one batch) at step 964: 4850732032.0000\n","Training loss (for one batch) at step 965: 59746983936.0000\n","Training loss (for one batch) at step 966: 205104840704.0000\n","Training loss (for one batch) at step 967: 32504598528.0000\n","Training loss (for one batch) at step 968: 45958189056.0000\n","Training loss (for one batch) at step 969: 18451558400.0000\n","Training loss (for one batch) at step 970: 3048156928.0000\n","Training loss (for one batch) at step 971: 98522906624.0000\n","Training loss (for one batch) at step 972: 28079468544.0000\n","Training loss (for one batch) at step 973: 3267342592.0000\n","Training loss (for one batch) at step 974: 1841272704.0000\n","Training loss (for one batch) at step 975: 40117239808.0000\n","Training loss (for one batch) at step 976: 16300589056.0000\n","Training loss (for one batch) at step 977: 777147056128.0000\n","Training loss (for one batch) at step 978: 20202684416.0000\n","Training loss (for one batch) at step 979: 11646734336.0000\n","Training loss (for one batch) at step 980: 73794191360.0000\n","Training loss (for one batch) at step 981: 1692429056.0000\n","Training loss (for one batch) at step 982: 81256595456.0000\n","Training loss (for one batch) at step 983: 2256912640.0000\n","Training loss (for one batch) at step 984: 5390941184.0000\n","Training loss (for one batch) at step 985: 3630240768.0000\n","Training loss (for one batch) at step 986: 43423006720.0000\n","Training loss (for one batch) at step 987: 433572773888.0000\n","Training loss (for one batch) at step 988: 16002451456.0000\n","Training loss (for one batch) at step 989: 211543408640.0000\n","Training loss (for one batch) at step 990: 217667452928.0000\n","Training loss (for one batch) at step 991: 105006153728.0000\n","Training loss (for one batch) at step 992: 55898533888.0000\n","Training loss (for one batch) at step 993: 4347434496.0000\n","Training loss (for one batch) at step 994: 7921624064.0000\n","Training loss (for one batch) at step 995: 3206693120.0000\n","Training loss (for one batch) at step 996: 128201252864.0000\n","Training loss (for one batch) at step 997: 7732073984.0000\n","Training loss (for one batch) at step 998: 408589271040.0000\n","Training loss (for one batch) at step 999: 14885274624.0000\n","Training loss (for one batch) at step 1000: 16671408128.0000\n","Training loss (for one batch) at step 1001: 6342898688.0000\n","Training loss (for one batch) at step 1002: 12412007424.0000\n","Training loss (for one batch) at step 1003: 1179079802880.0000\n","Training loss (for one batch) at step 1004: 7369911808.0000\n","Training loss (for one batch) at step 1005: 14355662848.0000\n","Training loss (for one batch) at step 1006: 46654320640.0000\n","Training loss (for one batch) at step 1007: 26290589696.0000\n","Training loss (for one batch) at step 1008: 18343516160.0000\n","Training loss (for one batch) at step 1009: 54318137344.0000\n","Training loss (for one batch) at step 1010: 15865445376.0000\n","Training loss (for one batch) at step 1011: 101399379968.0000\n","Training loss (for one batch) at step 1012: 12873016320.0000\n","Training loss (for one batch) at step 1013: 18532022272.0000\n","Training loss (for one batch) at step 1014: 26027108352.0000\n","Training loss (for one batch) at step 1015: 21014315008.0000\n","Training loss (for one batch) at step 1016: 61451370496.0000\n","Training loss (for one batch) at step 1017: 3607262208.0000\n","Training loss (for one batch) at step 1018: 70550945792.0000\n","Training loss (for one batch) at step 1019: 301757431808.0000\n","Training loss (for one batch) at step 1020: 186217234432.0000\n","Training loss (for one batch) at step 1021: 4713722880.0000\n","Training loss (for one batch) at step 1022: 438561046528.0000\n","Training loss (for one batch) at step 1023: 11746205696.0000\n","Training loss (for one batch) at step 1024: 3537532928.0000\n","Training loss (for one batch) at step 1025: 5453410816.0000\n","Training loss (for one batch) at step 1026: 23239782400.0000\n","Training loss (for one batch) at step 1027: 414717771776.0000\n","Training loss (for one batch) at step 1028: 112493821952.0000\n","Training loss (for one batch) at step 1029: 161566162944.0000\n","Training loss (for one batch) at step 1030: 37335552000.0000\n","Training loss (for one batch) at step 1031: 131210723328.0000\n","Training loss (for one batch) at step 1032: 4005657088.0000\n","Training loss (for one batch) at step 1033: 16459456512.0000\n","Training loss (for one batch) at step 1034: 8564272128.0000\n","Training loss (for one batch) at step 1035: 68922490880.0000\n","Training loss (for one batch) at step 1036: 48642019328.0000\n","Training loss (for one batch) at step 1037: 58240212992.0000\n","Training loss (for one batch) at step 1038: 301515931648.0000\n","Training loss (for one batch) at step 1039: 1256719515648.0000\n","Training loss (for one batch) at step 1040: 89859497984.0000\n","Training loss (for one batch) at step 1041: 14474222592.0000\n","Training loss (for one batch) at step 1042: 59791347712.0000\n","Training loss (for one batch) at step 1043: 315223506944.0000\n","Training loss (for one batch) at step 1044: 41179385856.0000\n","Training loss (for one batch) at step 1045: 2107392000.0000\n","Training loss (for one batch) at step 1046: 312372822016.0000\n","Training loss (for one batch) at step 1047: 61119229952.0000\n","Training loss (for one batch) at step 1048: 253050257408.0000\n","Training loss (for one batch) at step 1049: 54542929920.0000\n","Training loss (for one batch) at step 1050: 235192008704.0000\n","Training loss (for one batch) at step 1051: 8631088128.0000\n","Training loss (for one batch) at step 1052: 17853521920.0000\n","Training loss (for one batch) at step 1053: 1453575424.0000\n","Training loss (for one batch) at step 1054: 106135289856.0000\n","Training loss (for one batch) at step 1055: 15762805760.0000\n","Training loss (for one batch) at step 1056: 152656494592.0000\n","Training loss (for one batch) at step 1057: 177784520704.0000\n","Training loss (for one batch) at step 1058: 3565995520.0000\n","Training loss (for one batch) at step 1059: 18807779328.0000\n","Training loss (for one batch) at step 1060: 67962015744.0000\n","Training loss (for one batch) at step 1061: 146119819264.0000\n","Training loss (for one batch) at step 1062: 56699981824.0000\n","Training loss (for one batch) at step 1063: 20417789952.0000\n","Training loss (for one batch) at step 1064: 363357011968.0000\n","Training loss (for one batch) at step 1065: 1575980544.0000\n","Training loss (for one batch) at step 1066: 148623360000.0000\n","Training loss (for one batch) at step 1067: 114745147392.0000\n","Training loss (for one batch) at step 1068: 3233467858944.0000\n","Training loss (for one batch) at step 1069: 6527906304.0000\n","Training loss (for one batch) at step 1070: 30837039104.0000\n","Training loss (for one batch) at step 1071: 734911135744.0000\n","Training loss (for one batch) at step 1072: 135386365952.0000\n","Training loss (for one batch) at step 1073: 8501219840.0000\n","Training loss (for one batch) at step 1074: 41567490048.0000\n","Training loss (for one batch) at step 1075: 90682425344.0000\n","Training loss (for one batch) at step 1076: 4248286976.0000\n","Training loss (for one batch) at step 1077: 7906670592.0000\n","Training loss (for one batch) at step 1078: 6734108672.0000\n","Training loss (for one batch) at step 1079: 64443351040.0000\n","Training loss (for one batch) at step 1080: 604026372096.0000\n","Training loss (for one batch) at step 1081: 100568326144.0000\n","Training loss (for one batch) at step 1082: 1453187456.0000\n","Training loss (for one batch) at step 1083: 145631428608.0000\n","Training loss (for one batch) at step 1084: 51684319232.0000\n","Training loss (for one batch) at step 1085: 445963488.0000\n","Training loss (for one batch) at step 1086: 31095842816.0000\n","Training loss (for one batch) at step 1087: 111618097152.0000\n","Training loss (for one batch) at step 1088: 1762449536.0000\n","Training loss (for one batch) at step 1089: 231559888896.0000\n","Training loss (for one batch) at step 1090: 242541658112.0000\n","Training loss (for one batch) at step 1091: 20707479552.0000\n","Training loss (for one batch) at step 1092: 56622137344.0000\n","Training loss (for one batch) at step 1093: 205891846144.0000\n","Training loss (for one batch) at step 1094: 599322525696.0000\n","Training loss (for one batch) at step 1095: 75790032896.0000\n","Training loss (for one batch) at step 1096: 20269160.0000\n","Training loss (for one batch) at step 1097: 11915384832.0000\n","Training loss (for one batch) at step 1098: 30540206080.0000\n","Training loss (for one batch) at step 1099: 315412119552.0000\n","Training loss (for one batch) at step 1100: 21305225216.0000\n","Training loss (for one batch) at step 1101: 61910167552.0000\n","Training loss (for one batch) at step 1102: 35189944320.0000\n","Training loss (for one batch) at step 1103: 4795499520.0000\n","Training loss (for one batch) at step 1104: 13402712064.0000\n","Training loss (for one batch) at step 1105: 238189674496.0000\n","Training loss (for one batch) at step 1106: 14734497792.0000\n","Training loss (for one batch) at step 1107: 429994311680.0000\n","Training loss (for one batch) at step 1108: 22302939136.0000\n","Training loss (for one batch) at step 1109: 11635931136.0000\n","Training loss (for one batch) at step 1110: 13922124800.0000\n","Training loss (for one batch) at step 1111: 141077479424.0000\n","Training loss (for one batch) at step 1112: 26250272768.0000\n","Training loss (for one batch) at step 1113: 19812130816.0000\n","Training loss (for one batch) at step 1114: 131231784960.0000\n","Training loss (for one batch) at step 1115: 179665174528.0000\n","Training loss (for one batch) at step 1116: 2349438861312.0000\n","Training loss (for one batch) at step 1117: 11491289088.0000\n","Training loss (for one batch) at step 1118: 14038493184.0000\n","Training loss (for one batch) at step 1119: 5283615232.0000\n","Training loss (for one batch) at step 1120: 2196482293760.0000\n","Training loss (for one batch) at step 1121: 12228952064.0000\n","Training loss (for one batch) at step 1122: 3429470208.0000\n","Training loss (for one batch) at step 1123: 81671266304.0000\n","Training loss (for one batch) at step 1124: 13161919488.0000\n","Training loss (for one batch) at step 1125: 4752267264.0000\n","Training loss (for one batch) at step 1126: 8653123584.0000\n","Training loss (for one batch) at step 1127: 214894788608.0000\n","Training loss (for one batch) at step 1128: 3371525376.0000\n","Training loss (for one batch) at step 1129: 160803667968.0000\n","Training loss (for one batch) at step 1130: 95383863296.0000\n","Training loss (for one batch) at step 1131: 5722347520.0000\n","Training loss (for one batch) at step 1132: 146950619136.0000\n","Training loss (for one batch) at step 1133: 4905135616.0000\n","Training loss (for one batch) at step 1134: 820143915008.0000\n","Training loss (for one batch) at step 1135: 85964890112.0000\n","Training loss (for one batch) at step 1136: 1671616768.0000\n","Training loss (for one batch) at step 1137: 552300380160.0000\n","Training loss (for one batch) at step 1138: 96997548032.0000\n","Training loss (for one batch) at step 1139: 1523901399040.0000\n","Training loss (for one batch) at step 1140: 148030521344.0000\n","Training loss (for one batch) at step 1141: 22711894016.0000\n","Training loss (for one batch) at step 1142: 342104145920.0000\n","Training loss (for one batch) at step 1143: 9851871232.0000\n","Training loss (for one batch) at step 1144: 11886927872.0000\n","Training loss (for one batch) at step 1145: 298917003264.0000\n","Training loss (for one batch) at step 1146: 2857254656.0000\n","Training loss (for one batch) at step 1147: 20190228480.0000\n","Training loss (for one batch) at step 1148: 37101674496.0000\n","Training loss (for one batch) at step 1149: 800673408.0000\n","Training loss (for one batch) at step 1150: 28889718784.0000\n","Training loss (for one batch) at step 1151: 73213018112.0000\n","Training loss (for one batch) at step 1152: 115220824064.0000\n","Training loss (for one batch) at step 1153: 208747429888.0000\n","Training loss (for one batch) at step 1154: 61957939200.0000\n","Training loss (for one batch) at step 1155: 9359845376.0000\n","Training loss (for one batch) at step 1156: 11827281920.0000\n","Training loss (for one batch) at step 1157: 31201863680.0000\n","Training loss (for one batch) at step 1158: 5076846592.0000\n","Training loss (for one batch) at step 1159: 45291659264.0000\n","Training loss (for one batch) at step 1160: 263874379776.0000\n","Training loss (for one batch) at step 1161: 11152524288.0000\n","Training loss (for one batch) at step 1162: 67016896512.0000\n","Training loss (for one batch) at step 1163: 33337260032.0000\n","Training loss (for one batch) at step 1164: 27217317888.0000\n","Training loss (for one batch) at step 1165: 533901705216.0000\n","Training loss (for one batch) at step 1166: 1432521146368.0000\n","Training loss (for one batch) at step 1167: 26108192768.0000\n","Training loss (for one batch) at step 1168: 7202491392.0000\n","Training loss (for one batch) at step 1169: 35881922560.0000\n","Training loss (for one batch) at step 1170: 3101340672.0000\n","Training loss (for one batch) at step 1171: 66154917888.0000\n","Training loss (for one batch) at step 1172: 409533504.0000\n","Training loss (for one batch) at step 1173: 40320036864.0000\n","Training loss (for one batch) at step 1174: 53953130496.0000\n","Training loss (for one batch) at step 1175: 118152724480.0000\n","Training loss (for one batch) at step 1176: 1201622745088.0000\n","Training loss (for one batch) at step 1177: 162669712.0000\n","Training loss (for one batch) at step 1178: 6943688704.0000\n","Training loss (for one batch) at step 1179: 513787559936.0000\n","Training loss (for one batch) at step 1180: 57244516352.0000\n","Training loss (for one batch) at step 1181: 33426032640.0000\n","Training loss (for one batch) at step 1182: 25375191040.0000\n","Training loss (for one batch) at step 1183: 3694026752.0000\n","Training loss (for one batch) at step 1184: 247504224256.0000\n","Training loss (for one batch) at step 1185: 72601501696.0000\n","Training loss (for one batch) at step 1186: 271885811712.0000\n","Training loss (for one batch) at step 1187: 24052391936.0000\n","Training loss (for one batch) at step 1188: 96789217280.0000\n","Training loss (for one batch) at step 1189: 23867932672.0000\n","Training loss (for one batch) at step 1190: 358076317696.0000\n","Training loss (for one batch) at step 1191: 9770608640.0000\n","Training loss (for one batch) at step 1192: 106746249216.0000\n","Training loss (for one batch) at step 1193: 19133532160.0000\n","Training loss (for one batch) at step 1194: 6379544064.0000\n","Training loss (for one batch) at step 1195: 5449419587584.0000\n","Training loss (for one batch) at step 1196: 4347673600.0000\n","Training loss (for one batch) at step 1197: 77335650304.0000\n","Training loss (for one batch) at step 1198: 3249612800.0000\n","Training loss (for one batch) at step 1199: 230450282496.0000\n","Training loss (for one batch) at step 1200: 3720007680.0000\n","Training loss (for one batch) at step 1201: 22871601152.0000\n","Training loss (for one batch) at step 1202: 115479961600.0000\n","Training loss (for one batch) at step 1203: 492853854208.0000\n","Training loss (for one batch) at step 1204: 75734941696.0000\n","Training loss (for one batch) at step 1205: 6803212288.0000\n","Training loss (for one batch) at step 1206: 104475451392.0000\n","Training loss (for one batch) at step 1207: 10534647808.0000\n","Training loss (for one batch) at step 1208: 1152213760.0000\n","Training loss (for one batch) at step 1209: 154315292672.0000\n","Training loss (for one batch) at step 1210: 2513635840.0000\n","Training loss (for one batch) at step 1211: 67349696512.0000\n","Training loss (for one batch) at step 1212: 18380115968.0000\n","Training loss (for one batch) at step 1213: 246322790400.0000\n","Training loss (for one batch) at step 1214: 90093690880.0000\n","Training loss (for one batch) at step 1215: 123370881024.0000\n","Training loss (for one batch) at step 1216: 273469243392.0000\n","Training loss (for one batch) at step 1217: 19163918336.0000\n","Training loss (for one batch) at step 1218: 146592497664.0000\n","Training loss (for one batch) at step 1219: 19536074752.0000\n","Training loss (for one batch) at step 1220: 358864027648.0000\n","Training loss (for one batch) at step 1221: 7567273984.0000\n","Training loss (for one batch) at step 1222: 2940003840.0000\n","Training loss (for one batch) at step 1223: 302396243968.0000\n","Training loss (for one batch) at step 1224: 14306991104.0000\n","Training loss (for one batch) at step 1225: 6636579328.0000\n","Training loss (for one batch) at step 1226: 44084801536.0000\n","Training loss (for one batch) at step 1227: 6825369088.0000\n","Training loss (for one batch) at step 1228: 143217377280.0000\n","Training loss (for one batch) at step 1229: 14000205824.0000\n","Training loss (for one batch) at step 1230: 42238423040.0000\n","Training loss (for one batch) at step 1231: 1948548608.0000\n","Training loss (for one batch) at step 1232: 19151015936.0000\n","Training loss (for one batch) at step 1233: 13795745792.0000\n","Training loss (for one batch) at step 1234: 36057321472.0000\n","Training loss (for one batch) at step 1235: 20590964736.0000\n","Training loss (for one batch) at step 1236: 140946014208.0000\n","Training loss (for one batch) at step 1237: 7543417856.0000\n","Training loss (for one batch) at step 1238: 7211661312.0000\n","Training loss (for one batch) at step 1239: 9894981632.0000\n","Training loss (for one batch) at step 1240: 24228433920.0000\n","Training loss (for one batch) at step 1241: 134837149696.0000\n","Training loss (for one batch) at step 1242: 5527995392.0000\n","Training loss (for one batch) at step 1243: 96305774592.0000\n","Training loss (for one batch) at step 1244: 17520848896.0000\n","Training loss (for one batch) at step 1245: 46608355328.0000\n","Training loss (for one batch) at step 1246: 165715722240.0000\n","Training loss (for one batch) at step 1247: 68646236160.0000\n","Training loss (for one batch) at step 1248: 9063495680.0000\n","Training loss (for one batch) at step 1249: 32998928384.0000\n","Training loss (for one batch) at step 1250: 39798689792.0000\n","Training loss (for one batch) at step 1251: 3501440768.0000\n","Training loss (for one batch) at step 1252: 138319036416.0000\n","Training loss (for one batch) at step 1253: 8784780288.0000\n","Training loss (for one batch) at step 1254: 27367309312.0000\n","Training loss (for one batch) at step 1255: 4882037760.0000\n","Training loss (for one batch) at step 1256: 5637239808.0000\n","Training loss (for one batch) at step 1257: 86028025856.0000\n","Training loss (for one batch) at step 1258: 26382346240.0000\n","Training loss (for one batch) at step 1259: 1421240320.0000\n","Training loss (for one batch) at step 1260: 49891110912.0000\n","Training loss (for one batch) at step 1261: 30758189056.0000\n","Training loss (for one batch) at step 1262: 14833191936.0000\n","Training loss (for one batch) at step 1263: 60017700864.0000\n","Training loss (for one batch) at step 1264: 265150365696.0000\n","Training loss (for one batch) at step 1265: 155924119552.0000\n","Training loss (for one batch) at step 1266: 298370465792.0000\n","Training loss (for one batch) at step 1267: 17356029952.0000\n","Training loss (for one batch) at step 1268: 14213901312.0000\n","Training loss (for one batch) at step 1269: 7505082880.0000\n","Training loss (for one batch) at step 1270: 124761653248.0000\n","Training loss (for one batch) at step 1271: 185082576896.0000\n","Training loss (for one batch) at step 1272: 8081778688.0000\n","Training loss (for one batch) at step 1273: 7428813824.0000\n","Training loss (for one batch) at step 1274: 66349596672.0000\n","Training loss (for one batch) at step 1275: 69611520000.0000\n","Training loss (for one batch) at step 1276: 5610566656.0000\n","Training loss (for one batch) at step 1277: 3658196992.0000\n","Training loss (for one batch) at step 1278: 6436948992.0000\n","Training loss (for one batch) at step 1279: 12656398336.0000\n","Training loss (for one batch) at step 1280: 19836817408.0000\n","Training loss (for one batch) at step 1281: 11145286656.0000\n","Training loss (for one batch) at step 1282: 27984441344.0000\n","Training loss (for one batch) at step 1283: 4642193408.0000\n","Training loss (for one batch) at step 1284: 119001980928.0000\n","Training loss (for one batch) at step 1285: 194086027264.0000\n","Training loss (for one batch) at step 1286: 97358962688.0000\n","Training loss (for one batch) at step 1287: 213836185600.0000\n","Training loss (for one batch) at step 1288: 10267923456.0000\n","Training loss (for one batch) at step 1289: 158755307520.0000\n","Training loss (for one batch) at step 1290: 2204141879296.0000\n","Training loss (for one batch) at step 1291: 9378367488.0000\n","Training loss (for one batch) at step 1292: 128217137152.0000\n","Training loss (for one batch) at step 1293: 27621728256.0000\n","Training loss (for one batch) at step 1294: 5666903040.0000\n","Training loss (for one batch) at step 1295: 354235088896.0000\n","Training loss (for one batch) at step 1296: 8326488064.0000\n","Training loss (for one batch) at step 1297: 11142091776.0000\n","Training loss (for one batch) at step 1298: 1793517312.0000\n","Training loss (for one batch) at step 1299: 71068540928.0000\n","Training loss (for one batch) at step 1300: 231193673728.0000\n","Training loss (for one batch) at step 1301: 23267909632.0000\n","Training loss (for one batch) at step 1302: 53600182272.0000\n","Training loss (for one batch) at step 1303: 1377125531648.0000\n","Training loss (for one batch) at step 1304: 554117760.0000\n","Training loss (for one batch) at step 1305: 110749392896.0000\n","Training loss (for one batch) at step 1306: 13500235776.0000\n","Training loss (for one batch) at step 1307: 2272529664.0000\n","Training loss (for one batch) at step 1308: 9120608256.0000\n","Training loss (for one batch) at step 1309: 15481548800.0000\n","Training loss (for one batch) at step 1310: 16559998976.0000\n","Training loss (for one batch) at step 1311: 23257118720.0000\n","Training loss (for one batch) at step 1312: 19920164864.0000\n","Training loss (for one batch) at step 1313: 23589683200.0000\n","Training loss (for one batch) at step 1314: 65801252864.0000\n","Training loss (for one batch) at step 1315: 4296770560.0000\n","Training loss (for one batch) at step 1316: 27883057152.0000\n","Training loss (for one batch) at step 1317: 36470075392.0000\n","Training loss (for one batch) at step 1318: 660320026624.0000\n","Training loss (for one batch) at step 1319: 138521165824.0000\n","Training loss (for one batch) at step 1320: 3166088200192.0000\n","Training loss (for one batch) at step 1321: 6728835072.0000\n","Training loss (for one batch) at step 1322: 51135959040.0000\n","Training loss (for one batch) at step 1323: 29715658752.0000\n","Training loss (for one batch) at step 1324: 33626642432.0000\n","Training loss (for one batch) at step 1325: 4015473920.0000\n","Training loss (for one batch) at step 1326: 3555614208.0000\n","Training loss (for one batch) at step 1327: 31651950592.0000\n","Training loss (for one batch) at step 1328: 358614269952.0000\n","Training loss (for one batch) at step 1329: 42991751168.0000\n","Training loss (for one batch) at step 1330: 70845431808.0000\n","Training loss (for one batch) at step 1331: 17968148480.0000\n","Training loss (for one batch) at step 1332: 21557045248.0000\n","Training loss (for one batch) at step 1333: 2586405376.0000\n","Training loss (for one batch) at step 1334: 9625927680.0000\n","Training loss (for one batch) at step 1335: 1870706944.0000\n","Training loss (for one batch) at step 1336: 3302643968.0000\n","Training loss (for one batch) at step 1337: 18348109824.0000\n","Training loss (for one batch) at step 1338: 21452193792.0000\n","Training loss (for one batch) at step 1339: 13726807040.0000\n","Training loss (for one batch) at step 1340: 26643488768.0000\n","Training loss (for one batch) at step 1341: 57704808448.0000\n","Training loss (for one batch) at step 1342: 26314219520.0000\n","Training loss (for one batch) at step 1343: 45971857408.0000\n","Training loss (for one batch) at step 1344: 403973636096.0000\n","Training loss (for one batch) at step 1345: 6486611968.0000\n","Training loss (for one batch) at step 1346: 3481877760.0000\n","Training loss (for one batch) at step 1347: 150383820800.0000\n","Training loss (for one batch) at step 1348: 107891474432.0000\n","Training loss (for one batch) at step 1349: 74920017920.0000\n","Training loss (for one batch) at step 1350: 11939426304.0000\n","Training loss (for one batch) at step 1351: 86129950720.0000\n","Training loss (for one batch) at step 1352: 16019761152.0000\n","Training loss (for one batch) at step 1353: 5006007296.0000\n","Training loss (for one batch) at step 1354: 3226432256.0000\n","Training loss (for one batch) at step 1355: 8836737024.0000\n","Training loss (for one batch) at step 1356: 80386949120.0000\n","Training loss (for one batch) at step 1357: 6450794496.0000\n","Training loss (for one batch) at step 1358: 19008212992.0000\n","Training loss (for one batch) at step 1359: 6143258624.0000\n","Training loss (for one batch) at step 1360: 107149713408.0000\n","Training loss (for one batch) at step 1361: 5299388416.0000\n","Training loss (for one batch) at step 1362: 44961017856.0000\n","Training loss (for one batch) at step 1363: 118486368256.0000\n","Training loss (for one batch) at step 1364: 8232066560.0000\n","Training loss (for one batch) at step 1365: 36512927744.0000\n","Training loss (for one batch) at step 1366: 3872024832.0000\n","Training loss (for one batch) at step 1367: 24850753536.0000\n","Training loss (for one batch) at step 1368: 5955542016.0000\n","Training loss (for one batch) at step 1369: 18729508864.0000\n","Training loss (for one batch) at step 1370: 2507681024.0000\n","Training loss (for one batch) at step 1371: 6033966080.0000\n","Training loss (for one batch) at step 1372: 98995003392.0000\n","Training loss (for one batch) at step 1373: 170807164928.0000\n","Training loss (for one batch) at step 1374: 33227110400.0000\n","Training loss (for one batch) at step 1375: 158333075456.0000\n","Training loss (for one batch) at step 1376: 767957991424.0000\n","Training loss (for one batch) at step 1377: 34326286336.0000\n","Training loss (for one batch) at step 1378: 112303513600.0000\n","Training loss (for one batch) at step 1379: 3920851456.0000\n","Training loss (for one batch) at step 1380: 2267365120.0000\n","Training loss (for one batch) at step 1381: 62294851584.0000\n","Training loss (for one batch) at step 1382: 25870919680.0000\n","Training loss (for one batch) at step 1383: 1009636802560.0000\n","Training loss (for one batch) at step 1384: 138655481856.0000\n","Training loss (for one batch) at step 1385: 27390640128.0000\n","Training loss (for one batch) at step 1386: 267246092288.0000\n","Training loss (for one batch) at step 1387: 95156248576.0000\n","Training loss (for one batch) at step 1388: 3394219540480.0000\n","Training loss (for one batch) at step 1389: 38948769792.0000\n","Training loss (for one batch) at step 1390: 14481510400.0000\n","Training loss (for one batch) at step 1391: 41682415616.0000\n","Training loss (for one batch) at step 1392: 76225896448.0000\n","Training loss (for one batch) at step 1393: 468226637824.0000\n","Training loss (for one batch) at step 1394: 4905343909888.0000\n","Training loss (for one batch) at step 1395: 22730706944.0000\n","Training loss (for one batch) at step 1396: 14417895424.0000\n","Training loss (for one batch) at step 1397: 34197168128.0000\n","Training loss (for one batch) at step 1398: 388403036160.0000\n","Training loss (for one batch) at step 1399: 7002468352.0000\n","Training loss (for one batch) at step 1400: 146348949504.0000\n","Training loss (for one batch) at step 1401: 3605428224.0000\n","Training loss (for one batch) at step 1402: 388687101952.0000\n","Training loss (for one batch) at step 1403: 121729613824.0000\n","Training loss (for one batch) at step 1404: 286867587072.0000\n","Training loss (for one batch) at step 1405: 138478174208.0000\n","Training loss (for one batch) at step 1406: 63254450176.0000\n","Training loss (for one batch) at step 1407: 110086397952.0000\n","Training loss (for one batch) at step 1408: 22817101824.0000\n","Training loss (for one batch) at step 1409: 279764664320.0000\n","Training loss (for one batch) at step 1410: 1981624576.0000\n","Training loss (for one batch) at step 1411: 16386877440.0000\n","Training loss (for one batch) at step 1412: 48178212864.0000\n","Training loss (for one batch) at step 1413: 136652488704.0000\n","Training loss (for one batch) at step 1414: 215443488768.0000\n","Training loss (for one batch) at step 1415: 347234402304.0000\n","Training loss (for one batch) at step 1416: 6817962496.0000\n","Training loss (for one batch) at step 1417: 3734017536.0000\n","Training loss (for one batch) at step 1418: 14030716928.0000\n","Training loss (for one batch) at step 1419: 10889521152.0000\n","Training loss (for one batch) at step 1420: 298675208192.0000\n","Training loss (for one batch) at step 1421: 33505677312.0000\n","Training loss (for one batch) at step 1422: 42790191104.0000\n","Training loss (for one batch) at step 1423: 331107434496.0000\n","Training loss (for one batch) at step 1424: 2214132736.0000\n","Training loss (for one batch) at step 1425: 1595348608.0000\n","Training loss (for one batch) at step 1426: 593191698432.0000\n","Training loss (for one batch) at step 1427: 9458560000.0000\n","Training loss (for one batch) at step 1428: 76040208384.0000\n","Training loss (for one batch) at step 1429: 285356982272.0000\n","Training loss (for one batch) at step 1430: 70611369984.0000\n","Training loss (for one batch) at step 1431: 3696450048.0000\n","Training loss (for one batch) at step 1432: 8185176576.0000\n","Training loss (for one batch) at step 1433: 18250768384.0000\n","Training loss (for one batch) at step 1434: 12520879104.0000\n","Training loss (for one batch) at step 1435: 5871873024.0000\n","Training loss (for one batch) at step 1436: 35285262336.0000\n","Training loss (for one batch) at step 1437: 11690273792.0000\n","Training loss (for one batch) at step 1438: 8567164416.0000\n","Training loss (for one batch) at step 1439: 137293045760.0000\n","Training loss (for one batch) at step 1440: 93809573888.0000\n","Training loss (for one batch) at step 1441: 6672881664.0000\n","Training loss (for one batch) at step 1442: 161582399488.0000\n","Training loss (for one batch) at step 1443: 265393799168.0000\n","Training loss (for one batch) at step 1444: 2812598272.0000\n","Training loss (for one batch) at step 1445: 460460228608.0000\n","Training loss (for one batch) at step 1446: 58116104192.0000\n","Training loss (for one batch) at step 1447: 64212275200.0000\n","Training loss (for one batch) at step 1448: 3380405760.0000\n","Training loss (for one batch) at step 1449: 15094410240.0000\n","Training loss (for one batch) at step 1450: 5888115200.0000\n","Training loss (for one batch) at step 1451: 4982508032.0000\n","Training loss (for one batch) at step 1452: 57714462720.0000\n","Training loss (for one batch) at step 1453: 75256265965568.0000\n","Training loss (for one batch) at step 1454: 70601244672.0000\n","Training loss (for one batch) at step 1455: 142917386240.0000\n","Training loss (for one batch) at step 1456: 336153411584.0000\n","Training loss (for one batch) at step 1457: 13015342080.0000\n","Training loss (for one batch) at step 1458: 10038517760.0000\n","Training loss (for one batch) at step 1459: 1404366080.0000\n","Training loss (for one batch) at step 1460: 2705517824.0000\n","Training loss (for one batch) at step 1461: 6569316352.0000\n","Training loss (for one batch) at step 1462: 10133824512.0000\n","Training loss (for one batch) at step 1463: 7780009472.0000\n","Training loss (for one batch) at step 1464: 373025144832.0000\n","Training loss (for one batch) at step 1465: 92973522944.0000\n","Training loss (for one batch) at step 1466: 12080795648.0000\n","Training loss (for one batch) at step 1467: 59381346304.0000\n","Training loss (for one batch) at step 1468: 14881089536.0000\n","Training loss (for one batch) at step 1469: 3233001472.0000\n","Training loss (for one batch) at step 1470: 6849852928.0000\n","Training loss (for one batch) at step 1471: 13613352960.0000\n","Training loss (for one batch) at step 1472: 8496390656.0000\n","Training loss (for one batch) at step 1473: 3503199744.0000\n","Training loss (for one batch) at step 1474: 591359770624.0000\n","Training loss (for one batch) at step 1475: 17219219456.0000\n","Training loss (for one batch) at step 1476: 1817035136.0000\n","Training loss (for one batch) at step 1477: 17518657536.0000\n","Training loss (for one batch) at step 1478: 12183500800.0000\n","Training loss (for one batch) at step 1479: 11051449344.0000\n","Training loss (for one batch) at step 1480: 232921546752.0000\n","Training loss (for one batch) at step 1481: 167793999872.0000\n","Training loss (for one batch) at step 1482: 86759686144.0000\n","Training loss (for one batch) at step 1483: 13101802979328.0000\n","Training loss (for one batch) at step 1484: 12018511872.0000\n","Training loss (for one batch) at step 1485: 15857277952.0000\n","Training loss (for one batch) at step 1486: 5632816128.0000\n","Training loss (for one batch) at step 1487: 95204261888.0000\n","Training loss (for one batch) at step 1488: 13342808064.0000\n","Training loss (for one batch) at step 1489: 42921586688.0000\n","Training loss (for one batch) at step 1490: 4839844864.0000\n","Training loss (for one batch) at step 1491: 9865486336.0000\n","Training loss (for one batch) at step 1492: 36444712960.0000\n","Training loss (for one batch) at step 1493: 28207005696.0000\n","Training loss (for one batch) at step 1494: 328703279104.0000\n","Training loss (for one batch) at step 1495: 48512278528.0000\n","Training loss (for one batch) at step 1496: 1093871616.0000\n","Training loss (for one batch) at step 1497: 17874317312.0000\n","Training loss (for one batch) at step 1498: 122123616256.0000\n","Training loss (for one batch) at step 1499: 219258355712.0000\n","Training loss (for one batch) at step 1500: 81089028096.0000\n","Training loss (for one batch) at step 1501: 34951680000.0000\n","Training loss (for one batch) at step 1502: 18396860416.0000\n","Training loss (for one batch) at step 1503: 3949523968.0000\n","Training loss (for one batch) at step 1504: 2048274560.0000\n","Training loss (for one batch) at step 1505: 638462656512.0000\n","Training loss (for one batch) at step 1506: 151124525056.0000\n","Training loss (for one batch) at step 1507: 3643938048.0000\n","Training loss (for one batch) at step 1508: 4868926976.0000\n","Training loss (for one batch) at step 1509: 33248811008.0000\n","Training loss (for one batch) at step 1510: 5033725952.0000\n","Training loss (for one batch) at step 1511: 55894663168.0000\n","Training loss (for one batch) at step 1512: 17587724288.0000\n","Training loss (for one batch) at step 1513: 40637308928.0000\n","Training loss (for one batch) at step 1514: 15786878976.0000\n","Training loss (for one batch) at step 1515: 32634304512.0000\n","Training loss (for one batch) at step 1516: 90589888512.0000\n","Training loss (for one batch) at step 1517: 27576614912.0000\n","Training loss (for one batch) at step 1518: 37766995968.0000\n","Training loss (for one batch) at step 1519: 290866102272.0000\n","Training loss (for one batch) at step 1520: 3767461888.0000\n","Training loss (for one batch) at step 1521: 46879043584.0000\n","Training loss (for one batch) at step 1522: 1654744832.0000\n","Training loss (for one batch) at step 1523: 935204093952.0000\n","Training loss (for one batch) at step 1524: 8316477440.0000\n","Training loss (for one batch) at step 1525: 124555075584.0000\n","Training loss (for one batch) at step 1526: 7200351232.0000\n","Training loss (for one batch) at step 1527: 18722713600.0000\n","Training loss (for one batch) at step 1528: 152139972608.0000\n","Training loss (for one batch) at step 1529: 25850163200.0000\n","Training loss (for one batch) at step 1530: 95604310016.0000\n","Training loss (for one batch) at step 1531: 18166190080.0000\n","Training loss (for one batch) at step 1532: 8077993984.0000\n","Training loss (for one batch) at step 1533: 5415408640.0000\n","Training loss (for one batch) at step 1534: 102193250304.0000\n","Training loss (for one batch) at step 1535: 1364823900160.0000\n","Training loss (for one batch) at step 1536: 1498206080.0000\n","Training loss (for one batch) at step 1537: 21349457920.0000\n","Training loss (for one batch) at step 1538: 6416564736.0000\n","Training loss (for one batch) at step 1539: 28923267072.0000\n","Training loss (for one batch) at step 1540: 40884150272.0000\n","Training loss (for one batch) at step 1541: 56263557120.0000\n","Training loss (for one batch) at step 1542: 15404498944.0000\n","Training loss (for one batch) at step 1543: 37534969856.0000\n","Training loss (for one batch) at step 1544: 2244136960.0000\n","Training loss (for one batch) at step 1545: 3297223936.0000\n","Training loss (for one batch) at step 1546: 3674470656.0000\n","Training loss (for one batch) at step 1547: 98299846656.0000\n","Training loss (for one batch) at step 1548: 19294853120.0000\n","Training loss (for one batch) at step 1549: 19941955584.0000\n","Training loss (for one batch) at step 1550: 87654998016.0000\n","Training loss (for one batch) at step 1551: 256615104512.0000\n","Training loss (for one batch) at step 1552: 30238230528.0000\n","Training loss (for one batch) at step 1553: 122278699008.0000\n","Training loss (for one batch) at step 1554: 204448219136.0000\n","Training loss (for one batch) at step 1555: 25021669376.0000\n","Training loss (for one batch) at step 1556: 17461260288.0000\n","Training loss (for one batch) at step 1557: 12216940544.0000\n","Training loss (for one batch) at step 1558: 4582303232.0000\n","Training loss (for one batch) at step 1559: 34098857984.0000\n","Training loss (for one batch) at step 1560: 9612783616.0000\n","Training loss (for one batch) at step 1561: 32181940224.0000\n","Training loss (for one batch) at step 1562: 305345495040.0000\n","Training loss (for one batch) at step 1563: 392422555648.0000\n","Training loss (for one batch) at step 1564: 41049444352.0000\n","Training loss (for one batch) at step 1565: 16761564160.0000\n","Training loss (for one batch) at step 1566: 28358850560.0000\n","Training loss (for one batch) at step 1567: 19059085312.0000\n","Training loss (for one batch) at step 1568: 460563218432.0000\n","Training loss (for one batch) at step 1569: 16751711232.0000\n","Training loss (for one batch) at step 1570: 824588435456.0000\n","Training loss (for one batch) at step 1571: 2192453795840.0000\n","Training loss (for one batch) at step 1572: 13385068544.0000\n","Training loss (for one batch) at step 1573: 141716799488.0000\n","Training loss (for one batch) at step 1574: 16066482176.0000\n","Training loss (for one batch) at step 1575: 420094214144.0000\n","Training loss (for one batch) at step 1576: 11130791936.0000\n","Training loss (for one batch) at step 1577: 12763863040.0000\n","Training loss (for one batch) at step 1578: 6624897024.0000\n","Training loss (for one batch) at step 1579: 275194839040.0000\n","Training loss (for one batch) at step 1580: 14625057792.0000\n","Training loss (for one batch) at step 1581: 47416999936.0000\n","Training loss (for one batch) at step 1582: 2800612613816320.0000\n","Training loss (for one batch) at step 1583: 8058494976.0000\n","Training loss (for one batch) at step 1584: 17557460992.0000\n","Training loss (for one batch) at step 1585: 46865465344.0000\n","Training loss (for one batch) at step 1586: 3923788800.0000\n","Training loss (for one batch) at step 1587: 7574990336.0000\n","Training loss (for one batch) at step 1588: 121760759808.0000\n","Training loss (for one batch) at step 1589: 7079108096.0000\n","Training loss (for one batch) at step 1590: 22715789312.0000\n","Training loss (for one batch) at step 1591: 97131184128.0000\n","Training loss (for one batch) at step 1592: 15393865728.0000\n","Training loss (for one batch) at step 1593: 4844491776.0000\n","Training loss (for one batch) at step 1594: 15484169216.0000\n","Training loss (for one batch) at step 1595: 37499539456.0000\n","Training loss (for one batch) at step 1596: 23870717952.0000\n","Training loss (for one batch) at step 1597: 1527039360.0000\n","Training loss (for one batch) at step 1598: 360567472128.0000\n","Training loss (for one batch) at step 1599: 3393412608.0000\n","Training loss (for one batch) at step 1600: 158447878144.0000\n","Training loss (for one batch) at step 1601: 149643132928.0000\n","Training loss (for one batch) at step 1602: 2189768704.0000\n","Training loss (for one batch) at step 1603: 7167437312.0000\n","Training loss (for one batch) at step 1604: 2954880000.0000\n","Training loss (for one batch) at step 1605: 16728242176.0000\n","Training loss (for one batch) at step 1606: 38507847680.0000\n","Training loss (for one batch) at step 1607: 27297210368.0000\n","Training loss (for one batch) at step 1608: 185407012864.0000\n","Training loss (for one batch) at step 1609: 11622199296.0000\n","Training loss (for one batch) at step 1610: 209928224768.0000\n","Training loss (for one batch) at step 1611: 224052674560.0000\n","Training loss (for one batch) at step 1612: 170800840704.0000\n","Training loss (for one batch) at step 1613: 5096548864.0000\n","Training loss (for one batch) at step 1614: 59949211648.0000\n","Training loss (for one batch) at step 1615: 3663341056.0000\n","Training loss (for one batch) at step 1616: 127237128192.0000\n","Training loss (for one batch) at step 1617: 2700665683968.0000\n","Training loss (for one batch) at step 1618: 8657192960.0000\n","Training loss (for one batch) at step 1619: 28585801728.0000\n","Training loss (for one batch) at step 1620: 9811614720.0000\n","Training loss (for one batch) at step 1621: 8480838656.0000\n","Training loss (for one batch) at step 1622: 3070830336.0000\n","Training loss (for one batch) at step 1623: 1973196544.0000\n","Training loss (for one batch) at step 1624: 2932478208.0000\n","Training loss (for one batch) at step 1625: 21615585280.0000\n","Training loss (for one batch) at step 1626: 5503618048.0000\n","Training loss (for one batch) at step 1627: 44144631808.0000\n","Training loss (for one batch) at step 1628: 80054190080.0000\n","Training loss (for one batch) at step 1629: 24893863936.0000\n","Training loss (for one batch) at step 1630: 3546757632.0000\n","Training loss (for one batch) at step 1631: 88373002240.0000\n","Training loss (for one batch) at step 1632: 138698899456.0000\n","Training loss (for one batch) at step 1633: 274084560896.0000\n","Training loss (for one batch) at step 1634: 6178848768.0000\n","Training loss (for one batch) at step 1635: 17561718784.0000\n","Training loss (for one batch) at step 1636: 10599763968.0000\n","Training loss (for one batch) at step 1637: 6824590336.0000\n","Training loss (for one batch) at step 1638: 72663941120.0000\n","Training loss (for one batch) at step 1639: 3898263040.0000\n","Training loss (for one batch) at step 1640: 46423572480.0000\n","Training loss (for one batch) at step 1641: 73720594432.0000\n","Training loss (for one batch) at step 1642: 199552647168.0000\n","Training loss (for one batch) at step 1643: 385425604608.0000\n","Training loss (for one batch) at step 1644: 234593812480.0000\n","Training loss (for one batch) at step 1645: 20431845376.0000\n","Training loss (for one batch) at step 1646: 148139524096.0000\n","Training loss (for one batch) at step 1647: 131440041984.0000\n","Training loss (for one batch) at step 1648: 5269419982848.0000\n","Training loss (for one batch) at step 1649: 24105347072.0000\n","Training loss (for one batch) at step 1650: 22295015424.0000\n","Training loss (for one batch) at step 1651: 5155158528.0000\n","Training loss (for one batch) at step 1652: 17804234752.0000\n","Training loss (for one batch) at step 1653: 4818441216.0000\n","Training loss (for one batch) at step 1654: 71894614016.0000\n","Training loss (for one batch) at step 1655: 2335670528.0000\n","Training loss (for one batch) at step 1656: 13615110144.0000\n","Training loss (for one batch) at step 1657: 26004035584.0000\n","Training loss (for one batch) at step 1658: 41631539200.0000\n","Training loss (for one batch) at step 1659: 96988004352.0000\n","Training loss (for one batch) at step 1660: 205525090304.0000\n","Training loss (for one batch) at step 1661: 3032089600.0000\n","Training loss (for one batch) at step 1662: 44794146816.0000\n","Training loss (for one batch) at step 1663: 8230966272.0000\n","Training loss (for one batch) at step 1664: 30661783552.0000\n","Training loss (for one batch) at step 1665: 79338553344.0000\n","Training loss (for one batch) at step 1666: 720558161920.0000\n","Training loss (for one batch) at step 1667: 24279764992.0000\n","Training loss (for one batch) at step 1668: 615039959040.0000\n","Training loss (for one batch) at step 1669: 7245097984.0000\n","Training loss (for one batch) at step 1670: 10853249024.0000\n","Training loss (for one batch) at step 1671: 46016634880.0000\n","Training loss (for one batch) at step 1672: 51638382592.0000\n","Training loss (for one batch) at step 1673: 22606606336.0000\n","Training loss (for one batch) at step 1674: 26097526784.0000\n","Training loss (for one batch) at step 1675: 6479580160.0000\n","Training loss (for one batch) at step 1676: 2099427456.0000\n","Training loss (for one batch) at step 1677: 165546393600.0000\n","Training loss (for one batch) at step 1678: 60989566976.0000\n","Training loss (for one batch) at step 1679: 45130477568.0000\n","Training loss (for one batch) at step 1680: 442104938496.0000\n","Training loss (for one batch) at step 1681: 3396347904.0000\n","Training loss (for one batch) at step 1682: 4911579648.0000\n","Training loss (for one batch) at step 1683: 230422806528.0000\n","Training loss (for one batch) at step 1684: 45410115584.0000\n","Training loss (for one batch) at step 1685: 15881126912.0000\n","Training loss (for one batch) at step 1686: 505974882304.0000\n","Training loss (for one batch) at step 1687: 76093054976.0000\n","Training loss (for one batch) at step 1688: 5092201472.0000\n","Training loss (for one batch) at step 1689: 3480952320.0000\n","Training loss (for one batch) at step 1690: 1715254912.0000\n","Training loss (for one batch) at step 1691: 14068291584.0000\n","Training loss (for one batch) at step 1692: 222624088064.0000\n","Training loss (for one batch) at step 1693: 2012267413504.0000\n","\n","Start of epoch 48\n","Training loss (for one batch) at step 0: 38505881600.0000\n","Training loss (for one batch) at step 1: 32023353344.0000\n","Training loss (for one batch) at step 2: 48922267648.0000\n","Training loss (for one batch) at step 3: 19964786688.0000\n","Training loss (for one batch) at step 4: 5180250112.0000\n","Training loss (for one batch) at step 5: 358014386176.0000\n","Training loss (for one batch) at step 6: 18009589760.0000\n","Training loss (for one batch) at step 7: 275750289408.0000\n","Training loss (for one batch) at step 8: 5603191808.0000\n","Training loss (for one batch) at step 9: 8055739392.0000\n","Training loss (for one batch) at step 10: 54342529024.0000\n","Training loss (for one batch) at step 11: 49176469504.0000\n","Training loss (for one batch) at step 12: 5871376896.0000\n","Training loss (for one batch) at step 13: 5159551488.0000\n","Training loss (for one batch) at step 14: 35395158016.0000\n","Training loss (for one batch) at step 15: 19316682752.0000\n","Training loss (for one batch) at step 16: 30632347648.0000\n","Training loss (for one batch) at step 17: 5133726208.0000\n","Training loss (for one batch) at step 18: 197584945152.0000\n","Training loss (for one batch) at step 19: 171294294016.0000\n","Training loss (for one batch) at step 20: 18112710656.0000\n","Training loss (for one batch) at step 21: 3477987584.0000\n","Training loss (for one batch) at step 22: 294617841664.0000\n","Training loss (for one batch) at step 23: 20388532224.0000\n","Training loss (for one batch) at step 24: 85941223424.0000\n","Training loss (for one batch) at step 25: 4155348480.0000\n","Training loss (for one batch) at step 26: 988927885312.0000\n","Training loss (for one batch) at step 27: 98552029184.0000\n","Training loss (for one batch) at step 28: 134947020800.0000\n","Training loss (for one batch) at step 29: 16718824448.0000\n","Training loss (for one batch) at step 30: 128179568640.0000\n","Training loss (for one batch) at step 31: 125552050176.0000\n","Training loss (for one batch) at step 32: 451533537280.0000\n","Training loss (for one batch) at step 33: 3461523701760.0000\n","Training loss (for one batch) at step 34: 28837900288.0000\n","Training loss (for one batch) at step 35: 4628152832.0000\n","Training loss (for one batch) at step 36: 56697835520.0000\n","Training loss (for one batch) at step 37: 20649476096.0000\n","Training loss (for one batch) at step 38: 3201801216.0000\n","Training loss (for one batch) at step 39: 75980972032.0000\n","Training loss (for one batch) at step 40: 3460900864.0000\n","Training loss (for one batch) at step 41: 6030956544.0000\n","Training loss (for one batch) at step 42: 163378839552.0000\n","Training loss (for one batch) at step 43: 1360873088.0000\n","Training loss (for one batch) at step 44: 10667675648.0000\n","Training loss (for one batch) at step 45: 10175666176.0000\n","Training loss (for one batch) at step 46: 611366600704.0000\n","Training loss (for one batch) at step 47: 259441623040.0000\n","Training loss (for one batch) at step 48: 35490336768.0000\n","Training loss (for one batch) at step 49: 16628007936.0000\n","Training loss (for one batch) at step 50: 811558371328.0000\n","Training loss (for one batch) at step 51: 39466827776.0000\n","Training loss (for one batch) at step 52: 16849430528.0000\n","Training loss (for one batch) at step 53: 216469045248.0000\n","Training loss (for one batch) at step 54: 202765828096.0000\n","Training loss (for one batch) at step 55: 35527127040.0000\n","Training loss (for one batch) at step 56: 221350379520.0000\n","Training loss (for one batch) at step 57: 5218505216.0000\n","Training loss (for one batch) at step 58: 10774120448.0000\n","Training loss (for one batch) at step 59: 4215313408.0000\n","Training loss (for one batch) at step 60: 42478616576.0000\n","Training loss (for one batch) at step 61: 92873572352.0000\n","Training loss (for one batch) at step 62: 89125560320.0000\n","Training loss (for one batch) at step 63: 21493438464.0000\n","Training loss (for one batch) at step 64: 7632556544.0000\n","Training loss (for one batch) at step 65: 22193467392.0000\n","Training loss (for one batch) at step 66: 511032384.0000\n","Training loss (for one batch) at step 67: 359772028928.0000\n","Training loss (for one batch) at step 68: 202788667392.0000\n","Training loss (for one batch) at step 69: 195709485056.0000\n","Training loss (for one batch) at step 70: 822407856128.0000\n","Training loss (for one batch) at step 71: 955132215296.0000\n","Training loss (for one batch) at step 72: 6455595520.0000\n","Training loss (for one batch) at step 73: 24531658752.0000\n","Training loss (for one batch) at step 74: 22017560576.0000\n","Training loss (for one batch) at step 75: 70676021248.0000\n","Training loss (for one batch) at step 76: 126046527488.0000\n","Training loss (for one batch) at step 77: 5809104896.0000\n","Training loss (for one batch) at step 78: 11011721216.0000\n","Training loss (for one batch) at step 79: 39393411072.0000\n","Training loss (for one batch) at step 80: 728396672.0000\n","Training loss (for one batch) at step 81: 3636305664.0000\n","Training loss (for one batch) at step 82: 33834151936.0000\n","Training loss (for one batch) at step 83: 400214523904.0000\n","Training loss (for one batch) at step 84: 14278393856.0000\n","Training loss (for one batch) at step 85: 5966790144.0000\n","Training loss (for one batch) at step 86: 772232314880.0000\n","Training loss (for one batch) at step 87: 1619989376.0000\n","Training loss (for one batch) at step 88: 192153812992.0000\n","Training loss (for one batch) at step 89: 183130193920.0000\n","Training loss (for one batch) at step 90: 208204496896.0000\n","Training loss (for one batch) at step 91: 11183426560.0000\n","Training loss (for one batch) at step 92: 16752934912.0000\n","Training loss (for one batch) at step 93: 145033101312.0000\n","Training loss (for one batch) at step 94: 6546263552.0000\n","Training loss (for one batch) at step 95: 279627661312.0000\n","Training loss (for one batch) at step 96: 213419245568.0000\n","Training loss (for one batch) at step 97: 692724170752.0000\n","Training loss (for one batch) at step 98: 12211990528.0000\n","Training loss (for one batch) at step 99: 12281271296.0000\n","Training loss (for one batch) at step 100: 58890039296.0000\n","Training loss (for one batch) at step 101: 60898951168.0000\n","Training loss (for one batch) at step 102: 13415028736.0000\n","Training loss (for one batch) at step 103: 37517471744.0000\n","Training loss (for one batch) at step 104: 109036830720.0000\n","Training loss (for one batch) at step 105: 5203989504.0000\n","Training loss (for one batch) at step 106: 957391898148864.0000\n","Training loss (for one batch) at step 107: 9967201280.0000\n","Training loss (for one batch) at step 108: 26708713472.0000\n","Training loss (for one batch) at step 109: 13590781952.0000\n","Training loss (for one batch) at step 110: 20966684672.0000\n","Training loss (for one batch) at step 111: 87367630848.0000\n","Training loss (for one batch) at step 112: 192441008128.0000\n","Training loss (for one batch) at step 113: 45613105152.0000\n","Training loss (for one batch) at step 114: 9282384896.0000\n","Training loss (for one batch) at step 115: 158278385664.0000\n","Training loss (for one batch) at step 116: 153340051456.0000\n","Training loss (for one batch) at step 117: 17540214784.0000\n","Training loss (for one batch) at step 118: 8658183168.0000\n","Training loss (for one batch) at step 119: 11330230272.0000\n","Training loss (for one batch) at step 120: 12761106432.0000\n","Training loss (for one batch) at step 121: 130064891904.0000\n","Training loss (for one batch) at step 122: 200325627904.0000\n","Training loss (for one batch) at step 123: 57439559680.0000\n","Training loss (for one batch) at step 124: 18032046080.0000\n","Training loss (for one batch) at step 125: 18596900864.0000\n","Training loss (for one batch) at step 126: 271143206912.0000\n","Training loss (for one batch) at step 127: 28673638400.0000\n","Training loss (for one batch) at step 128: 155983642624.0000\n","Training loss (for one batch) at step 129: 5848253952.0000\n","Training loss (for one batch) at step 130: 10100277248.0000\n","Training loss (for one batch) at step 131: 5639738368.0000\n","Training loss (for one batch) at step 132: 92594864128.0000\n","Training loss (for one batch) at step 133: 10751191040.0000\n","Training loss (for one batch) at step 134: 33675364352.0000\n","Training loss (for one batch) at step 135: 590073495552.0000\n","Training loss (for one batch) at step 136: 54021963776.0000\n","Training loss (for one batch) at step 137: 126074912768.0000\n","Training loss (for one batch) at step 138: 2165410365440.0000\n","Training loss (for one batch) at step 139: 33457469440.0000\n","Training loss (for one batch) at step 140: 6296135680.0000\n","Training loss (for one batch) at step 141: 504792809472.0000\n","Training loss (for one batch) at step 142: 11491427328.0000\n","Training loss (for one batch) at step 143: 12465506304.0000\n","Training loss (for one batch) at step 144: 72810774528.0000\n","Training loss (for one batch) at step 145: 17728968704.0000\n","Training loss (for one batch) at step 146: 8799842304.0000\n","Training loss (for one batch) at step 147: 155695104000.0000\n","Training loss (for one batch) at step 148: 199164559360.0000\n","Training loss (for one batch) at step 149: 23351164928.0000\n","Training loss (for one batch) at step 150: 16738349056.0000\n","Training loss (for one batch) at step 151: 11958069248.0000\n","Training loss (for one batch) at step 152: 8995600384.0000\n","Training loss (for one batch) at step 153: 7687695360.0000\n","Training loss (for one batch) at step 154: 39980642304.0000\n","Training loss (for one batch) at step 155: 9066917888.0000\n","Training loss (for one batch) at step 156: 55848378368.0000\n","Training loss (for one batch) at step 157: 2895464960.0000\n","Training loss (for one batch) at step 158: 91288674304.0000\n","Training loss (for one batch) at step 159: 4681788416.0000\n","Training loss (for one batch) at step 160: 167441416192.0000\n","Training loss (for one batch) at step 161: 13552722944.0000\n","Training loss (for one batch) at step 162: 122517585920.0000\n","Training loss (for one batch) at step 163: 55079194624.0000\n","Training loss (for one batch) at step 164: 23395913728.0000\n","Training loss (for one batch) at step 165: 111122055168.0000\n","Training loss (for one batch) at step 166: 236625362944.0000\n","Training loss (for one batch) at step 167: 11026319360.0000\n","Training loss (for one batch) at step 168: 91448082432.0000\n","Training loss (for one batch) at step 169: 309217624064.0000\n","Training loss (for one batch) at step 170: 191228870656.0000\n","Training loss (for one batch) at step 171: 10334666752.0000\n","Training loss (for one batch) at step 172: 436119437312.0000\n","Training loss (for one batch) at step 173: 555971510272.0000\n","Training loss (for one batch) at step 174: 4130171136.0000\n","Training loss (for one batch) at step 175: 21779914752.0000\n","Training loss (for one batch) at step 176: 18685358080.0000\n","Training loss (for one batch) at step 177: 60539461632.0000\n","Training loss (for one batch) at step 178: 1432728320.0000\n","Training loss (for one batch) at step 179: 1306362624.0000\n","Training loss (for one batch) at step 180: 110429618176.0000\n","Training loss (for one batch) at step 181: 58836234240.0000\n","Training loss (for one batch) at step 182: 4087643648.0000\n","Training loss (for one batch) at step 183: 117102018560.0000\n","Training loss (for one batch) at step 184: 9831446528.0000\n","Training loss (for one batch) at step 185: 79178317824.0000\n","Training loss (for one batch) at step 186: 332949651456.0000\n","Training loss (for one batch) at step 187: 69342715904.0000\n","Training loss (for one batch) at step 188: 326705414144.0000\n","Training loss (for one batch) at step 189: 35932741632.0000\n","Training loss (for one batch) at step 190: 208862363648.0000\n","Training loss (for one batch) at step 191: 4194513408.0000\n","Training loss (for one batch) at step 192: 181317451776.0000\n","Training loss (for one batch) at step 193: 3395024384.0000\n","Training loss (for one batch) at step 194: 841887318016.0000\n","Training loss (for one batch) at step 195: 267373281280.0000\n","Training loss (for one batch) at step 196: 10314948608.0000\n","Training loss (for one batch) at step 197: 73409544192.0000\n","Training loss (for one batch) at step 198: 2811350272.0000\n","Training loss (for one batch) at step 199: 272799531008.0000\n","Training loss (for one batch) at step 200: 13351139328.0000\n","Training loss (for one batch) at step 201: 39947931648.0000\n","Training loss (for one batch) at step 202: 47730860032.0000\n","Training loss (for one batch) at step 203: 14108265472.0000\n","Training loss (for one batch) at step 204: 11424135168.0000\n","Training loss (for one batch) at step 205: 726895493120.0000\n","Training loss (for one batch) at step 206: 27631345664.0000\n","Training loss (for one batch) at step 207: 21208023040.0000\n","Training loss (for one batch) at step 208: 68105658368.0000\n","Training loss (for one batch) at step 209: 6699896320.0000\n","Training loss (for one batch) at step 210: 244876705792.0000\n","Training loss (for one batch) at step 211: 3928049664.0000\n","Training loss (for one batch) at step 212: 50603618304.0000\n","Training loss (for one batch) at step 213: 26443345920.0000\n","Training loss (for one batch) at step 214: 6502313984.0000\n","Training loss (for one batch) at step 215: 3185520148480.0000\n","Training loss (for one batch) at step 216: 3830473728.0000\n","Training loss (for one batch) at step 217: 1176947328.0000\n","Training loss (for one batch) at step 218: 3734961920.0000\n","Training loss (for one batch) at step 219: 30627547136.0000\n","Training loss (for one batch) at step 220: 858880540672.0000\n","Training loss (for one batch) at step 221: 3583351552.0000\n","Training loss (for one batch) at step 222: 15325188096.0000\n","Training loss (for one batch) at step 223: 19152244736.0000\n","Training loss (for one batch) at step 224: 35645526016.0000\n","Training loss (for one batch) at step 225: 262682787840.0000\n","Training loss (for one batch) at step 226: 4544425472.0000\n","Training loss (for one batch) at step 227: 256037109760.0000\n","Training loss (for one batch) at step 228: 2800890112.0000\n","Training loss (for one batch) at step 229: 18487613440.0000\n","Training loss (for one batch) at step 230: 6013437952.0000\n","Training loss (for one batch) at step 231: 79845703680.0000\n","Training loss (for one batch) at step 232: 21777772544.0000\n","Training loss (for one batch) at step 233: 218500890624.0000\n","Training loss (for one batch) at step 234: 38538928128.0000\n","Training loss (for one batch) at step 235: 6463339008.0000\n","Training loss (for one batch) at step 236: 5805617664.0000\n","Training loss (for one batch) at step 237: 183879024640.0000\n","Training loss (for one batch) at step 238: 182408904704.0000\n","Training loss (for one batch) at step 239: 87334289408.0000\n","Training loss (for one batch) at step 240: 528926441472.0000\n","Training loss (for one batch) at step 241: 12974313472.0000\n","Training loss (for one batch) at step 242: 30607935488.0000\n","Training loss (for one batch) at step 243: 3782213376.0000\n","Training loss (for one batch) at step 244: 1417940224.0000\n","Training loss (for one batch) at step 245: 29926502400.0000\n","Training loss (for one batch) at step 246: 4842899456.0000\n","Training loss (for one batch) at step 247: 183407050752.0000\n","Training loss (for one batch) at step 248: 120901672960.0000\n","Training loss (for one batch) at step 249: 125765853184.0000\n","Training loss (for one batch) at step 250: 128622493696.0000\n","Training loss (for one batch) at step 251: 39782572032.0000\n","Training loss (for one batch) at step 252: 13433910272.0000\n","Training loss (for one batch) at step 253: 27987171328.0000\n","Training loss (for one batch) at step 254: 16117405696.0000\n","Training loss (for one batch) at step 255: 38557335552.0000\n","Training loss (for one batch) at step 256: 106171514880.0000\n","Training loss (for one batch) at step 257: 18127007744.0000\n","Training loss (for one batch) at step 258: 14305653760.0000\n","Training loss (for one batch) at step 259: 15761724416.0000\n","Training loss (for one batch) at step 260: 25148131328.0000\n","Training loss (for one batch) at step 261: 150709485568.0000\n","Training loss (for one batch) at step 262: 12204524544.0000\n","Training loss (for one batch) at step 263: 392926298112.0000\n","Training loss (for one batch) at step 264: 310809952256.0000\n","Training loss (for one batch) at step 265: 15234195456.0000\n","Training loss (for one batch) at step 266: 49441828864.0000\n","Training loss (for one batch) at step 267: 16737016832.0000\n","Training loss (for one batch) at step 268: 25401958400.0000\n","Training loss (for one batch) at step 269: 70213222400.0000\n","Training loss (for one batch) at step 270: 5812187136.0000\n","Training loss (for one batch) at step 271: 202536402944.0000\n","Training loss (for one batch) at step 272: 4510138368.0000\n","Training loss (for one batch) at step 273: 163675865088.0000\n","Training loss (for one batch) at step 274: 41198682112.0000\n","Training loss (for one batch) at step 275: 39747842048.0000\n","Training loss (for one batch) at step 276: 123233574912.0000\n","Training loss (for one batch) at step 277: 17662881792.0000\n","Training loss (for one batch) at step 278: 132090232832.0000\n","Training loss (for one batch) at step 279: 154514522112.0000\n","Training loss (for one batch) at step 280: 19587330048.0000\n","Training loss (for one batch) at step 281: 10857289728.0000\n","Training loss (for one batch) at step 282: 27340525568.0000\n","Training loss (for one batch) at step 283: 286988664832.0000\n","Training loss (for one batch) at step 284: 28789526528.0000\n","Training loss (for one batch) at step 285: 13485575168.0000\n","Training loss (for one batch) at step 286: 374355427328.0000\n","Training loss (for one batch) at step 287: 6079268352.0000\n","Training loss (for one batch) at step 288: 131761913856.0000\n","Training loss (for one batch) at step 289: 490884399104.0000\n","Training loss (for one batch) at step 290: 10251307008.0000\n","Training loss (for one batch) at step 291: 322847801344.0000\n","Training loss (for one batch) at step 292: 35191812096.0000\n","Training loss (for one batch) at step 293: 1360131915776.0000\n","Training loss (for one batch) at step 294: 3551705088.0000\n","Training loss (for one batch) at step 295: 375954112512.0000\n","Training loss (for one batch) at step 296: 39622303744.0000\n","Training loss (for one batch) at step 297: 285340958720.0000\n","Training loss (for one batch) at step 298: 87013072896.0000\n","Training loss (for one batch) at step 299: 10158671872.0000\n","Training loss (for one batch) at step 300: 532470923264.0000\n","Training loss (for one batch) at step 301: 39151230976.0000\n","Training loss (for one batch) at step 302: 36786860032.0000\n","Training loss (for one batch) at step 303: 22778157056.0000\n","Training loss (for one batch) at step 304: 11430060032.0000\n","Training loss (for one batch) at step 305: 4352569344.0000\n","Training loss (for one batch) at step 306: 506539376640.0000\n","Training loss (for one batch) at step 307: 17866129408.0000\n","Training loss (for one batch) at step 308: 24797405184.0000\n","Training loss (for one batch) at step 309: 16212566016.0000\n","Training loss (for one batch) at step 310: 3106422016.0000\n","Training loss (for one batch) at step 311: 3235807744.0000\n","Training loss (for one batch) at step 312: 43356131328.0000\n","Training loss (for one batch) at step 313: 67896274944.0000\n","Training loss (for one batch) at step 314: 3894336552960.0000\n","Training loss (for one batch) at step 315: 516407394304.0000\n","Training loss (for one batch) at step 316: 3218488688640.0000\n","Training loss (for one batch) at step 317: 30693685248.0000\n","Training loss (for one batch) at step 318: 8010177024.0000\n","Training loss (for one batch) at step 319: 190419501056.0000\n","Training loss (for one batch) at step 320: 2820036096.0000\n","Training loss (for one batch) at step 321: 11368047616.0000\n","Training loss (for one batch) at step 322: 2269104640.0000\n","Training loss (for one batch) at step 323: 11320764416.0000\n","Training loss (for one batch) at step 324: 10465046528.0000\n","Training loss (for one batch) at step 325: 12275863552.0000\n","Training loss (for one batch) at step 326: 6040995840.0000\n","Training loss (for one batch) at step 327: 8134095872.0000\n","Training loss (for one batch) at step 328: 1914735360.0000\n","Training loss (for one batch) at step 329: 33015183360.0000\n","Training loss (for one batch) at step 330: 335179677696.0000\n","Training loss (for one batch) at step 331: 9343777792.0000\n","Training loss (for one batch) at step 332: 156640477184.0000\n","Training loss (for one batch) at step 333: 179915866112.0000\n","Training loss (for one batch) at step 334: 315834105856.0000\n","Training loss (for one batch) at step 335: 271659024384.0000\n","Training loss (for one batch) at step 336: 2869942026240.0000\n","Training loss (for one batch) at step 337: 59169386496.0000\n","Training loss (for one batch) at step 338: 43709583360.0000\n","Training loss (for one batch) at step 339: 34000091136.0000\n","Training loss (for one batch) at step 340: 31250851840.0000\n","Training loss (for one batch) at step 341: 42267516928.0000\n","Training loss (for one batch) at step 342: 21654581248.0000\n","Training loss (for one batch) at step 343: 5250938880.0000\n","Training loss (for one batch) at step 344: 5398731776.0000\n","Training loss (for one batch) at step 345: 59249037312.0000\n","Training loss (for one batch) at step 346: 11708003328.0000\n","Training loss (for one batch) at step 347: 102825533440.0000\n","Training loss (for one batch) at step 348: 16020867072.0000\n","Training loss (for one batch) at step 349: 42688946176.0000\n","Training loss (for one batch) at step 350: 899969664.0000\n","Training loss (for one batch) at step 351: 137738518528.0000\n","Training loss (for one batch) at step 352: 53667151872.0000\n","Training loss (for one batch) at step 353: 665402015744.0000\n","Training loss (for one batch) at step 354: 4472504320.0000\n","Training loss (for one batch) at step 355: 20258809856.0000\n","Training loss (for one batch) at step 356: 14766239744.0000\n","Training loss (for one batch) at step 357: 10810708992.0000\n","Training loss (for one batch) at step 358: 249416859648.0000\n","Training loss (for one batch) at step 359: 12600348672.0000\n","Training loss (for one batch) at step 360: 1017437952.0000\n","Training loss (for one batch) at step 361: 19548409856.0000\n","Training loss (for one batch) at step 362: 14805467136.0000\n","Training loss (for one batch) at step 363: 263829159936.0000\n","Training loss (for one batch) at step 364: 396832014336.0000\n","Training loss (for one batch) at step 365: 24167108608.0000\n","Training loss (for one batch) at step 366: 29162713088.0000\n","Training loss (for one batch) at step 367: 18135093248.0000\n","Training loss (for one batch) at step 368: 24515512320.0000\n","Training loss (for one batch) at step 369: 925073152.0000\n","Training loss (for one batch) at step 370: 7164473856.0000\n","Training loss (for one batch) at step 371: 29186967552.0000\n","Training loss (for one batch) at step 372: 72712445952.0000\n","Training loss (for one batch) at step 373: 6338332672.0000\n","Training loss (for one batch) at step 374: 3448061440.0000\n","Training loss (for one batch) at step 375: 263885357056.0000\n","Training loss (for one batch) at step 376: 22338879488.0000\n","Training loss (for one batch) at step 377: 6692360704.0000\n","Training loss (for one batch) at step 378: 38837977088.0000\n","Training loss (for one batch) at step 379: 69342576640.0000\n","Training loss (for one batch) at step 380: 8694386688.0000\n","Training loss (for one batch) at step 381: 8709369856.0000\n","Training loss (for one batch) at step 382: 132860469248.0000\n","Training loss (for one batch) at step 383: 12502779904.0000\n","Training loss (for one batch) at step 384: 3032872648704.0000\n","Training loss (for one batch) at step 385: 6424204800.0000\n","Training loss (for one batch) at step 386: 411694563328.0000\n","Training loss (for one batch) at step 387: 5574481408.0000\n","Training loss (for one batch) at step 388: 50594070528.0000\n","Training loss (for one batch) at step 389: 58699079680.0000\n","Training loss (for one batch) at step 390: 188104392704.0000\n","Training loss (for one batch) at step 391: 3106377216.0000\n","Training loss (for one batch) at step 392: 53141340160.0000\n","Training loss (for one batch) at step 393: 18595926016.0000\n","Training loss (for one batch) at step 394: 11357673472.0000\n","Training loss (for one batch) at step 395: 45245407232.0000\n","Training loss (for one batch) at step 396: 58819350528.0000\n","Training loss (for one batch) at step 397: 4872631808.0000\n","Training loss (for one batch) at step 398: 2840981760.0000\n","Training loss (for one batch) at step 399: 58078945280.0000\n","Training loss (for one batch) at step 400: 11268416512.0000\n","Training loss (for one batch) at step 401: 39419154432.0000\n","Training loss (for one batch) at step 402: 91741110272.0000\n","Training loss (for one batch) at step 403: 455047053312.0000\n","Training loss (for one batch) at step 404: 10114650112.0000\n","Training loss (for one batch) at step 405: 118954950656.0000\n","Training loss (for one batch) at step 406: 69379112960.0000\n","Training loss (for one batch) at step 407: 3721309440.0000\n","Training loss (for one batch) at step 408: 274016632832.0000\n","Training loss (for one batch) at step 409: 301110001664.0000\n","Training loss (for one batch) at step 410: 7511950848.0000\n","Training loss (for one batch) at step 411: 62950313984.0000\n","Training loss (for one batch) at step 412: 95404720128.0000\n","Training loss (for one batch) at step 413: 306027823104.0000\n","Training loss (for one batch) at step 414: 6620657664.0000\n","Training loss (for one batch) at step 415: 19328376832.0000\n","Training loss (for one batch) at step 416: 2723911168.0000\n","Training loss (for one batch) at step 417: 24182476800.0000\n","Training loss (for one batch) at step 418: 941244481536.0000\n","Training loss (for one batch) at step 419: 87107649536.0000\n","Training loss (for one batch) at step 420: 67579977728.0000\n","Training loss (for one batch) at step 421: 9186582528.0000\n","Training loss (for one batch) at step 422: 9596904448.0000\n","Training loss (for one batch) at step 423: 23815913472.0000\n","Training loss (for one batch) at step 424: 59805155328.0000\n","Training loss (for one batch) at step 425: 9260702720.0000\n","Training loss (for one batch) at step 426: 118882902016.0000\n","Training loss (for one batch) at step 427: 179320029184.0000\n","Training loss (for one batch) at step 428: 286706237440.0000\n","Training loss (for one batch) at step 429: 306693963776.0000\n","Training loss (for one batch) at step 430: 62786174976.0000\n","Training loss (for one batch) at step 431: 10745356288.0000\n","Training loss (for one batch) at step 432: 361483632640.0000\n","Training loss (for one batch) at step 433: 6997926912.0000\n","Training loss (for one batch) at step 434: 13041530880.0000\n","Training loss (for one batch) at step 435: 260622204928.0000\n","Training loss (for one batch) at step 436: 26671450112.0000\n","Training loss (for one batch) at step 437: 13818373120.0000\n","Training loss (for one batch) at step 438: 549548720128.0000\n","Training loss (for one batch) at step 439: 802034417664.0000\n","Training loss (for one batch) at step 440: 85760327680.0000\n","Training loss (for one batch) at step 441: 184220729344.0000\n","Training loss (for one batch) at step 442: 68824457216.0000\n","Training loss (for one batch) at step 443: 39113814016.0000\n","Training loss (for one batch) at step 444: 3707619072.0000\n","Training loss (for one batch) at step 445: 85894701056.0000\n","Training loss (for one batch) at step 446: 194184708096.0000\n","Training loss (for one batch) at step 447: 23474552832.0000\n","Training loss (for one batch) at step 448: 5716676608.0000\n","Training loss (for one batch) at step 449: 7354180096.0000\n","Training loss (for one batch) at step 450: 265454534656.0000\n","Training loss (for one batch) at step 451: 385670053888.0000\n","Training loss (for one batch) at step 452: 633062162432.0000\n","Training loss (for one batch) at step 453: 50042003456.0000\n","Training loss (for one batch) at step 454: 2638536769536.0000\n","Training loss (for one batch) at step 455: 70650896384.0000\n","Training loss (for one batch) at step 456: 24252393472.0000\n","Training loss (for one batch) at step 457: 77035421696.0000\n","Training loss (for one batch) at step 458: 29946523648.0000\n","Training loss (for one batch) at step 459: 32954081280.0000\n","Training loss (for one batch) at step 460: 292907057152.0000\n","Training loss (for one batch) at step 461: 4121719296.0000\n","Training loss (for one batch) at step 462: 79060115456.0000\n","Training loss (for one batch) at step 463: 16360987648.0000\n","Training loss (for one batch) at step 464: 9121746944.0000\n","Training loss (for one batch) at step 465: 18998337536.0000\n","Training loss (for one batch) at step 466: 6793193984.0000\n","Training loss (for one batch) at step 467: 5603672064.0000\n","Training loss (for one batch) at step 468: 195075211264.0000\n","Training loss (for one batch) at step 469: 49899773952.0000\n","Training loss (for one batch) at step 470: 46138011648.0000\n","Training loss (for one batch) at step 471: 92006432768.0000\n","Training loss (for one batch) at step 472: 9978225664.0000\n","Training loss (for one batch) at step 473: 7401142272.0000\n","Training loss (for one batch) at step 474: 55659610112.0000\n","Training loss (for one batch) at step 475: 557214990336.0000\n","Training loss (for one batch) at step 476: 15146127360.0000\n","Training loss (for one batch) at step 477: 29801850880.0000\n","Training loss (for one batch) at step 478: 22142732288.0000\n","Training loss (for one batch) at step 479: 336200990720.0000\n","Training loss (for one batch) at step 480: 7897748480.0000\n","Training loss (for one batch) at step 481: 27723409408.0000\n","Training loss (for one batch) at step 482: 20489259008.0000\n","Training loss (for one batch) at step 483: 48982888448.0000\n","Training loss (for one batch) at step 484: 216397103104.0000\n","Training loss (for one batch) at step 485: 1485251712.0000\n","Training loss (for one batch) at step 486: 2241498368.0000\n","Training loss (for one batch) at step 487: 164095737856.0000\n","Training loss (for one batch) at step 488: 5735104000.0000\n","Training loss (for one batch) at step 489: 73727508480.0000\n","Training loss (for one batch) at step 490: 85696446464.0000\n","Training loss (for one batch) at step 491: 82954993664.0000\n","Training loss (for one batch) at step 492: 4991032320.0000\n","Training loss (for one batch) at step 493: 10638029824.0000\n","Training loss (for one batch) at step 494: 7832989184.0000\n","Training loss (for one batch) at step 495: 27896969216.0000\n","Training loss (for one batch) at step 496: 282982744064.0000\n","Training loss (for one batch) at step 497: 48454299648.0000\n","Training loss (for one batch) at step 498: 10682362880.0000\n","Training loss (for one batch) at step 499: 92410216448.0000\n","Training loss (for one batch) at step 500: 8827374592.0000\n","Training loss (for one batch) at step 501: 3763168000.0000\n","Training loss (for one batch) at step 502: 34135932928.0000\n","Training loss (for one batch) at step 503: 20468916224.0000\n","Training loss (for one batch) at step 504: 3679855837184.0000\n","Training loss (for one batch) at step 505: 29222256640.0000\n","Training loss (for one batch) at step 506: 287283380224.0000\n","Training loss (for one batch) at step 507: 25300871168.0000\n","Training loss (for one batch) at step 508: 40539512832.0000\n","Training loss (for one batch) at step 509: 4382738432.0000\n","Training loss (for one batch) at step 510: 4230059520.0000\n","Training loss (for one batch) at step 511: 13743386624.0000\n","Training loss (for one batch) at step 512: 21167865856.0000\n","Training loss (for one batch) at step 513: 564918026240.0000\n","Training loss (for one batch) at step 514: 161770307584.0000\n","Training loss (for one batch) at step 515: 28039643136.0000\n","Training loss (for one batch) at step 516: 18405619712.0000\n","Training loss (for one batch) at step 517: 3099113728.0000\n","Training loss (for one batch) at step 518: 9054047232.0000\n","Training loss (for one batch) at step 519: 3615409152.0000\n","Training loss (for one batch) at step 520: 59733729280.0000\n","Training loss (for one batch) at step 521: 29934835712.0000\n","Training loss (for one batch) at step 522: 15288069120.0000\n","Training loss (for one batch) at step 523: 9493424128.0000\n","Training loss (for one batch) at step 524: 1909905408.0000\n","Training loss (for one batch) at step 525: 7939357696.0000\n","Training loss (for one batch) at step 526: 31559348224.0000\n","Training loss (for one batch) at step 527: 13132929024.0000\n","Training loss (for one batch) at step 528: 339856523264.0000\n","Training loss (for one batch) at step 529: 21609619456.0000\n","Training loss (for one batch) at step 530: 304217063424.0000\n","Training loss (for one batch) at step 531: 548270702592.0000\n","Training loss (for one batch) at step 532: 23172151296.0000\n","Training loss (for one batch) at step 533: 10922492928.0000\n","Training loss (for one batch) at step 534: 26532104192.0000\n","Training loss (for one batch) at step 535: 97978580992.0000\n","Training loss (for one batch) at step 536: 367367225344.0000\n","Training loss (for one batch) at step 537: 9305601024.0000\n","Training loss (for one batch) at step 538: 62201364480.0000\n","Training loss (for one batch) at step 539: 2858785280.0000\n","Training loss (for one batch) at step 540: 3685116416.0000\n","Training loss (for one batch) at step 541: 17103907840.0000\n","Training loss (for one batch) at step 542: 59535323136.0000\n","Training loss (for one batch) at step 543: 261770379264.0000\n","Training loss (for one batch) at step 544: 124287205376.0000\n","Training loss (for one batch) at step 545: 66747289600.0000\n","Training loss (for one batch) at step 546: 283366064128.0000\n","Training loss (for one batch) at step 547: 22807775232.0000\n","Training loss (for one batch) at step 548: 331229822976.0000\n","Training loss (for one batch) at step 549: 207562883072.0000\n","Training loss (for one batch) at step 550: 11232683008.0000\n","Training loss (for one batch) at step 551: 1729277952.0000\n","Training loss (for one batch) at step 552: 1023029056.0000\n","Training loss (for one batch) at step 553: 34529390592.0000\n","Training loss (for one batch) at step 554: 200637153280.0000\n","Training loss (for one batch) at step 555: 84869259264.0000\n","Training loss (for one batch) at step 556: 64736522240.0000\n","Training loss (for one batch) at step 557: 26464458752.0000\n","Training loss (for one batch) at step 558: 11941837824.0000\n","Training loss (for one batch) at step 559: 6000385024.0000\n","Training loss (for one batch) at step 560: 20129728512.0000\n","Training loss (for one batch) at step 561: 2375115776.0000\n","Training loss (for one batch) at step 562: 32837339136.0000\n","Training loss (for one batch) at step 563: 26794766336.0000\n","Training loss (for one batch) at step 564: 8480961536.0000\n","Training loss (for one batch) at step 565: 159396855808.0000\n","Training loss (for one batch) at step 566: 12352003072.0000\n","Training loss (for one batch) at step 567: 1187344547840.0000\n","Training loss (for one batch) at step 568: 6117160448.0000\n","Training loss (for one batch) at step 569: 22720923648.0000\n","Training loss (for one batch) at step 570: 6517913088.0000\n","Training loss (for one batch) at step 571: 222393319424.0000\n","Training loss (for one batch) at step 572: 264680177664.0000\n","Training loss (for one batch) at step 573: 2536272384.0000\n","Training loss (for one batch) at step 574: 35139407872.0000\n","Training loss (for one batch) at step 575: 11977180160.0000\n","Training loss (for one batch) at step 576: 61578903552.0000\n","Training loss (for one batch) at step 577: 119799726080.0000\n","Training loss (for one batch) at step 578: 12410778624.0000\n","Training loss (for one batch) at step 579: 81087045632.0000\n","Training loss (for one batch) at step 580: 81919574016.0000\n","Training loss (for one batch) at step 581: 65468944384.0000\n","Training loss (for one batch) at step 582: 16001853440.0000\n","Training loss (for one batch) at step 583: 13526116352.0000\n","Training loss (for one batch) at step 584: 18243538944.0000\n","Training loss (for one batch) at step 585: 44404539392.0000\n","Training loss (for one batch) at step 586: 589205209088.0000\n","Training loss (for one batch) at step 587: 20874000384.0000\n","Training loss (for one batch) at step 588: 180587872256.0000\n","Training loss (for one batch) at step 589: 20340123648.0000\n","Training loss (for one batch) at step 590: 249587302400.0000\n","Training loss (for one batch) at step 591: 36946673664.0000\n","Training loss (for one batch) at step 592: 243817332736.0000\n","Training loss (for one batch) at step 593: 7108673536.0000\n","Training loss (for one batch) at step 594: 32976957440.0000\n","Training loss (for one batch) at step 595: 493222461440.0000\n","Training loss (for one batch) at step 596: 1727844057088.0000\n","Training loss (for one batch) at step 597: 76630007808.0000\n","Training loss (for one batch) at step 598: 3656397824.0000\n","Training loss (for one batch) at step 599: 76443918336.0000\n","Training loss (for one batch) at step 600: 26074664960.0000\n","Training loss (for one batch) at step 601: 13960641536.0000\n","Training loss (for one batch) at step 602: 29497313280.0000\n","Training loss (for one batch) at step 603: 163605610496.0000\n","Training loss (for one batch) at step 604: 22814978048.0000\n","Training loss (for one batch) at step 605: 315264303104.0000\n","Training loss (for one batch) at step 606: 370968068096.0000\n","Training loss (for one batch) at step 607: 541885235200.0000\n","Training loss (for one batch) at step 608: 69593776128.0000\n","Training loss (for one batch) at step 609: 4883850752.0000\n","Training loss (for one batch) at step 610: 8861360128.0000\n","Training loss (for one batch) at step 611: 153092046848.0000\n","Training loss (for one batch) at step 612: 29030090752.0000\n","Training loss (for one batch) at step 613: 14346489856.0000\n","Training loss (for one batch) at step 614: 108568035328.0000\n","Training loss (for one batch) at step 615: 5933254656.0000\n","Training loss (for one batch) at step 616: 1288181383168.0000\n","Training loss (for one batch) at step 617: 158973001728.0000\n","Training loss (for one batch) at step 618: 669630857216.0000\n","Training loss (for one batch) at step 619: 28943437824.0000\n","Training loss (for one batch) at step 620: 17892472832.0000\n","Training loss (for one batch) at step 621: 736969228288.0000\n","Training loss (for one batch) at step 622: 6571976704.0000\n","Training loss (for one batch) at step 623: 3799457792.0000\n","Training loss (for one batch) at step 624: 2959650048.0000\n","Training loss (for one batch) at step 625: 21635475456.0000\n","Training loss (for one batch) at step 626: 58675331072.0000\n","Training loss (for one batch) at step 627: 2040526848.0000\n","Training loss (for one batch) at step 628: 4264424192.0000\n","Training loss (for one batch) at step 629: 4636323840.0000\n","Training loss (for one batch) at step 630: 8427619328.0000\n","Training loss (for one batch) at step 631: 92409896960.0000\n","Training loss (for one batch) at step 632: 483067756544.0000\n","Training loss (for one batch) at step 633: 776033796096.0000\n","Training loss (for one batch) at step 634: 115685834752.0000\n","Training loss (for one batch) at step 635: 12527860736.0000\n","Training loss (for one batch) at step 636: 138487889920.0000\n","Training loss (for one batch) at step 637: 3988436736.0000\n","Training loss (for one batch) at step 638: 24376295424.0000\n","Training loss (for one batch) at step 639: 15165259776.0000\n","Training loss (for one batch) at step 640: 95099756544.0000\n","Training loss (for one batch) at step 641: 961205824.0000\n","Training loss (for one batch) at step 642: 4389588992.0000\n","Training loss (for one batch) at step 643: 80959168512.0000\n","Training loss (for one batch) at step 644: 71396073472.0000\n","Training loss (for one batch) at step 645: 7607616000.0000\n","Training loss (for one batch) at step 646: 11131844608.0000\n","Training loss (for one batch) at step 647: 12894496768.0000\n","Training loss (for one batch) at step 648: 34172923904.0000\n","Training loss (for one batch) at step 649: 710504153088.0000\n","Training loss (for one batch) at step 650: 15708069888.0000\n","Training loss (for one batch) at step 651: 20943693824.0000\n","Training loss (for one batch) at step 652: 16730356736.0000\n","Training loss (for one batch) at step 653: 33013264384.0000\n","Training loss (for one batch) at step 654: 5916407808.0000\n","Training loss (for one batch) at step 655: 248810504192.0000\n","Training loss (for one batch) at step 656: 5353505280.0000\n","Training loss (for one batch) at step 657: 536750391296.0000\n","Training loss (for one batch) at step 658: 3228468736.0000\n","Training loss (for one batch) at step 659: 1841474816.0000\n","Training loss (for one batch) at step 660: 2949745920.0000\n","Training loss (for one batch) at step 661: 151059070976.0000\n","Training loss (for one batch) at step 662: 4212358144.0000\n","Training loss (for one batch) at step 663: 141474594816.0000\n","Training loss (for one batch) at step 664: 10287429632.0000\n","Training loss (for one batch) at step 665: 163329441792.0000\n","Training loss (for one batch) at step 666: 620142133248.0000\n","Training loss (for one batch) at step 667: 35356934144.0000\n","Training loss (for one batch) at step 668: 27899721728.0000\n","Training loss (for one batch) at step 669: 20161576960.0000\n","Training loss (for one batch) at step 670: 1441821056.0000\n","Training loss (for one batch) at step 671: 26617554944.0000\n","Training loss (for one batch) at step 672: 164070948864.0000\n","Training loss (for one batch) at step 673: 4900617728.0000\n","Training loss (for one batch) at step 674: 10829394944.0000\n","Training loss (for one batch) at step 675: 54399176704.0000\n","Training loss (for one batch) at step 676: 70174883840.0000\n","Training loss (for one batch) at step 677: 10124704768.0000\n","Training loss (for one batch) at step 678: 26612633600.0000\n","Training loss (for one batch) at step 679: 14348088320.0000\n","Training loss (for one batch) at step 680: 2751236864.0000\n","Training loss (for one batch) at step 681: 4914038784.0000\n","Training loss (for one batch) at step 682: 8811465728.0000\n","Training loss (for one batch) at step 683: 3721776640.0000\n","Training loss (for one batch) at step 684: 110536507392.0000\n","Training loss (for one batch) at step 685: 5166583296.0000\n","Training loss (for one batch) at step 686: 185247137792.0000\n","Training loss (for one batch) at step 687: 195359670272.0000\n","Training loss (for one batch) at step 688: 70522978304.0000\n","Training loss (for one batch) at step 689: 180562427904.0000\n","Training loss (for one batch) at step 690: 13600325632.0000\n","Training loss (for one batch) at step 691: 22881865728.0000\n","Training loss (for one batch) at step 692: 10369727488.0000\n","Training loss (for one batch) at step 693: 80754515968.0000\n","Training loss (for one batch) at step 694: 1146711703552.0000\n","Training loss (for one batch) at step 695: 19336769536.0000\n","Training loss (for one batch) at step 696: 4488995840.0000\n","Training loss (for one batch) at step 697: 2471163854848.0000\n","Training loss (for one batch) at step 698: 170834460672.0000\n","Training loss (for one batch) at step 699: 10539665408.0000\n","Training loss (for one batch) at step 700: 932184588288.0000\n","Training loss (for one batch) at step 701: 2915164672.0000\n","Training loss (for one batch) at step 702: 5657396224.0000\n","Training loss (for one batch) at step 703: 315831517184.0000\n","Training loss (for one batch) at step 704: 30256967680.0000\n","Training loss (for one batch) at step 705: 4205574656.0000\n","Training loss (for one batch) at step 706: 203458691072.0000\n","Training loss (for one batch) at step 707: 48502996992.0000\n","Training loss (for one batch) at step 708: 20309774336.0000\n","Training loss (for one batch) at step 709: 25067786240.0000\n","Training loss (for one batch) at step 710: 38756638720.0000\n","Training loss (for one batch) at step 711: 67748347904.0000\n","Training loss (for one batch) at step 712: 240794009600.0000\n","Training loss (for one batch) at step 713: 118121881600.0000\n","Training loss (for one batch) at step 714: 11645151232.0000\n","Training loss (for one batch) at step 715: 223178178560.0000\n","Training loss (for one batch) at step 716: 144761176064.0000\n","Training loss (for one batch) at step 717: 104037998592.0000\n","Training loss (for one batch) at step 718: 465057087488.0000\n","Training loss (for one batch) at step 719: 72055455744.0000\n","Training loss (for one batch) at step 720: 11846505472.0000\n","Training loss (for one batch) at step 721: 8795985920.0000\n","Training loss (for one batch) at step 722: 28317204480.0000\n","Training loss (for one batch) at step 723: 108265234432.0000\n","Training loss (for one batch) at step 724: 12788770816.0000\n","Training loss (for one batch) at step 725: 4900617216.0000\n","Training loss (for one batch) at step 726: 7030462464.0000\n","Training loss (for one batch) at step 727: 55147298816.0000\n","Training loss (for one batch) at step 728: 6587169280.0000\n","Training loss (for one batch) at step 729: 83143499776.0000\n","Training loss (for one batch) at step 730: 11232424960.0000\n","Training loss (for one batch) at step 731: 62169448448.0000\n","Training loss (for one batch) at step 732: 12254863360.0000\n","Training loss (for one batch) at step 733: 8515370496.0000\n","Training loss (for one batch) at step 734: 18333345792.0000\n","Training loss (for one batch) at step 735: 300350832640.0000\n","Training loss (for one batch) at step 736: 5675552768.0000\n","Training loss (for one batch) at step 737: 19855452160.0000\n","Training loss (for one batch) at step 738: 81707474944.0000\n","Training loss (for one batch) at step 739: 16210757632.0000\n","Training loss (for one batch) at step 740: 27277053952.0000\n","Training loss (for one batch) at step 741: 274109988864.0000\n","Training loss (for one batch) at step 742: 1335778344960.0000\n","Training loss (for one batch) at step 743: 292603232256.0000\n","Training loss (for one batch) at step 744: 152852692992.0000\n","Training loss (for one batch) at step 745: 198703677440.0000\n","Training loss (for one batch) at step 746: 8197069824.0000\n","Training loss (for one batch) at step 747: 4513776128.0000\n","Training loss (for one batch) at step 748: 28012832768.0000\n","Training loss (for one batch) at step 749: 25702768640.0000\n","Training loss (for one batch) at step 750: 76132343808.0000\n","Training loss (for one batch) at step 751: 154364559360.0000\n","Training loss (for one batch) at step 752: 4075252224.0000\n","Training loss (for one batch) at step 753: 6721643520.0000\n","Training loss (for one batch) at step 754: 17153125376.0000\n","Training loss (for one batch) at step 755: 14449717248.0000\n","Training loss (for one batch) at step 756: 9600399360.0000\n","Training loss (for one batch) at step 757: 801170849792.0000\n","Training loss (for one batch) at step 758: 62889844736.0000\n","Training loss (for one batch) at step 759: 38302068736.0000\n","Training loss (for one batch) at step 760: 141819625472.0000\n","Training loss (for one batch) at step 761: 12454045696.0000\n","Training loss (for one batch) at step 762: 10683882496.0000\n","Training loss (for one batch) at step 763: 22091763712.0000\n","Training loss (for one batch) at step 764: 5391370240.0000\n","Training loss (for one batch) at step 765: 30884321280.0000\n","Training loss (for one batch) at step 766: 18251620352.0000\n","Training loss (for one batch) at step 767: 570712640.0000\n","Training loss (for one batch) at step 768: 28009943040.0000\n","Training loss (for one batch) at step 769: 4552453632.0000\n","Training loss (for one batch) at step 770: 9746946048.0000\n","Training loss (for one batch) at step 771: 136845328384.0000\n","Training loss (for one batch) at step 772: 34332643328.0000\n","Training loss (for one batch) at step 773: 61509820416.0000\n","Training loss (for one batch) at step 774: 118432956416.0000\n","Training loss (for one batch) at step 775: 6947732480.0000\n","Training loss (for one batch) at step 776: 1351503970304.0000\n","Training loss (for one batch) at step 777: 24518412288.0000\n","Training loss (for one batch) at step 778: 1643135488.0000\n","Training loss (for one batch) at step 779: 202677256192.0000\n","Training loss (for one batch) at step 780: 76095856640.0000\n","Training loss (for one batch) at step 781: 319961595904.0000\n","Training loss (for one batch) at step 782: 3612787712.0000\n","Training loss (for one batch) at step 783: 5989071360.0000\n","Training loss (for one batch) at step 784: 16243620864.0000\n","Training loss (for one batch) at step 785: 11510942720.0000\n","Training loss (for one batch) at step 786: 218918682624.0000\n","Training loss (for one batch) at step 787: 48622411776.0000\n","Training loss (for one batch) at step 788: 85801091072.0000\n","Training loss (for one batch) at step 789: 209696243712.0000\n","Training loss (for one batch) at step 790: 211976093696.0000\n","Training loss (for one batch) at step 791: 9289073664.0000\n","Training loss (for one batch) at step 792: 52808933376.0000\n","Training loss (for one batch) at step 793: 30979708928.0000\n","Training loss (for one batch) at step 794: 5070469988352.0000\n","Training loss (for one batch) at step 795: 9788849152.0000\n","Training loss (for one batch) at step 796: 143774941184.0000\n","Training loss (for one batch) at step 797: 16688586752.0000\n","Training loss (for one batch) at step 798: 40012374016.0000\n","Training loss (for one batch) at step 799: 9545391104.0000\n","Training loss (for one batch) at step 800: 178337759232.0000\n","Training loss (for one batch) at step 801: 217216221184.0000\n","Training loss (for one batch) at step 802: 9813852160.0000\n","Training loss (for one batch) at step 803: 11798850560.0000\n","Training loss (for one batch) at step 804: 37546868736.0000\n","Training loss (for one batch) at step 805: 40043802624.0000\n","Training loss (for one batch) at step 806: 3730975232.0000\n","Training loss (for one batch) at step 807: 232977022976.0000\n","Training loss (for one batch) at step 808: 19194427392.0000\n","Training loss (for one batch) at step 809: 71554924544.0000\n","Training loss (for one batch) at step 810: 2965034752.0000\n","Training loss (for one batch) at step 811: 2209706672128.0000\n","Training loss (for one batch) at step 812: 38943399936.0000\n","Training loss (for one batch) at step 813: 3803446784.0000\n","Training loss (for one batch) at step 814: 5807587840.0000\n","Training loss (for one batch) at step 815: 9025357824.0000\n","Training loss (for one batch) at step 816: 1363122304.0000\n","Training loss (for one batch) at step 817: 212979957760.0000\n","Training loss (for one batch) at step 818: 40299630592.0000\n","Training loss (for one batch) at step 819: 15145524224.0000\n","Training loss (for one batch) at step 820: 2865425408.0000\n","Training loss (for one batch) at step 821: 24585752576.0000\n","Training loss (for one batch) at step 822: 4799831040.0000\n","Training loss (for one batch) at step 823: 184897028096.0000\n","Training loss (for one batch) at step 824: 29973610496.0000\n","Training loss (for one batch) at step 825: 3510521344.0000\n","Training loss (for one batch) at step 826: 314972241920.0000\n","Training loss (for one batch) at step 827: 17851080704.0000\n","Training loss (for one batch) at step 828: 30005237760.0000\n","Training loss (for one batch) at step 829: 6419397120.0000\n","Training loss (for one batch) at step 830: 341567176704.0000\n","Training loss (for one batch) at step 831: 1917913856.0000\n","Training loss (for one batch) at step 832: 85151866880.0000\n","Training loss (for one batch) at step 833: 7163701248.0000\n","Training loss (for one batch) at step 834: 5543363584.0000\n","Training loss (for one batch) at step 835: 157824532480.0000\n","Training loss (for one batch) at step 836: 41080082432.0000\n","Training loss (for one batch) at step 837: 403636551680.0000\n","Training loss (for one batch) at step 838: 6356455424.0000\n","Training loss (for one batch) at step 839: 52997603328.0000\n","Training loss (for one batch) at step 840: 7490162688.0000\n","Training loss (for one batch) at step 841: 37634949120.0000\n","Training loss (for one batch) at step 842: 47863517184.0000\n","Training loss (for one batch) at step 843: 60740026368.0000\n","Training loss (for one batch) at step 844: 226894561280.0000\n","Training loss (for one batch) at step 845: 168852078592.0000\n","Training loss (for one batch) at step 846: 14928102400.0000\n","Training loss (for one batch) at step 847: 530795724800.0000\n","Training loss (for one batch) at step 848: 3792033280.0000\n","Training loss (for one batch) at step 849: 113813962752.0000\n","Training loss (for one batch) at step 850: 225357660160.0000\n","Training loss (for one batch) at step 851: 1154728722432.0000\n","Training loss (for one batch) at step 852: 12456463360.0000\n","Training loss (for one batch) at step 853: 12098375680.0000\n","Training loss (for one batch) at step 854: 1068801916928.0000\n","Training loss (for one batch) at step 855: 9462447104.0000\n","Training loss (for one batch) at step 856: 221351198720.0000\n","Training loss (for one batch) at step 857: 2244666368.0000\n","Training loss (for one batch) at step 858: 4419506176.0000\n","Training loss (for one batch) at step 859: 13890833408.0000\n","Training loss (for one batch) at step 860: 162431893504.0000\n","Training loss (for one batch) at step 861: 280295931904.0000\n","Training loss (for one batch) at step 862: 7306577920.0000\n","Training loss (for one batch) at step 863: 342557229056.0000\n","Training loss (for one batch) at step 864: 284742156288.0000\n","Training loss (for one batch) at step 865: 77667614720.0000\n","Training loss (for one batch) at step 866: 295948124160.0000\n","Training loss (for one batch) at step 867: 12113659904.0000\n","Training loss (for one batch) at step 868: 139779817472.0000\n","Training loss (for one batch) at step 869: 25803880448.0000\n","Training loss (for one batch) at step 870: 442456309760.0000\n","Training loss (for one batch) at step 871: 577357873152.0000\n","Training loss (for one batch) at step 872: 174379384832.0000\n","Training loss (for one batch) at step 873: 1269761920.0000\n","Training loss (for one batch) at step 874: 86689906688.0000\n","Training loss (for one batch) at step 875: 175804383232.0000\n","Training loss (for one batch) at step 876: 7577249280.0000\n","Training loss (for one batch) at step 877: 183572774912.0000\n","Training loss (for one batch) at step 878: 700244754432.0000\n","Training loss (for one batch) at step 879: 67734585344.0000\n","Training loss (for one batch) at step 880: 3148091904.0000\n","Training loss (for one batch) at step 881: 18423312384.0000\n","Training loss (for one batch) at step 882: 4394202112.0000\n","Training loss (for one batch) at step 883: 10756829184.0000\n","Training loss (for one batch) at step 884: 43174346752.0000\n","Training loss (for one batch) at step 885: 7279606784.0000\n","Training loss (for one batch) at step 886: 16015718400.0000\n","Training loss (for one batch) at step 887: 169552494592.0000\n","Training loss (for one batch) at step 888: 1874131025920.0000\n","Training loss (for one batch) at step 889: 25224204288.0000\n","Training loss (for one batch) at step 890: 240731439104.0000\n","Training loss (for one batch) at step 891: 353614266368.0000\n","Training loss (for one batch) at step 892: 96962838528.0000\n","Training loss (for one batch) at step 893: 3442432278528.0000\n","Training loss (for one batch) at step 894: 8201631744.0000\n","Training loss (for one batch) at step 895: 364288081920.0000\n","Training loss (for one batch) at step 896: 113833623552.0000\n","Training loss (for one batch) at step 897: 299779588096.0000\n","Training loss (for one batch) at step 898: 941117210624.0000\n","Training loss (for one batch) at step 899: 382946836480.0000\n","Training loss (for one batch) at step 900: 416577257472.0000\n","Training loss (for one batch) at step 901: 197697880064.0000\n","Training loss (for one batch) at step 902: 8109392384.0000\n","Training loss (for one batch) at step 903: 27131295744.0000\n","Training loss (for one batch) at step 904: 16703748096.0000\n","Training loss (for one batch) at step 905: 2953675776.0000\n","Training loss (for one batch) at step 906: 2716113920.0000\n","Training loss (for one batch) at step 907: 64036765696.0000\n","Training loss (for one batch) at step 908: 225804582912.0000\n","Training loss (for one batch) at step 909: 45651673088.0000\n","Training loss (for one batch) at step 910: 6148508672.0000\n","Training loss (for one batch) at step 911: 13050982400.0000\n","Training loss (for one batch) at step 912: 19213772800.0000\n","Training loss (for one batch) at step 913: 92516966400.0000\n","Training loss (for one batch) at step 914: 18690981888.0000\n","Training loss (for one batch) at step 915: 160956235776.0000\n","Training loss (for one batch) at step 916: 9185914880.0000\n","Training loss (for one batch) at step 917: 577346142208.0000\n","Training loss (for one batch) at step 918: 19695183872.0000\n","Training loss (for one batch) at step 919: 3108432640.0000\n","Training loss (for one batch) at step 920: 10917793792.0000\n","Training loss (for one batch) at step 921: 151307337728.0000\n","Training loss (for one batch) at step 922: 7701916160.0000\n","Training loss (for one batch) at step 923: 799578718208.0000\n","Training loss (for one batch) at step 924: 4160831744.0000\n","Training loss (for one batch) at step 925: 14119329792.0000\n","Training loss (for one batch) at step 926: 18116007936.0000\n","Training loss (for one batch) at step 927: 93925163008.0000\n","Training loss (for one batch) at step 928: 7393104384.0000\n","Training loss (for one batch) at step 929: 28793915392.0000\n","Training loss (for one batch) at step 930: 6228022272.0000\n","Training loss (for one batch) at step 931: 23892631552.0000\n","Training loss (for one batch) at step 932: 6135024640.0000\n","Training loss (for one batch) at step 933: 2432660224.0000\n","Training loss (for one batch) at step 934: 19157981184.0000\n","Training loss (for one batch) at step 935: 15194088448.0000\n","Training loss (for one batch) at step 936: 4888012800.0000\n","Training loss (for one batch) at step 937: 779948392448.0000\n","Training loss (for one batch) at step 938: 310142894080.0000\n","Training loss (for one batch) at step 939: 533294972928.0000\n","Training loss (for one batch) at step 940: 2174857216.0000\n","Training loss (for one batch) at step 941: 28954077184.0000\n","Training loss (for one batch) at step 942: 9672512512.0000\n","Training loss (for one batch) at step 943: 11301953536.0000\n","Training loss (for one batch) at step 944: 534530293760.0000\n","Training loss (for one batch) at step 945: 44588695552.0000\n","Training loss (for one batch) at step 946: 101984722944.0000\n","Training loss (for one batch) at step 947: 261874679808.0000\n","Training loss (for one batch) at step 948: 262685523968.0000\n","Training loss (for one batch) at step 949: 89679765504.0000\n","Training loss (for one batch) at step 950: 94875836416.0000\n","Training loss (for one batch) at step 951: 4918464000.0000\n","Training loss (for one batch) at step 952: 6128995328.0000\n","Training loss (for one batch) at step 953: 245346877440.0000\n","Training loss (for one batch) at step 954: 154594738176.0000\n","Training loss (for one batch) at step 955: 4331673600.0000\n","Training loss (for one batch) at step 956: 4151838720.0000\n","Training loss (for one batch) at step 957: 253150871552.0000\n","Training loss (for one batch) at step 958: 12422858752.0000\n","Training loss (for one batch) at step 959: 189728571392.0000\n","Training loss (for one batch) at step 960: 75034894336.0000\n","Training loss (for one batch) at step 961: 15874302976.0000\n","Training loss (for one batch) at step 962: 191338987520.0000\n","Training loss (for one batch) at step 963: 102956351488.0000\n","Training loss (for one batch) at step 964: 4850731520.0000\n","Training loss (for one batch) at step 965: 59746983936.0000\n","Training loss (for one batch) at step 966: 205104840704.0000\n","Training loss (for one batch) at step 967: 32504598528.0000\n","Training loss (for one batch) at step 968: 45958189056.0000\n","Training loss (for one batch) at step 969: 18451558400.0000\n","Training loss (for one batch) at step 970: 3048156928.0000\n","Training loss (for one batch) at step 971: 98522906624.0000\n","Training loss (for one batch) at step 972: 28079468544.0000\n","Training loss (for one batch) at step 973: 3267342592.0000\n","Training loss (for one batch) at step 974: 1841272704.0000\n","Training loss (for one batch) at step 975: 40117239808.0000\n","Training loss (for one batch) at step 976: 16300588032.0000\n","Training loss (for one batch) at step 977: 777147056128.0000\n","Training loss (for one batch) at step 978: 20202684416.0000\n","Training loss (for one batch) at step 979: 11646733312.0000\n","Training loss (for one batch) at step 980: 73794191360.0000\n","Training loss (for one batch) at step 981: 1692429056.0000\n","Training loss (for one batch) at step 982: 81256595456.0000\n","Training loss (for one batch) at step 983: 2256912128.0000\n","Training loss (for one batch) at step 984: 5390941184.0000\n","Training loss (for one batch) at step 985: 3630239744.0000\n","Training loss (for one batch) at step 986: 43423006720.0000\n","Training loss (for one batch) at step 987: 433572773888.0000\n","Training loss (for one batch) at step 988: 16002451456.0000\n","Training loss (for one batch) at step 989: 211543392256.0000\n","Training loss (for one batch) at step 990: 217667452928.0000\n","Training loss (for one batch) at step 991: 105006153728.0000\n","Training loss (for one batch) at step 992: 55898533888.0000\n","Training loss (for one batch) at step 993: 4347433984.0000\n","Training loss (for one batch) at step 994: 7921623040.0000\n","Training loss (for one batch) at step 995: 3206693120.0000\n","Training loss (for one batch) at step 996: 128201252864.0000\n","Training loss (for one batch) at step 997: 7732073472.0000\n","Training loss (for one batch) at step 998: 408589271040.0000\n","Training loss (for one batch) at step 999: 14885271552.0000\n","Training loss (for one batch) at step 1000: 16671408128.0000\n","Training loss (for one batch) at step 1001: 6342898176.0000\n","Training loss (for one batch) at step 1002: 12412007424.0000\n","Training loss (for one batch) at step 1003: 1179079802880.0000\n","Training loss (for one batch) at step 1004: 7369911808.0000\n","Training loss (for one batch) at step 1005: 14355662848.0000\n","Training loss (for one batch) at step 1006: 46654312448.0000\n","Training loss (for one batch) at step 1007: 26290587648.0000\n","Training loss (for one batch) at step 1008: 18343516160.0000\n","Training loss (for one batch) at step 1009: 54318137344.0000\n","Training loss (for one batch) at step 1010: 15865445376.0000\n","Training loss (for one batch) at step 1011: 101399379968.0000\n","Training loss (for one batch) at step 1012: 12873016320.0000\n","Training loss (for one batch) at step 1013: 18532022272.0000\n","Training loss (for one batch) at step 1014: 26027108352.0000\n","Training loss (for one batch) at step 1015: 21014315008.0000\n","Training loss (for one batch) at step 1016: 61451370496.0000\n","Training loss (for one batch) at step 1017: 3607261696.0000\n","Training loss (for one batch) at step 1018: 70550945792.0000\n","Training loss (for one batch) at step 1019: 301757431808.0000\n","Training loss (for one batch) at step 1020: 186217234432.0000\n","Training loss (for one batch) at step 1021: 4713722880.0000\n","Training loss (for one batch) at step 1022: 438561046528.0000\n","Training loss (for one batch) at step 1023: 11746205696.0000\n","Training loss (for one batch) at step 1024: 3537532416.0000\n","Training loss (for one batch) at step 1025: 5453410816.0000\n","Training loss (for one batch) at step 1026: 23239782400.0000\n","Training loss (for one batch) at step 1027: 414717771776.0000\n","Training loss (for one batch) at step 1028: 112493821952.0000\n","Training loss (for one batch) at step 1029: 161566162944.0000\n","Training loss (for one batch) at step 1030: 37335552000.0000\n","Training loss (for one batch) at step 1031: 131210723328.0000\n","Training loss (for one batch) at step 1032: 4005656576.0000\n","Training loss (for one batch) at step 1033: 16459455488.0000\n","Training loss (for one batch) at step 1034: 8564272128.0000\n","Training loss (for one batch) at step 1035: 68922490880.0000\n","Training loss (for one batch) at step 1036: 48642019328.0000\n","Training loss (for one batch) at step 1037: 58240212992.0000\n","Training loss (for one batch) at step 1038: 301515931648.0000\n","Training loss (for one batch) at step 1039: 1256719515648.0000\n","Training loss (for one batch) at step 1040: 89859497984.0000\n","Training loss (for one batch) at step 1041: 14474220544.0000\n","Training loss (for one batch) at step 1042: 59791343616.0000\n","Training loss (for one batch) at step 1043: 315223506944.0000\n","Training loss (for one batch) at step 1044: 41179385856.0000\n","Training loss (for one batch) at step 1045: 2107391872.0000\n","Training loss (for one batch) at step 1046: 312372822016.0000\n","Training loss (for one batch) at step 1047: 61119229952.0000\n","Training loss (for one batch) at step 1048: 253050257408.0000\n","Training loss (for one batch) at step 1049: 54542929920.0000\n","Training loss (for one batch) at step 1050: 235192008704.0000\n","Training loss (for one batch) at step 1051: 8631088128.0000\n","Training loss (for one batch) at step 1052: 17853521920.0000\n","Training loss (for one batch) at step 1053: 1453575168.0000\n","Training loss (for one batch) at step 1054: 106135289856.0000\n","Training loss (for one batch) at step 1055: 15762805760.0000\n","Training loss (for one batch) at step 1056: 152656494592.0000\n","Training loss (for one batch) at step 1057: 177784520704.0000\n","Training loss (for one batch) at step 1058: 3565995008.0000\n","Training loss (for one batch) at step 1059: 18807779328.0000\n","Training loss (for one batch) at step 1060: 67962015744.0000\n","Training loss (for one batch) at step 1061: 146119819264.0000\n","Training loss (for one batch) at step 1062: 56699973632.0000\n","Training loss (for one batch) at step 1063: 20417789952.0000\n","Training loss (for one batch) at step 1064: 363357011968.0000\n","Training loss (for one batch) at step 1065: 1575980288.0000\n","Training loss (for one batch) at step 1066: 148623360000.0000\n","Training loss (for one batch) at step 1067: 114745147392.0000\n","Training loss (for one batch) at step 1068: 3233467858944.0000\n","Training loss (for one batch) at step 1069: 6527905792.0000\n","Training loss (for one batch) at step 1070: 30837039104.0000\n","Training loss (for one batch) at step 1071: 734911135744.0000\n","Training loss (for one batch) at step 1072: 135386365952.0000\n","Training loss (for one batch) at step 1073: 8501218816.0000\n","Training loss (for one batch) at step 1074: 41567485952.0000\n","Training loss (for one batch) at step 1075: 90682425344.0000\n","Training loss (for one batch) at step 1076: 4248286208.0000\n","Training loss (for one batch) at step 1077: 7906670592.0000\n","Training loss (for one batch) at step 1078: 6734108160.0000\n","Training loss (for one batch) at step 1079: 64443351040.0000\n","Training loss (for one batch) at step 1080: 604026372096.0000\n","Training loss (for one batch) at step 1081: 100568326144.0000\n","Training loss (for one batch) at step 1082: 1453187328.0000\n","Training loss (for one batch) at step 1083: 145631412224.0000\n","Training loss (for one batch) at step 1084: 51684319232.0000\n","Training loss (for one batch) at step 1085: 445963456.0000\n","Training loss (for one batch) at step 1086: 31095842816.0000\n","Training loss (for one batch) at step 1087: 111618097152.0000\n","Training loss (for one batch) at step 1088: 1762449024.0000\n","Training loss (for one batch) at step 1089: 231559888896.0000\n","Training loss (for one batch) at step 1090: 242541658112.0000\n","Training loss (for one batch) at step 1091: 20707475456.0000\n","Training loss (for one batch) at step 1092: 56622137344.0000\n","Training loss (for one batch) at step 1093: 205891846144.0000\n","Training loss (for one batch) at step 1094: 599322525696.0000\n","Training loss (for one batch) at step 1095: 75790032896.0000\n","Training loss (for one batch) at step 1096: 20269144.0000\n","Training loss (for one batch) at step 1097: 11915384832.0000\n","Training loss (for one batch) at step 1098: 30540206080.0000\n","Training loss (for one batch) at step 1099: 315412119552.0000\n","Training loss (for one batch) at step 1100: 21305225216.0000\n","Training loss (for one batch) at step 1101: 61910167552.0000\n","Training loss (for one batch) at step 1102: 35189944320.0000\n","Training loss (for one batch) at step 1103: 4795499520.0000\n","Training loss (for one batch) at step 1104: 13402712064.0000\n","Training loss (for one batch) at step 1105: 238189674496.0000\n","Training loss (for one batch) at step 1106: 14734497792.0000\n","Training loss (for one batch) at step 1107: 429994311680.0000\n","Training loss (for one batch) at step 1108: 22302939136.0000\n","Training loss (for one batch) at step 1109: 11635930112.0000\n","Training loss (for one batch) at step 1110: 13922123776.0000\n","Training loss (for one batch) at step 1111: 141077479424.0000\n","Training loss (for one batch) at step 1112: 26250272768.0000\n","Training loss (for one batch) at step 1113: 19812130816.0000\n","Training loss (for one batch) at step 1114: 131231784960.0000\n","Training loss (for one batch) at step 1115: 179665174528.0000\n","Training loss (for one batch) at step 1116: 2349438861312.0000\n","Training loss (for one batch) at step 1117: 11491289088.0000\n","Training loss (for one batch) at step 1118: 14038493184.0000\n","Training loss (for one batch) at step 1119: 5283614720.0000\n","Training loss (for one batch) at step 1120: 2196482293760.0000\n","Training loss (for one batch) at step 1121: 12228952064.0000\n","Training loss (for one batch) at step 1122: 3429470208.0000\n","Training loss (for one batch) at step 1123: 81671266304.0000\n","Training loss (for one batch) at step 1124: 13161919488.0000\n","Training loss (for one batch) at step 1125: 4752266752.0000\n","Training loss (for one batch) at step 1126: 8653123584.0000\n","Training loss (for one batch) at step 1127: 214894788608.0000\n","Training loss (for one batch) at step 1128: 3371525120.0000\n","Training loss (for one batch) at step 1129: 160803667968.0000\n","Training loss (for one batch) at step 1130: 95383863296.0000\n","Training loss (for one batch) at step 1131: 5722346496.0000\n","Training loss (for one batch) at step 1132: 146950619136.0000\n","Training loss (for one batch) at step 1133: 4905135616.0000\n","Training loss (for one batch) at step 1134: 820143915008.0000\n","Training loss (for one batch) at step 1135: 85964890112.0000\n","Training loss (for one batch) at step 1136: 1671616512.0000\n","Training loss (for one batch) at step 1137: 552300380160.0000\n","Training loss (for one batch) at step 1138: 96997548032.0000\n","Training loss (for one batch) at step 1139: 1523901399040.0000\n","Training loss (for one batch) at step 1140: 148030521344.0000\n","Training loss (for one batch) at step 1141: 22711894016.0000\n","Training loss (for one batch) at step 1142: 342104145920.0000\n","Training loss (for one batch) at step 1143: 9851871232.0000\n","Training loss (for one batch) at step 1144: 11886927872.0000\n","Training loss (for one batch) at step 1145: 298917003264.0000\n","Training loss (for one batch) at step 1146: 2857254400.0000\n","Training loss (for one batch) at step 1147: 20190226432.0000\n","Training loss (for one batch) at step 1148: 37101670400.0000\n","Training loss (for one batch) at step 1149: 800673280.0000\n","Training loss (for one batch) at step 1150: 28889718784.0000\n","Training loss (for one batch) at step 1151: 73213018112.0000\n","Training loss (for one batch) at step 1152: 115220824064.0000\n","Training loss (for one batch) at step 1153: 208747429888.0000\n","Training loss (for one batch) at step 1154: 61957939200.0000\n","Training loss (for one batch) at step 1155: 9359845376.0000\n","Training loss (for one batch) at step 1156: 11827281920.0000\n","Training loss (for one batch) at step 1157: 31201863680.0000\n","Training loss (for one batch) at step 1158: 5076846080.0000\n","Training loss (for one batch) at step 1159: 45291659264.0000\n","Training loss (for one batch) at step 1160: 263874379776.0000\n","Training loss (for one batch) at step 1161: 11152524288.0000\n","Training loss (for one batch) at step 1162: 67016896512.0000\n","Training loss (for one batch) at step 1163: 33337260032.0000\n","Training loss (for one batch) at step 1164: 27217317888.0000\n","Training loss (for one batch) at step 1165: 533901705216.0000\n","Training loss (for one batch) at step 1166: 1432521146368.0000\n","Training loss (for one batch) at step 1167: 26108192768.0000\n","Training loss (for one batch) at step 1168: 7202491392.0000\n","Training loss (for one batch) at step 1169: 35881922560.0000\n","Training loss (for one batch) at step 1170: 3101340672.0000\n","Training loss (for one batch) at step 1171: 66154917888.0000\n","Training loss (for one batch) at step 1172: 409533376.0000\n","Training loss (for one batch) at step 1173: 40320036864.0000\n","Training loss (for one batch) at step 1174: 53953130496.0000\n","Training loss (for one batch) at step 1175: 118152724480.0000\n","Training loss (for one batch) at step 1176: 1201622745088.0000\n","Training loss (for one batch) at step 1177: 162669680.0000\n","Training loss (for one batch) at step 1178: 6943688192.0000\n","Training loss (for one batch) at step 1179: 513787559936.0000\n","Training loss (for one batch) at step 1180: 57244516352.0000\n","Training loss (for one batch) at step 1181: 33426032640.0000\n","Training loss (for one batch) at step 1182: 25375191040.0000\n","Training loss (for one batch) at step 1183: 3694026752.0000\n","Training loss (for one batch) at step 1184: 247504224256.0000\n","Training loss (for one batch) at step 1185: 72601501696.0000\n","Training loss (for one batch) at step 1186: 271885811712.0000\n","Training loss (for one batch) at step 1187: 24052391936.0000\n","Training loss (for one batch) at step 1188: 96789200896.0000\n","Training loss (for one batch) at step 1189: 23867932672.0000\n","Training loss (for one batch) at step 1190: 358076317696.0000\n","Training loss (for one batch) at step 1191: 9770608640.0000\n","Training loss (for one batch) at step 1192: 106746249216.0000\n","Training loss (for one batch) at step 1193: 19133532160.0000\n","Training loss (for one batch) at step 1194: 6379544064.0000\n","Training loss (for one batch) at step 1195: 5449419587584.0000\n","Training loss (for one batch) at step 1196: 4347673600.0000\n","Training loss (for one batch) at step 1197: 77335650304.0000\n","Training loss (for one batch) at step 1198: 3249612544.0000\n","Training loss (for one batch) at step 1199: 230450282496.0000\n","Training loss (for one batch) at step 1200: 3720007680.0000\n","Training loss (for one batch) at step 1201: 22871599104.0000\n","Training loss (for one batch) at step 1202: 115479961600.0000\n","Training loss (for one batch) at step 1203: 492853854208.0000\n","Training loss (for one batch) at step 1204: 75734941696.0000\n","Training loss (for one batch) at step 1205: 6803212288.0000\n","Training loss (for one batch) at step 1206: 104475451392.0000\n","Training loss (for one batch) at step 1207: 10534646784.0000\n","Training loss (for one batch) at step 1208: 1152213504.0000\n","Training loss (for one batch) at step 1209: 154315292672.0000\n","Training loss (for one batch) at step 1210: 2513635584.0000\n","Training loss (for one batch) at step 1211: 67349696512.0000\n","Training loss (for one batch) at step 1212: 18380115968.0000\n","Training loss (for one batch) at step 1213: 246322757632.0000\n","Training loss (for one batch) at step 1214: 90093690880.0000\n","Training loss (for one batch) at step 1215: 123370881024.0000\n","Training loss (for one batch) at step 1216: 273469243392.0000\n","Training loss (for one batch) at step 1217: 19163918336.0000\n","Training loss (for one batch) at step 1218: 146592497664.0000\n","Training loss (for one batch) at step 1219: 19536070656.0000\n","Training loss (for one batch) at step 1220: 358864027648.0000\n","Training loss (for one batch) at step 1221: 7567273984.0000\n","Training loss (for one batch) at step 1222: 2940003584.0000\n","Training loss (for one batch) at step 1223: 302396243968.0000\n","Training loss (for one batch) at step 1224: 14306988032.0000\n","Training loss (for one batch) at step 1225: 6636578304.0000\n","Training loss (for one batch) at step 1226: 44084801536.0000\n","Training loss (for one batch) at step 1227: 6825368576.0000\n","Training loss (for one batch) at step 1228: 143217377280.0000\n","Training loss (for one batch) at step 1229: 14000203776.0000\n","Training loss (for one batch) at step 1230: 42238418944.0000\n","Training loss (for one batch) at step 1231: 1948548608.0000\n","Training loss (for one batch) at step 1232: 19151015936.0000\n","Training loss (for one batch) at step 1233: 13795745792.0000\n","Training loss (for one batch) at step 1234: 36057321472.0000\n","Training loss (for one batch) at step 1235: 20590960640.0000\n","Training loss (for one batch) at step 1236: 140946014208.0000\n","Training loss (for one batch) at step 1237: 7543417344.0000\n","Training loss (for one batch) at step 1238: 7211661312.0000\n","Training loss (for one batch) at step 1239: 9894980608.0000\n","Training loss (for one batch) at step 1240: 24228431872.0000\n","Training loss (for one batch) at step 1241: 134837149696.0000\n","Training loss (for one batch) at step 1242: 5527995392.0000\n","Training loss (for one batch) at step 1243: 96305774592.0000\n","Training loss (for one batch) at step 1244: 17520846848.0000\n","Training loss (for one batch) at step 1245: 46608355328.0000\n","Training loss (for one batch) at step 1246: 165715722240.0000\n","Training loss (for one batch) at step 1247: 68646236160.0000\n","Training loss (for one batch) at step 1248: 9063495680.0000\n","Training loss (for one batch) at step 1249: 32998928384.0000\n","Training loss (for one batch) at step 1250: 39798689792.0000\n","Training loss (for one batch) at step 1251: 3501440768.0000\n","Training loss (for one batch) at step 1252: 138319036416.0000\n","Training loss (for one batch) at step 1253: 8784780288.0000\n","Training loss (for one batch) at step 1254: 27367309312.0000\n","Training loss (for one batch) at step 1255: 4882037760.0000\n","Training loss (for one batch) at step 1256: 5637239296.0000\n","Training loss (for one batch) at step 1257: 86028025856.0000\n","Training loss (for one batch) at step 1258: 26382346240.0000\n","Training loss (for one batch) at step 1259: 1421240192.0000\n","Training loss (for one batch) at step 1260: 49891110912.0000\n","Training loss (for one batch) at step 1261: 30758187008.0000\n","Training loss (for one batch) at step 1262: 14833191936.0000\n","Training loss (for one batch) at step 1263: 60017700864.0000\n","Training loss (for one batch) at step 1264: 265150365696.0000\n","Training loss (for one batch) at step 1265: 155924119552.0000\n","Training loss (for one batch) at step 1266: 298370465792.0000\n","Training loss (for one batch) at step 1267: 17356029952.0000\n","Training loss (for one batch) at step 1268: 14213901312.0000\n","Training loss (for one batch) at step 1269: 7505082880.0000\n","Training loss (for one batch) at step 1270: 124761653248.0000\n","Training loss (for one batch) at step 1271: 185082576896.0000\n","Training loss (for one batch) at step 1272: 8081778688.0000\n","Training loss (for one batch) at step 1273: 7428813312.0000\n","Training loss (for one batch) at step 1274: 66349596672.0000\n","Training loss (for one batch) at step 1275: 69611520000.0000\n","Training loss (for one batch) at step 1276: 5610565632.0000\n","Training loss (for one batch) at step 1277: 3658196992.0000\n","Training loss (for one batch) at step 1278: 6436948992.0000\n","Training loss (for one batch) at step 1279: 12656397312.0000\n","Training loss (for one batch) at step 1280: 19836815360.0000\n","Training loss (for one batch) at step 1281: 11145286656.0000\n","Training loss (for one batch) at step 1282: 27984437248.0000\n","Training loss (for one batch) at step 1283: 4642193408.0000\n","Training loss (for one batch) at step 1284: 119001980928.0000\n","Training loss (for one batch) at step 1285: 194086027264.0000\n","Training loss (for one batch) at step 1286: 97358962688.0000\n","Training loss (for one batch) at step 1287: 213836152832.0000\n","Training loss (for one batch) at step 1288: 10267923456.0000\n","Training loss (for one batch) at step 1289: 158755307520.0000\n","Training loss (for one batch) at step 1290: 2204141879296.0000\n","Training loss (for one batch) at step 1291: 9378367488.0000\n","Training loss (for one batch) at step 1292: 128217137152.0000\n","Training loss (for one batch) at step 1293: 27621728256.0000\n","Training loss (for one batch) at step 1294: 5666902528.0000\n","Training loss (for one batch) at step 1295: 354235088896.0000\n","Training loss (for one batch) at step 1296: 8326488064.0000\n","Training loss (for one batch) at step 1297: 11142089728.0000\n","Training loss (for one batch) at step 1298: 1793517056.0000\n","Training loss (for one batch) at step 1299: 71068540928.0000\n","Training loss (for one batch) at step 1300: 231193673728.0000\n","Training loss (for one batch) at step 1301: 23267909632.0000\n","Training loss (for one batch) at step 1302: 53600182272.0000\n","Training loss (for one batch) at step 1303: 1377125531648.0000\n","Training loss (for one batch) at step 1304: 554117632.0000\n","Training loss (for one batch) at step 1305: 110749392896.0000\n","Training loss (for one batch) at step 1306: 13500234752.0000\n","Training loss (for one batch) at step 1307: 2272529664.0000\n","Training loss (for one batch) at step 1308: 9120608256.0000\n","Training loss (for one batch) at step 1309: 15481548800.0000\n","Training loss (for one batch) at step 1310: 16559998976.0000\n","Training loss (for one batch) at step 1311: 23257118720.0000\n","Training loss (for one batch) at step 1312: 19920164864.0000\n","Training loss (for one batch) at step 1313: 23589683200.0000\n","Training loss (for one batch) at step 1314: 65801252864.0000\n","Training loss (for one batch) at step 1315: 4296770560.0000\n","Training loss (for one batch) at step 1316: 27883057152.0000\n","Training loss (for one batch) at step 1317: 36470075392.0000\n","Training loss (for one batch) at step 1318: 660320026624.0000\n","Training loss (for one batch) at step 1319: 138521165824.0000\n","Training loss (for one batch) at step 1320: 3166088200192.0000\n","Training loss (for one batch) at step 1321: 6728835072.0000\n","Training loss (for one batch) at step 1322: 51135959040.0000\n","Training loss (for one batch) at step 1323: 29715656704.0000\n","Training loss (for one batch) at step 1324: 33626638336.0000\n","Training loss (for one batch) at step 1325: 4015473664.0000\n","Training loss (for one batch) at step 1326: 3555613952.0000\n","Training loss (for one batch) at step 1327: 31651950592.0000\n","Training loss (for one batch) at step 1328: 358614269952.0000\n","Training loss (for one batch) at step 1329: 42991751168.0000\n","Training loss (for one batch) at step 1330: 70845431808.0000\n","Training loss (for one batch) at step 1331: 17968148480.0000\n","Training loss (for one batch) at step 1332: 21557045248.0000\n","Training loss (for one batch) at step 1333: 2586404864.0000\n","Training loss (for one batch) at step 1334: 9625927680.0000\n","Training loss (for one batch) at step 1335: 1870706944.0000\n","Training loss (for one batch) at step 1336: 3302643712.0000\n","Training loss (for one batch) at step 1337: 18348109824.0000\n","Training loss (for one batch) at step 1338: 21452193792.0000\n","Training loss (for one batch) at step 1339: 13726806016.0000\n","Training loss (for one batch) at step 1340: 26643488768.0000\n","Training loss (for one batch) at step 1341: 57704808448.0000\n","Training loss (for one batch) at step 1342: 26314217472.0000\n","Training loss (for one batch) at step 1343: 45971857408.0000\n","Training loss (for one batch) at step 1344: 403973636096.0000\n","Training loss (for one batch) at step 1345: 6486611456.0000\n","Training loss (for one batch) at step 1346: 3481877760.0000\n","Training loss (for one batch) at step 1347: 150383820800.0000\n","Training loss (for one batch) at step 1348: 107891474432.0000\n","Training loss (for one batch) at step 1349: 74920017920.0000\n","Training loss (for one batch) at step 1350: 11939426304.0000\n","Training loss (for one batch) at step 1351: 86129950720.0000\n","Training loss (for one batch) at step 1352: 16019760128.0000\n","Training loss (for one batch) at step 1353: 5006006784.0000\n","Training loss (for one batch) at step 1354: 3226432000.0000\n","Training loss (for one batch) at step 1355: 8836737024.0000\n","Training loss (for one batch) at step 1356: 80386932736.0000\n","Training loss (for one batch) at step 1357: 6450794496.0000\n","Training loss (for one batch) at step 1358: 19008212992.0000\n","Training loss (for one batch) at step 1359: 6143257600.0000\n","Training loss (for one batch) at step 1360: 107149713408.0000\n","Training loss (for one batch) at step 1361: 5299388416.0000\n","Training loss (for one batch) at step 1362: 44961017856.0000\n","Training loss (for one batch) at step 1363: 118486351872.0000\n","Training loss (for one batch) at step 1364: 8232066560.0000\n","Training loss (for one batch) at step 1365: 36512927744.0000\n","Training loss (for one batch) at step 1366: 3872024832.0000\n","Training loss (for one batch) at step 1367: 24850753536.0000\n","Training loss (for one batch) at step 1368: 5955542016.0000\n","Training loss (for one batch) at step 1369: 18729508864.0000\n","Training loss (for one batch) at step 1370: 2507680768.0000\n","Training loss (for one batch) at step 1371: 6033966080.0000\n","Training loss (for one batch) at step 1372: 98995003392.0000\n","Training loss (for one batch) at step 1373: 170807164928.0000\n","Training loss (for one batch) at step 1374: 33227110400.0000\n","Training loss (for one batch) at step 1375: 158333075456.0000\n","Training loss (for one batch) at step 1376: 767957991424.0000\n","Training loss (for one batch) at step 1377: 34326286336.0000\n","Training loss (for one batch) at step 1378: 112303489024.0000\n","Training loss (for one batch) at step 1379: 3920851456.0000\n","Training loss (for one batch) at step 1380: 2267364608.0000\n","Training loss (for one batch) at step 1381: 62294851584.0000\n","Training loss (for one batch) at step 1382: 25870919680.0000\n","Training loss (for one batch) at step 1383: 1009636802560.0000\n","Training loss (for one batch) at step 1384: 138655481856.0000\n","Training loss (for one batch) at step 1385: 27390638080.0000\n","Training loss (for one batch) at step 1386: 267246092288.0000\n","Training loss (for one batch) at step 1387: 95156248576.0000\n","Training loss (for one batch) at step 1388: 3394219540480.0000\n","Training loss (for one batch) at step 1389: 38948769792.0000\n","Training loss (for one batch) at step 1390: 14481510400.0000\n","Training loss (for one batch) at step 1391: 41682415616.0000\n","Training loss (for one batch) at step 1392: 76225896448.0000\n","Training loss (for one batch) at step 1393: 468226637824.0000\n","Training loss (for one batch) at step 1394: 4905343909888.0000\n","Training loss (for one batch) at step 1395: 22730706944.0000\n","Training loss (for one batch) at step 1396: 14417893376.0000\n","Training loss (for one batch) at step 1397: 34197168128.0000\n","Training loss (for one batch) at step 1398: 388403036160.0000\n","Training loss (for one batch) at step 1399: 7002468352.0000\n","Training loss (for one batch) at step 1400: 146348933120.0000\n","Training loss (for one batch) at step 1401: 3605428224.0000\n","Training loss (for one batch) at step 1402: 388687101952.0000\n","Training loss (for one batch) at step 1403: 121729613824.0000\n","Training loss (for one batch) at step 1404: 286867587072.0000\n","Training loss (for one batch) at step 1405: 138478174208.0000\n","Training loss (for one batch) at step 1406: 63254450176.0000\n","Training loss (for one batch) at step 1407: 110086397952.0000\n","Training loss (for one batch) at step 1408: 22817101824.0000\n","Training loss (for one batch) at step 1409: 279764664320.0000\n","Training loss (for one batch) at step 1410: 1981624320.0000\n","Training loss (for one batch) at step 1411: 16386877440.0000\n","Training loss (for one batch) at step 1412: 48178208768.0000\n","Training loss (for one batch) at step 1413: 136652488704.0000\n","Training loss (for one batch) at step 1414: 215443488768.0000\n","Training loss (for one batch) at step 1415: 347234402304.0000\n","Training loss (for one batch) at step 1416: 6817962496.0000\n","Training loss (for one batch) at step 1417: 3734017536.0000\n","Training loss (for one batch) at step 1418: 14030716928.0000\n","Training loss (for one batch) at step 1419: 10889520128.0000\n","Training loss (for one batch) at step 1420: 298675208192.0000\n","Training loss (for one batch) at step 1421: 33505677312.0000\n","Training loss (for one batch) at step 1422: 42790187008.0000\n","Training loss (for one batch) at step 1423: 331107434496.0000\n","Training loss (for one batch) at step 1424: 2214132480.0000\n","Training loss (for one batch) at step 1425: 1595348352.0000\n","Training loss (for one batch) at step 1426: 593191698432.0000\n","Training loss (for one batch) at step 1427: 9458560000.0000\n","Training loss (for one batch) at step 1428: 76040208384.0000\n","Training loss (for one batch) at step 1429: 285356982272.0000\n","Training loss (for one batch) at step 1430: 70611369984.0000\n","Training loss (for one batch) at step 1431: 3696449792.0000\n","Training loss (for one batch) at step 1432: 8185176064.0000\n","Training loss (for one batch) at step 1433: 18250768384.0000\n","Training loss (for one batch) at step 1434: 12520878080.0000\n","Training loss (for one batch) at step 1435: 5871872000.0000\n","Training loss (for one batch) at step 1436: 35285262336.0000\n","Training loss (for one batch) at step 1437: 11690273792.0000\n","Training loss (for one batch) at step 1438: 8567164416.0000\n","Training loss (for one batch) at step 1439: 137293045760.0000\n","Training loss (for one batch) at step 1440: 93809573888.0000\n","Training loss (for one batch) at step 1441: 6672881664.0000\n","Training loss (for one batch) at step 1442: 161582399488.0000\n","Training loss (for one batch) at step 1443: 265393799168.0000\n","Training loss (for one batch) at step 1444: 2812598272.0000\n","Training loss (for one batch) at step 1445: 460460228608.0000\n","Training loss (for one batch) at step 1446: 58116104192.0000\n","Training loss (for one batch) at step 1447: 64212275200.0000\n","Training loss (for one batch) at step 1448: 3380405248.0000\n","Training loss (for one batch) at step 1449: 15094410240.0000\n","Training loss (for one batch) at step 1450: 5888114688.0000\n","Training loss (for one batch) at step 1451: 4982507520.0000\n","Training loss (for one batch) at step 1452: 57714462720.0000\n","Training loss (for one batch) at step 1453: 75256265965568.0000\n","Training loss (for one batch) at step 1454: 70601244672.0000\n","Training loss (for one batch) at step 1455: 142917386240.0000\n","Training loss (for one batch) at step 1456: 336153411584.0000\n","Training loss (for one batch) at step 1457: 13015342080.0000\n","Training loss (for one batch) at step 1458: 10038517760.0000\n","Training loss (for one batch) at step 1459: 1404365952.0000\n","Training loss (for one batch) at step 1460: 2705517056.0000\n","Training loss (for one batch) at step 1461: 6569315328.0000\n","Training loss (for one batch) at step 1462: 10133824512.0000\n","Training loss (for one batch) at step 1463: 7780009472.0000\n","Training loss (for one batch) at step 1464: 373025144832.0000\n","Training loss (for one batch) at step 1465: 92973522944.0000\n","Training loss (for one batch) at step 1466: 12080793600.0000\n","Training loss (for one batch) at step 1467: 59381346304.0000\n","Training loss (for one batch) at step 1468: 14881089536.0000\n","Training loss (for one batch) at step 1469: 3233001472.0000\n","Training loss (for one batch) at step 1470: 6849851904.0000\n","Training loss (for one batch) at step 1471: 13613352960.0000\n","Training loss (for one batch) at step 1472: 8496390656.0000\n","Training loss (for one batch) at step 1473: 3503199744.0000\n","Training loss (for one batch) at step 1474: 591359770624.0000\n","Training loss (for one batch) at step 1475: 17219217408.0000\n","Training loss (for one batch) at step 1476: 1817035136.0000\n","Training loss (for one batch) at step 1477: 17518657536.0000\n","Training loss (for one batch) at step 1478: 12183500800.0000\n","Training loss (for one batch) at step 1479: 11051448320.0000\n","Training loss (for one batch) at step 1480: 232921546752.0000\n","Training loss (for one batch) at step 1481: 167793999872.0000\n","Training loss (for one batch) at step 1482: 86759686144.0000\n","Training loss (for one batch) at step 1483: 13101802979328.0000\n","Training loss (for one batch) at step 1484: 12018511872.0000\n","Training loss (for one batch) at step 1485: 15857277952.0000\n","Training loss (for one batch) at step 1486: 5632815104.0000\n","Training loss (for one batch) at step 1487: 95204261888.0000\n","Training loss (for one batch) at step 1488: 13342807040.0000\n","Training loss (for one batch) at step 1489: 42921586688.0000\n","Training loss (for one batch) at step 1490: 4839844864.0000\n","Training loss (for one batch) at step 1491: 9865485312.0000\n","Training loss (for one batch) at step 1492: 36444712960.0000\n","Training loss (for one batch) at step 1493: 28207005696.0000\n","Training loss (for one batch) at step 1494: 328703279104.0000\n","Training loss (for one batch) at step 1495: 48512278528.0000\n","Training loss (for one batch) at step 1496: 1093871616.0000\n","Training loss (for one batch) at step 1497: 17874317312.0000\n","Training loss (for one batch) at step 1498: 122123616256.0000\n","Training loss (for one batch) at step 1499: 219258355712.0000\n","Training loss (for one batch) at step 1500: 81089028096.0000\n","Training loss (for one batch) at step 1501: 34951680000.0000\n","Training loss (for one batch) at step 1502: 18396860416.0000\n","Training loss (for one batch) at step 1503: 3949523712.0000\n","Training loss (for one batch) at step 1504: 2048274176.0000\n","Training loss (for one batch) at step 1505: 638462656512.0000\n","Training loss (for one batch) at step 1506: 151124525056.0000\n","Training loss (for one batch) at step 1507: 3643937792.0000\n","Training loss (for one batch) at step 1508: 4868926464.0000\n","Training loss (for one batch) at step 1509: 33248806912.0000\n","Training loss (for one batch) at step 1510: 5033725952.0000\n","Training loss (for one batch) at step 1511: 55894654976.0000\n","Training loss (for one batch) at step 1512: 17587720192.0000\n","Training loss (for one batch) at step 1513: 40637308928.0000\n","Training loss (for one batch) at step 1514: 15786878976.0000\n","Training loss (for one batch) at step 1515: 32634304512.0000\n","Training loss (for one batch) at step 1516: 90589888512.0000\n","Training loss (for one batch) at step 1517: 27576614912.0000\n","Training loss (for one batch) at step 1518: 37766995968.0000\n","Training loss (for one batch) at step 1519: 290866102272.0000\n","Training loss (for one batch) at step 1520: 3767461632.0000\n","Training loss (for one batch) at step 1521: 46879043584.0000\n","Training loss (for one batch) at step 1522: 1654744832.0000\n","Training loss (for one batch) at step 1523: 935204093952.0000\n","Training loss (for one batch) at step 1524: 8316477440.0000\n","Training loss (for one batch) at step 1525: 124555051008.0000\n","Training loss (for one batch) at step 1526: 7200351232.0000\n","Training loss (for one batch) at step 1527: 18722713600.0000\n","Training loss (for one batch) at step 1528: 152139972608.0000\n","Training loss (for one batch) at step 1529: 25850163200.0000\n","Training loss (for one batch) at step 1530: 95604293632.0000\n","Training loss (for one batch) at step 1531: 18166190080.0000\n","Training loss (for one batch) at step 1532: 8077993984.0000\n","Training loss (for one batch) at step 1533: 5415408640.0000\n","Training loss (for one batch) at step 1534: 102193250304.0000\n","Training loss (for one batch) at step 1535: 1364823900160.0000\n","Training loss (for one batch) at step 1536: 1498205696.0000\n","Training loss (for one batch) at step 1537: 21349457920.0000\n","Training loss (for one batch) at step 1538: 6416564736.0000\n","Training loss (for one batch) at step 1539: 28923267072.0000\n","Training loss (for one batch) at step 1540: 40884150272.0000\n","Training loss (for one batch) at step 1541: 56263553024.0000\n","Training loss (for one batch) at step 1542: 15404498944.0000\n","Training loss (for one batch) at step 1543: 37534969856.0000\n","Training loss (for one batch) at step 1544: 2244136960.0000\n","Training loss (for one batch) at step 1545: 3297223680.0000\n","Training loss (for one batch) at step 1546: 3674470144.0000\n","Training loss (for one batch) at step 1547: 98299846656.0000\n","Training loss (for one batch) at step 1548: 19294853120.0000\n","Training loss (for one batch) at step 1549: 19941955584.0000\n","Training loss (for one batch) at step 1550: 87654998016.0000\n","Training loss (for one batch) at step 1551: 256615104512.0000\n","Training loss (for one batch) at step 1552: 30238230528.0000\n","Training loss (for one batch) at step 1553: 122278699008.0000\n","Training loss (for one batch) at step 1554: 204448219136.0000\n","Training loss (for one batch) at step 1555: 25021667328.0000\n","Training loss (for one batch) at step 1556: 17461260288.0000\n","Training loss (for one batch) at step 1557: 12216940544.0000\n","Training loss (for one batch) at step 1558: 4582302720.0000\n","Training loss (for one batch) at step 1559: 34098851840.0000\n","Training loss (for one batch) at step 1560: 9612781568.0000\n","Training loss (for one batch) at step 1561: 32181940224.0000\n","Training loss (for one batch) at step 1562: 305345495040.0000\n","Training loss (for one batch) at step 1563: 392422490112.0000\n","Training loss (for one batch) at step 1564: 41049444352.0000\n","Training loss (for one batch) at step 1565: 16761564160.0000\n","Training loss (for one batch) at step 1566: 28358848512.0000\n","Training loss (for one batch) at step 1567: 19059083264.0000\n","Training loss (for one batch) at step 1568: 460563218432.0000\n","Training loss (for one batch) at step 1569: 16751711232.0000\n","Training loss (for one batch) at step 1570: 824588435456.0000\n","Training loss (for one batch) at step 1571: 2192453795840.0000\n","Training loss (for one batch) at step 1572: 13385067520.0000\n","Training loss (for one batch) at step 1573: 141716799488.0000\n","Training loss (for one batch) at step 1574: 16066482176.0000\n","Training loss (for one batch) at step 1575: 420094214144.0000\n","Training loss (for one batch) at step 1576: 11130791936.0000\n","Training loss (for one batch) at step 1577: 12763863040.0000\n","Training loss (for one batch) at step 1578: 6624897024.0000\n","Training loss (for one batch) at step 1579: 275194773504.0000\n","Training loss (for one batch) at step 1580: 14625057792.0000\n","Training loss (for one batch) at step 1581: 47416995840.0000\n","Training loss (for one batch) at step 1582: 2800612613816320.0000\n","Training loss (for one batch) at step 1583: 8058493952.0000\n","Training loss (for one batch) at step 1584: 17557460992.0000\n","Training loss (for one batch) at step 1585: 46865465344.0000\n","Training loss (for one batch) at step 1586: 3923788288.0000\n","Training loss (for one batch) at step 1587: 7574989824.0000\n","Training loss (for one batch) at step 1588: 121760759808.0000\n","Training loss (for one batch) at step 1589: 7079107584.0000\n","Training loss (for one batch) at step 1590: 22715789312.0000\n","Training loss (for one batch) at step 1591: 97131184128.0000\n","Training loss (for one batch) at step 1592: 15393865728.0000\n","Training loss (for one batch) at step 1593: 4844491776.0000\n","Training loss (for one batch) at step 1594: 15484168192.0000\n","Training loss (for one batch) at step 1595: 37499539456.0000\n","Training loss (for one batch) at step 1596: 23870717952.0000\n","Training loss (for one batch) at step 1597: 1527039104.0000\n","Training loss (for one batch) at step 1598: 360567472128.0000\n","Training loss (for one batch) at step 1599: 3393412096.0000\n","Training loss (for one batch) at step 1600: 158447878144.0000\n","Training loss (for one batch) at step 1601: 149643132928.0000\n","Training loss (for one batch) at step 1602: 2189768704.0000\n","Training loss (for one batch) at step 1603: 7167437312.0000\n","Training loss (for one batch) at step 1604: 2954879744.0000\n","Training loss (for one batch) at step 1605: 16728242176.0000\n","Training loss (for one batch) at step 1606: 38507847680.0000\n","Training loss (for one batch) at step 1607: 27297210368.0000\n","Training loss (for one batch) at step 1608: 185407012864.0000\n","Training loss (for one batch) at step 1609: 11622198272.0000\n","Training loss (for one batch) at step 1610: 209928224768.0000\n","Training loss (for one batch) at step 1611: 224052674560.0000\n","Training loss (for one batch) at step 1612: 170800807936.0000\n","Training loss (for one batch) at step 1613: 5096548864.0000\n","Training loss (for one batch) at step 1614: 59949203456.0000\n","Training loss (for one batch) at step 1615: 3663340288.0000\n","Training loss (for one batch) at step 1616: 127237128192.0000\n","Training loss (for one batch) at step 1617: 2700665683968.0000\n","Training loss (for one batch) at step 1618: 8657192960.0000\n","Training loss (for one batch) at step 1619: 28585801728.0000\n","Training loss (for one batch) at step 1620: 9811612672.0000\n","Training loss (for one batch) at step 1621: 8480838144.0000\n","Training loss (for one batch) at step 1622: 3070830336.0000\n","Training loss (for one batch) at step 1623: 1973196288.0000\n","Training loss (for one batch) at step 1624: 2932477952.0000\n","Training loss (for one batch) at step 1625: 21615585280.0000\n","Training loss (for one batch) at step 1626: 5503618048.0000\n","Training loss (for one batch) at step 1627: 44144631808.0000\n","Training loss (for one batch) at step 1628: 80054190080.0000\n","Training loss (for one batch) at step 1629: 24893863936.0000\n","Training loss (for one batch) at step 1630: 3546757120.0000\n","Training loss (for one batch) at step 1631: 88373002240.0000\n","Training loss (for one batch) at step 1632: 138698899456.0000\n","Training loss (for one batch) at step 1633: 274084560896.0000\n","Training loss (for one batch) at step 1634: 6178848768.0000\n","Training loss (for one batch) at step 1635: 17561718784.0000\n","Training loss (for one batch) at step 1636: 10599763968.0000\n","Training loss (for one batch) at step 1637: 6824590336.0000\n","Training loss (for one batch) at step 1638: 72663941120.0000\n","Training loss (for one batch) at step 1639: 3898262784.0000\n","Training loss (for one batch) at step 1640: 46423572480.0000\n","Training loss (for one batch) at step 1641: 73720594432.0000\n","Training loss (for one batch) at step 1642: 199552647168.0000\n","Training loss (for one batch) at step 1643: 385425604608.0000\n","Training loss (for one batch) at step 1644: 234593812480.0000\n","Training loss (for one batch) at step 1645: 20431845376.0000\n","Training loss (for one batch) at step 1646: 148139524096.0000\n","Training loss (for one batch) at step 1647: 131440041984.0000\n","Training loss (for one batch) at step 1648: 5269419982848.0000\n","Training loss (for one batch) at step 1649: 24105347072.0000\n","Training loss (for one batch) at step 1650: 22295011328.0000\n","Training loss (for one batch) at step 1651: 5155158528.0000\n","Training loss (for one batch) at step 1652: 17804234752.0000\n","Training loss (for one batch) at step 1653: 4818441216.0000\n","Training loss (for one batch) at step 1654: 71894614016.0000\n","Training loss (for one batch) at step 1655: 2335670016.0000\n","Training loss (for one batch) at step 1656: 13615110144.0000\n","Training loss (for one batch) at step 1657: 26004031488.0000\n","Training loss (for one batch) at step 1658: 41631539200.0000\n","Training loss (for one batch) at step 1659: 96988004352.0000\n","Training loss (for one batch) at step 1660: 205525090304.0000\n","Training loss (for one batch) at step 1661: 3032089600.0000\n","Training loss (for one batch) at step 1662: 44794146816.0000\n","Training loss (for one batch) at step 1663: 8230966272.0000\n","Training loss (for one batch) at step 1664: 30661783552.0000\n","Training loss (for one batch) at step 1665: 79338553344.0000\n","Training loss (for one batch) at step 1666: 720558161920.0000\n","Training loss (for one batch) at step 1667: 24279764992.0000\n","Training loss (for one batch) at step 1668: 615039959040.0000\n","Training loss (for one batch) at step 1669: 7245096960.0000\n","Training loss (for one batch) at step 1670: 10853249024.0000\n","Training loss (for one batch) at step 1671: 46016634880.0000\n","Training loss (for one batch) at step 1672: 51638382592.0000\n","Training loss (for one batch) at step 1673: 22606606336.0000\n","Training loss (for one batch) at step 1674: 26097526784.0000\n","Training loss (for one batch) at step 1675: 6479580160.0000\n","Training loss (for one batch) at step 1676: 2099427328.0000\n","Training loss (for one batch) at step 1677: 165546393600.0000\n","Training loss (for one batch) at step 1678: 60989558784.0000\n","Training loss (for one batch) at step 1679: 45130477568.0000\n","Training loss (for one batch) at step 1680: 442104938496.0000\n","Training loss (for one batch) at step 1681: 3396347648.0000\n","Training loss (for one batch) at step 1682: 4911578624.0000\n","Training loss (for one batch) at step 1683: 230422806528.0000\n","Training loss (for one batch) at step 1684: 45410115584.0000\n","Training loss (for one batch) at step 1685: 15881125888.0000\n","Training loss (for one batch) at step 1686: 505974882304.0000\n","Training loss (for one batch) at step 1687: 76093054976.0000\n","Training loss (for one batch) at step 1688: 5092200448.0000\n","Training loss (for one batch) at step 1689: 3480952320.0000\n","Training loss (for one batch) at step 1690: 1715254528.0000\n","Training loss (for one batch) at step 1691: 14068290560.0000\n","Training loss (for one batch) at step 1692: 222624088064.0000\n","Training loss (for one batch) at step 1693: 2012267413504.0000\n","\n","Start of epoch 49\n","Training loss (for one batch) at step 0: 38505881600.0000\n","Training loss (for one batch) at step 1: 32023353344.0000\n","Training loss (for one batch) at step 2: 48922267648.0000\n","Training loss (for one batch) at step 3: 19964786688.0000\n","Training loss (for one batch) at step 4: 5180249600.0000\n","Training loss (for one batch) at step 5: 358014386176.0000\n","Training loss (for one batch) at step 6: 18009589760.0000\n","Training loss (for one batch) at step 7: 275750289408.0000\n","Training loss (for one batch) at step 8: 5603190784.0000\n","Training loss (for one batch) at step 9: 8055739392.0000\n","Training loss (for one batch) at step 10: 54342529024.0000\n","Training loss (for one batch) at step 11: 49176469504.0000\n","Training loss (for one batch) at step 12: 5871376896.0000\n","Training loss (for one batch) at step 13: 5159551488.0000\n","Training loss (for one batch) at step 14: 35395149824.0000\n","Training loss (for one batch) at step 15: 19316682752.0000\n","Training loss (for one batch) at step 16: 30632347648.0000\n","Training loss (for one batch) at step 17: 5133725696.0000\n","Training loss (for one batch) at step 18: 197584945152.0000\n","Training loss (for one batch) at step 19: 171294294016.0000\n","Training loss (for one batch) at step 20: 18112708608.0000\n","Training loss (for one batch) at step 21: 3477987328.0000\n","Training loss (for one batch) at step 22: 294617841664.0000\n","Training loss (for one batch) at step 23: 20388532224.0000\n","Training loss (for one batch) at step 24: 85941223424.0000\n","Training loss (for one batch) at step 25: 4155348480.0000\n","Training loss (for one batch) at step 26: 988927885312.0000\n","Training loss (for one batch) at step 27: 98552029184.0000\n","Training loss (for one batch) at step 28: 134947020800.0000\n","Training loss (for one batch) at step 29: 16718824448.0000\n","Training loss (for one batch) at step 30: 128179568640.0000\n","Training loss (for one batch) at step 31: 125552033792.0000\n","Training loss (for one batch) at step 32: 451533537280.0000\n","Training loss (for one batch) at step 33: 3461523701760.0000\n","Training loss (for one batch) at step 34: 28837900288.0000\n","Training loss (for one batch) at step 35: 4628152320.0000\n","Training loss (for one batch) at step 36: 56697831424.0000\n","Training loss (for one batch) at step 37: 20649474048.0000\n","Training loss (for one batch) at step 38: 3201801216.0000\n","Training loss (for one batch) at step 39: 75980972032.0000\n","Training loss (for one batch) at step 40: 3460900608.0000\n","Training loss (for one batch) at step 41: 6030956544.0000\n","Training loss (for one batch) at step 42: 163378839552.0000\n","Training loss (for one batch) at step 43: 1360872960.0000\n","Training loss (for one batch) at step 44: 10667675648.0000\n","Training loss (for one batch) at step 45: 10175666176.0000\n","Training loss (for one batch) at step 46: 611366600704.0000\n","Training loss (for one batch) at step 47: 259441623040.0000\n","Training loss (for one batch) at step 48: 35490336768.0000\n","Training loss (for one batch) at step 49: 16628007936.0000\n","Training loss (for one batch) at step 50: 811558371328.0000\n","Training loss (for one batch) at step 51: 39466827776.0000\n","Training loss (for one batch) at step 52: 16849430528.0000\n","Training loss (for one batch) at step 53: 216469045248.0000\n","Training loss (for one batch) at step 54: 202765811712.0000\n","Training loss (for one batch) at step 55: 35527127040.0000\n","Training loss (for one batch) at step 56: 221350379520.0000\n","Training loss (for one batch) at step 57: 5218504704.0000\n","Training loss (for one batch) at step 58: 10774119424.0000\n","Training loss (for one batch) at step 59: 4215313152.0000\n","Training loss (for one batch) at step 60: 42478616576.0000\n","Training loss (for one batch) at step 61: 92873572352.0000\n","Training loss (for one batch) at step 62: 89125552128.0000\n","Training loss (for one batch) at step 63: 21493438464.0000\n","Training loss (for one batch) at step 64: 7632556544.0000\n","Training loss (for one batch) at step 65: 22193467392.0000\n","Training loss (for one batch) at step 66: 511032288.0000\n","Training loss (for one batch) at step 67: 359772028928.0000\n","Training loss (for one batch) at step 68: 202788667392.0000\n","Training loss (for one batch) at step 69: 195709485056.0000\n","Training loss (for one batch) at step 70: 822407856128.0000\n","Training loss (for one batch) at step 71: 955132215296.0000\n","Training loss (for one batch) at step 72: 6455595008.0000\n","Training loss (for one batch) at step 73: 24531658752.0000\n","Training loss (for one batch) at step 74: 22017560576.0000\n","Training loss (for one batch) at step 75: 70676021248.0000\n","Training loss (for one batch) at step 76: 126046527488.0000\n","Training loss (for one batch) at step 77: 5809104384.0000\n","Training loss (for one batch) at step 78: 11011721216.0000\n","Training loss (for one batch) at step 79: 39393411072.0000\n","Training loss (for one batch) at step 80: 728396608.0000\n","Training loss (for one batch) at step 81: 3636305408.0000\n","Training loss (for one batch) at step 82: 33834151936.0000\n","Training loss (for one batch) at step 83: 400214523904.0000\n","Training loss (for one batch) at step 84: 14278393856.0000\n","Training loss (for one batch) at step 85: 5966790144.0000\n","Training loss (for one batch) at step 86: 772232314880.0000\n","Training loss (for one batch) at step 87: 1619988992.0000\n","Training loss (for one batch) at step 88: 192153812992.0000\n","Training loss (for one batch) at step 89: 183130193920.0000\n","Training loss (for one batch) at step 90: 208204496896.0000\n","Training loss (for one batch) at step 91: 11183425536.0000\n","Training loss (for one batch) at step 92: 16752934912.0000\n","Training loss (for one batch) at step 93: 145033101312.0000\n","Training loss (for one batch) at step 94: 6546263040.0000\n","Training loss (for one batch) at step 95: 279627661312.0000\n","Training loss (for one batch) at step 96: 213419245568.0000\n","Training loss (for one batch) at step 97: 692724170752.0000\n","Training loss (for one batch) at step 98: 12211990528.0000\n","Training loss (for one batch) at step 99: 12281271296.0000\n","Training loss (for one batch) at step 100: 58890039296.0000\n","Training loss (for one batch) at step 101: 60898951168.0000\n","Training loss (for one batch) at step 102: 13415028736.0000\n","Training loss (for one batch) at step 103: 37517471744.0000\n","Training loss (for one batch) at step 104: 109036830720.0000\n","Training loss (for one batch) at step 105: 5203989504.0000\n","Training loss (for one batch) at step 106: 957391898148864.0000\n","Training loss (for one batch) at step 107: 9967201280.0000\n","Training loss (for one batch) at step 108: 26708709376.0000\n","Training loss (for one batch) at step 109: 13590781952.0000\n","Training loss (for one batch) at step 110: 20966684672.0000\n","Training loss (for one batch) at step 111: 87367622656.0000\n","Training loss (for one batch) at step 112: 192440991744.0000\n","Training loss (for one batch) at step 113: 45613101056.0000\n","Training loss (for one batch) at step 114: 9282383872.0000\n","Training loss (for one batch) at step 115: 158278385664.0000\n","Training loss (for one batch) at step 116: 153340051456.0000\n","Training loss (for one batch) at step 117: 17540212736.0000\n","Training loss (for one batch) at step 118: 8658183168.0000\n","Training loss (for one batch) at step 119: 11330230272.0000\n","Training loss (for one batch) at step 120: 12761106432.0000\n","Training loss (for one batch) at step 121: 130064891904.0000\n","Training loss (for one batch) at step 122: 200325627904.0000\n","Training loss (for one batch) at step 123: 57439559680.0000\n","Training loss (for one batch) at step 124: 18032046080.0000\n","Training loss (for one batch) at step 125: 18596900864.0000\n","Training loss (for one batch) at step 126: 271143206912.0000\n","Training loss (for one batch) at step 127: 28673638400.0000\n","Training loss (for one batch) at step 128: 155983642624.0000\n","Training loss (for one batch) at step 129: 5848253440.0000\n","Training loss (for one batch) at step 130: 10100275200.0000\n","Training loss (for one batch) at step 131: 5639737856.0000\n","Training loss (for one batch) at step 132: 92594855936.0000\n","Training loss (for one batch) at step 133: 10751191040.0000\n","Training loss (for one batch) at step 134: 33675364352.0000\n","Training loss (for one batch) at step 135: 590073495552.0000\n","Training loss (for one batch) at step 136: 54021963776.0000\n","Training loss (for one batch) at step 137: 126074912768.0000\n","Training loss (for one batch) at step 138: 2165410365440.0000\n","Training loss (for one batch) at step 139: 33457469440.0000\n","Training loss (for one batch) at step 140: 6296135680.0000\n","Training loss (for one batch) at step 141: 504792809472.0000\n","Training loss (for one batch) at step 142: 11491427328.0000\n","Training loss (for one batch) at step 143: 12465506304.0000\n","Training loss (for one batch) at step 144: 72810774528.0000\n","Training loss (for one batch) at step 145: 17728968704.0000\n","Training loss (for one batch) at step 146: 8799842304.0000\n","Training loss (for one batch) at step 147: 155695104000.0000\n","Training loss (for one batch) at step 148: 199164559360.0000\n","Training loss (for one batch) at step 149: 23351164928.0000\n","Training loss (for one batch) at step 150: 16738347008.0000\n","Training loss (for one batch) at step 151: 11958067200.0000\n","Training loss (for one batch) at step 152: 8995599360.0000\n","Training loss (for one batch) at step 153: 7687693312.0000\n","Training loss (for one batch) at step 154: 39980642304.0000\n","Training loss (for one batch) at step 155: 9066917888.0000\n","Training loss (for one batch) at step 156: 55848378368.0000\n","Training loss (for one batch) at step 157: 2895464704.0000\n","Training loss (for one batch) at step 158: 91288674304.0000\n","Training loss (for one batch) at step 159: 4681788416.0000\n","Training loss (for one batch) at step 160: 167441416192.0000\n","Training loss (for one batch) at step 161: 13552720896.0000\n","Training loss (for one batch) at step 162: 122517569536.0000\n","Training loss (for one batch) at step 163: 55079194624.0000\n","Training loss (for one batch) at step 164: 23395913728.0000\n","Training loss (for one batch) at step 165: 111122055168.0000\n","Training loss (for one batch) at step 166: 236625362944.0000\n","Training loss (for one batch) at step 167: 11026319360.0000\n","Training loss (for one batch) at step 168: 91448082432.0000\n","Training loss (for one batch) at step 169: 309217624064.0000\n","Training loss (for one batch) at step 170: 191228870656.0000\n","Training loss (for one batch) at step 171: 10334665728.0000\n","Training loss (for one batch) at step 172: 436119437312.0000\n","Training loss (for one batch) at step 173: 555971510272.0000\n","Training loss (for one batch) at step 174: 4130171136.0000\n","Training loss (for one batch) at step 175: 21779914752.0000\n","Training loss (for one batch) at step 176: 18685358080.0000\n","Training loss (for one batch) at step 177: 60539461632.0000\n","Training loss (for one batch) at step 178: 1432728320.0000\n","Training loss (for one batch) at step 179: 1306362368.0000\n","Training loss (for one batch) at step 180: 110429618176.0000\n","Training loss (for one batch) at step 181: 58836234240.0000\n","Training loss (for one batch) at step 182: 4087643648.0000\n","Training loss (for one batch) at step 183: 117102018560.0000\n","Training loss (for one batch) at step 184: 9831446528.0000\n","Training loss (for one batch) at step 185: 79178317824.0000\n","Training loss (for one batch) at step 186: 332949651456.0000\n","Training loss (for one batch) at step 187: 69342715904.0000\n","Training loss (for one batch) at step 188: 326705414144.0000\n","Training loss (for one batch) at step 189: 35932741632.0000\n","Training loss (for one batch) at step 190: 208862363648.0000\n","Training loss (for one batch) at step 191: 4194513152.0000\n","Training loss (for one batch) at step 192: 181317451776.0000\n","Training loss (for one batch) at step 193: 3395024384.0000\n","Training loss (for one batch) at step 194: 841887318016.0000\n","Training loss (for one batch) at step 195: 267373281280.0000\n","Training loss (for one batch) at step 196: 10314947584.0000\n","Training loss (for one batch) at step 197: 73409544192.0000\n","Training loss (for one batch) at step 198: 2811350016.0000\n","Training loss (for one batch) at step 199: 272799531008.0000\n","Training loss (for one batch) at step 200: 13351139328.0000\n","Training loss (for one batch) at step 201: 39947927552.0000\n","Training loss (for one batch) at step 202: 47730860032.0000\n","Training loss (for one batch) at step 203: 14108265472.0000\n","Training loss (for one batch) at step 204: 11424135168.0000\n","Training loss (for one batch) at step 205: 726895493120.0000\n","Training loss (for one batch) at step 206: 27631345664.0000\n","Training loss (for one batch) at step 207: 21208023040.0000\n","Training loss (for one batch) at step 208: 68105658368.0000\n","Training loss (for one batch) at step 209: 6699896320.0000\n","Training loss (for one batch) at step 210: 244876705792.0000\n","Training loss (for one batch) at step 211: 3928049408.0000\n","Training loss (for one batch) at step 212: 50603618304.0000\n","Training loss (for one batch) at step 213: 26443345920.0000\n","Training loss (for one batch) at step 214: 6502313984.0000\n","Training loss (for one batch) at step 215: 3185520148480.0000\n","Training loss (for one batch) at step 216: 3830473728.0000\n","Training loss (for one batch) at step 217: 1176947200.0000\n","Training loss (for one batch) at step 218: 3734961920.0000\n","Training loss (for one batch) at step 219: 30627547136.0000\n","Training loss (for one batch) at step 220: 858880540672.0000\n","Training loss (for one batch) at step 221: 3583351296.0000\n","Training loss (for one batch) at step 222: 15325188096.0000\n","Training loss (for one batch) at step 223: 19152244736.0000\n","Training loss (for one batch) at step 224: 35645526016.0000\n","Training loss (for one batch) at step 225: 262682787840.0000\n","Training loss (for one batch) at step 226: 4544424960.0000\n","Training loss (for one batch) at step 227: 256037109760.0000\n","Training loss (for one batch) at step 228: 2800889856.0000\n","Training loss (for one batch) at step 229: 18487613440.0000\n","Training loss (for one batch) at step 230: 6013436928.0000\n","Training loss (for one batch) at step 231: 79845703680.0000\n","Training loss (for one batch) at step 232: 21777772544.0000\n","Training loss (for one batch) at step 233: 218500890624.0000\n","Training loss (for one batch) at step 234: 38538928128.0000\n","Training loss (for one batch) at step 235: 6463339008.0000\n","Training loss (for one batch) at step 236: 5805617152.0000\n","Training loss (for one batch) at step 237: 183879024640.0000\n","Training loss (for one batch) at step 238: 182408904704.0000\n","Training loss (for one batch) at step 239: 87334289408.0000\n","Training loss (for one batch) at step 240: 528926441472.0000\n","Training loss (for one batch) at step 241: 12974313472.0000\n","Training loss (for one batch) at step 242: 30607933440.0000\n","Training loss (for one batch) at step 243: 3782212864.0000\n","Training loss (for one batch) at step 244: 1417940096.0000\n","Training loss (for one batch) at step 245: 29926502400.0000\n","Training loss (for one batch) at step 246: 4842899456.0000\n","Training loss (for one batch) at step 247: 183407050752.0000\n","Training loss (for one batch) at step 248: 120901648384.0000\n","Training loss (for one batch) at step 249: 125765853184.0000\n","Training loss (for one batch) at step 250: 128622493696.0000\n","Training loss (for one batch) at step 251: 39782572032.0000\n","Training loss (for one batch) at step 252: 13433910272.0000\n","Training loss (for one batch) at step 253: 27987171328.0000\n","Training loss (for one batch) at step 254: 16117405696.0000\n","Training loss (for one batch) at step 255: 38557335552.0000\n","Training loss (for one batch) at step 256: 106171514880.0000\n","Training loss (for one batch) at step 257: 18127007744.0000\n","Training loss (for one batch) at step 258: 14305653760.0000\n","Training loss (for one batch) at step 259: 15761724416.0000\n","Training loss (for one batch) at step 260: 25148131328.0000\n","Training loss (for one batch) at step 261: 150709485568.0000\n","Training loss (for one batch) at step 262: 12204522496.0000\n","Training loss (for one batch) at step 263: 392926298112.0000\n","Training loss (for one batch) at step 264: 310809952256.0000\n","Training loss (for one batch) at step 265: 15234195456.0000\n","Training loss (for one batch) at step 266: 49441828864.0000\n","Training loss (for one batch) at step 267: 16737016832.0000\n","Training loss (for one batch) at step 268: 25401958400.0000\n","Training loss (for one batch) at step 269: 70213222400.0000\n","Training loss (for one batch) at step 270: 5812186112.0000\n","Training loss (for one batch) at step 271: 202536402944.0000\n","Training loss (for one batch) at step 272: 4510138368.0000\n","Training loss (for one batch) at step 273: 163675865088.0000\n","Training loss (for one batch) at step 274: 41198682112.0000\n","Training loss (for one batch) at step 275: 39747837952.0000\n","Training loss (for one batch) at step 276: 123233574912.0000\n","Training loss (for one batch) at step 277: 17662879744.0000\n","Training loss (for one batch) at step 278: 132090232832.0000\n","Training loss (for one batch) at step 279: 154514522112.0000\n","Training loss (for one batch) at step 280: 19587330048.0000\n","Training loss (for one batch) at step 281: 10857289728.0000\n","Training loss (for one batch) at step 282: 27340525568.0000\n","Training loss (for one batch) at step 283: 286988664832.0000\n","Training loss (for one batch) at step 284: 28789526528.0000\n","Training loss (for one batch) at step 285: 13485575168.0000\n","Training loss (for one batch) at step 286: 374355427328.0000\n","Training loss (for one batch) at step 287: 6079268352.0000\n","Training loss (for one batch) at step 288: 131761913856.0000\n","Training loss (for one batch) at step 289: 490884399104.0000\n","Training loss (for one batch) at step 290: 10251307008.0000\n","Training loss (for one batch) at step 291: 322847801344.0000\n","Training loss (for one batch) at step 292: 35191812096.0000\n","Training loss (for one batch) at step 293: 1360131915776.0000\n","Training loss (for one batch) at step 294: 3551705088.0000\n","Training loss (for one batch) at step 295: 375954112512.0000\n","Training loss (for one batch) at step 296: 39622303744.0000\n","Training loss (for one batch) at step 297: 285340958720.0000\n","Training loss (for one batch) at step 298: 87013056512.0000\n","Training loss (for one batch) at step 299: 10158671872.0000\n","Training loss (for one batch) at step 300: 532470923264.0000\n","Training loss (for one batch) at step 301: 39151230976.0000\n","Training loss (for one batch) at step 302: 36786855936.0000\n","Training loss (for one batch) at step 303: 22778157056.0000\n","Training loss (for one batch) at step 304: 11430057984.0000\n","Training loss (for one batch) at step 305: 4352568320.0000\n","Training loss (for one batch) at step 306: 506539376640.0000\n","Training loss (for one batch) at step 307: 17866127360.0000\n","Training loss (for one batch) at step 308: 24797405184.0000\n","Training loss (for one batch) at step 309: 16212566016.0000\n","Training loss (for one batch) at step 310: 3106421760.0000\n","Training loss (for one batch) at step 311: 3235807744.0000\n","Training loss (for one batch) at step 312: 43356131328.0000\n","Training loss (for one batch) at step 313: 67896274944.0000\n","Training loss (for one batch) at step 314: 3894336552960.0000\n","Training loss (for one batch) at step 315: 516407394304.0000\n","Training loss (for one batch) at step 316: 3218488688640.0000\n","Training loss (for one batch) at step 317: 30693685248.0000\n","Training loss (for one batch) at step 318: 8010177024.0000\n","Training loss (for one batch) at step 319: 190419501056.0000\n","Training loss (for one batch) at step 320: 2820035584.0000\n","Training loss (for one batch) at step 321: 11368046592.0000\n","Training loss (for one batch) at step 322: 2269104384.0000\n","Training loss (for one batch) at step 323: 11320764416.0000\n","Training loss (for one batch) at step 324: 10465046528.0000\n","Training loss (for one batch) at step 325: 12275861504.0000\n","Training loss (for one batch) at step 326: 6040994816.0000\n","Training loss (for one batch) at step 327: 8134095872.0000\n","Training loss (for one batch) at step 328: 1914735360.0000\n","Training loss (for one batch) at step 329: 33015183360.0000\n","Training loss (for one batch) at step 330: 335179677696.0000\n","Training loss (for one batch) at step 331: 9343777792.0000\n","Training loss (for one batch) at step 332: 156640477184.0000\n","Training loss (for one batch) at step 333: 179915866112.0000\n","Training loss (for one batch) at step 334: 315834105856.0000\n","Training loss (for one batch) at step 335: 271659024384.0000\n","Training loss (for one batch) at step 336: 2869942026240.0000\n","Training loss (for one batch) at step 337: 59169386496.0000\n","Training loss (for one batch) at step 338: 43709583360.0000\n","Training loss (for one batch) at step 339: 34000091136.0000\n","Training loss (for one batch) at step 340: 31250851840.0000\n","Training loss (for one batch) at step 341: 42267516928.0000\n","Training loss (for one batch) at step 342: 21654579200.0000\n","Training loss (for one batch) at step 343: 5250938368.0000\n","Training loss (for one batch) at step 344: 5398731776.0000\n","Training loss (for one batch) at step 345: 59249033216.0000\n","Training loss (for one batch) at step 346: 11708003328.0000\n","Training loss (for one batch) at step 347: 102825533440.0000\n","Training loss (for one batch) at step 348: 16020866048.0000\n","Training loss (for one batch) at step 349: 42688946176.0000\n","Training loss (for one batch) at step 350: 899969600.0000\n","Training loss (for one batch) at step 351: 137738518528.0000\n","Training loss (for one batch) at step 352: 53667151872.0000\n","Training loss (for one batch) at step 353: 665402015744.0000\n","Training loss (for one batch) at step 354: 4472503296.0000\n","Training loss (for one batch) at step 355: 20258809856.0000\n","Training loss (for one batch) at step 356: 14766237696.0000\n","Training loss (for one batch) at step 357: 10810707968.0000\n","Training loss (for one batch) at step 358: 249416826880.0000\n","Training loss (for one batch) at step 359: 12600346624.0000\n","Training loss (for one batch) at step 360: 1017437888.0000\n","Training loss (for one batch) at step 361: 19548409856.0000\n","Training loss (for one batch) at step 362: 14805465088.0000\n","Training loss (for one batch) at step 363: 263829159936.0000\n","Training loss (for one batch) at step 364: 396832014336.0000\n","Training loss (for one batch) at step 365: 24167108608.0000\n","Training loss (for one batch) at step 366: 29162713088.0000\n","Training loss (for one batch) at step 367: 18135089152.0000\n","Training loss (for one batch) at step 368: 24515512320.0000\n","Training loss (for one batch) at step 369: 925073024.0000\n","Training loss (for one batch) at step 370: 7164473344.0000\n","Training loss (for one batch) at step 371: 29186967552.0000\n","Training loss (for one batch) at step 372: 72712445952.0000\n","Training loss (for one batch) at step 373: 6338331648.0000\n","Training loss (for one batch) at step 374: 3448060928.0000\n","Training loss (for one batch) at step 375: 263885357056.0000\n","Training loss (for one batch) at step 376: 22338877440.0000\n","Training loss (for one batch) at step 377: 6692360192.0000\n","Training loss (for one batch) at step 378: 38837977088.0000\n","Training loss (for one batch) at step 379: 69342576640.0000\n","Training loss (for one batch) at step 380: 8694386688.0000\n","Training loss (for one batch) at step 381: 8709369856.0000\n","Training loss (for one batch) at step 382: 132860469248.0000\n","Training loss (for one batch) at step 383: 12502778880.0000\n","Training loss (for one batch) at step 384: 3032872648704.0000\n","Training loss (for one batch) at step 385: 6424204800.0000\n","Training loss (for one batch) at step 386: 411694563328.0000\n","Training loss (for one batch) at step 387: 5574480896.0000\n","Training loss (for one batch) at step 388: 50594070528.0000\n","Training loss (for one batch) at step 389: 58699079680.0000\n","Training loss (for one batch) at step 390: 188104392704.0000\n","Training loss (for one batch) at step 391: 3106376960.0000\n","Training loss (for one batch) at step 392: 53141340160.0000\n","Training loss (for one batch) at step 393: 18595926016.0000\n","Training loss (for one batch) at step 394: 11357673472.0000\n","Training loss (for one batch) at step 395: 45245407232.0000\n","Training loss (for one batch) at step 396: 58819350528.0000\n","Training loss (for one batch) at step 397: 4872631296.0000\n","Training loss (for one batch) at step 398: 2840981504.0000\n","Training loss (for one batch) at step 399: 58078945280.0000\n","Training loss (for one batch) at step 400: 11268416512.0000\n","Training loss (for one batch) at step 401: 39419154432.0000\n","Training loss (for one batch) at step 402: 91741110272.0000\n","Training loss (for one batch) at step 403: 455047053312.0000\n","Training loss (for one batch) at step 404: 10114650112.0000\n","Training loss (for one batch) at step 405: 118954950656.0000\n","Training loss (for one batch) at step 406: 69379104768.0000\n","Training loss (for one batch) at step 407: 3721309440.0000\n","Training loss (for one batch) at step 408: 274016632832.0000\n","Training loss (for one batch) at step 409: 301109968896.0000\n","Training loss (for one batch) at step 410: 7511950336.0000\n","Training loss (for one batch) at step 411: 62950313984.0000\n","Training loss (for one batch) at step 412: 95404720128.0000\n","Training loss (for one batch) at step 413: 306027823104.0000\n","Training loss (for one batch) at step 414: 6620656640.0000\n","Training loss (for one batch) at step 415: 19328376832.0000\n","Training loss (for one batch) at step 416: 2723911168.0000\n","Training loss (for one batch) at step 417: 24182474752.0000\n","Training loss (for one batch) at step 418: 941244481536.0000\n","Training loss (for one batch) at step 419: 87107649536.0000\n","Training loss (for one batch) at step 420: 67579977728.0000\n","Training loss (for one batch) at step 421: 9186582528.0000\n","Training loss (for one batch) at step 422: 9596904448.0000\n","Training loss (for one batch) at step 423: 23815913472.0000\n","Training loss (for one batch) at step 424: 59805155328.0000\n","Training loss (for one batch) at step 425: 9260702720.0000\n","Training loss (for one batch) at step 426: 118882902016.0000\n","Training loss (for one batch) at step 427: 179320029184.0000\n","Training loss (for one batch) at step 428: 286706237440.0000\n","Training loss (for one batch) at step 429: 306693963776.0000\n","Training loss (for one batch) at step 430: 62786166784.0000\n","Training loss (for one batch) at step 431: 10745356288.0000\n","Training loss (for one batch) at step 432: 361483632640.0000\n","Training loss (for one batch) at step 433: 6997926912.0000\n","Training loss (for one batch) at step 434: 13041530880.0000\n","Training loss (for one batch) at step 435: 260622188544.0000\n","Training loss (for one batch) at step 436: 26671450112.0000\n","Training loss (for one batch) at step 437: 13818373120.0000\n","Training loss (for one batch) at step 438: 549548720128.0000\n","Training loss (for one batch) at step 439: 802034417664.0000\n","Training loss (for one batch) at step 440: 85760327680.0000\n","Training loss (for one batch) at step 441: 184220729344.0000\n","Training loss (for one batch) at step 442: 68824457216.0000\n","Training loss (for one batch) at step 443: 39113814016.0000\n","Training loss (for one batch) at step 444: 3707619072.0000\n","Training loss (for one batch) at step 445: 85894701056.0000\n","Training loss (for one batch) at step 446: 194184708096.0000\n","Training loss (for one batch) at step 447: 23474548736.0000\n","Training loss (for one batch) at step 448: 5716676096.0000\n","Training loss (for one batch) at step 449: 7354179584.0000\n","Training loss (for one batch) at step 450: 265454534656.0000\n","Training loss (for one batch) at step 451: 385670053888.0000\n","Training loss (for one batch) at step 452: 633062162432.0000\n","Training loss (for one batch) at step 453: 50042003456.0000\n","Training loss (for one batch) at step 454: 2638536769536.0000\n","Training loss (for one batch) at step 455: 70650896384.0000\n","Training loss (for one batch) at step 456: 24252393472.0000\n","Training loss (for one batch) at step 457: 77035421696.0000\n","Training loss (for one batch) at step 458: 29946521600.0000\n","Training loss (for one batch) at step 459: 32954081280.0000\n","Training loss (for one batch) at step 460: 292907057152.0000\n","Training loss (for one batch) at step 461: 4121719040.0000\n","Training loss (for one batch) at step 462: 79060115456.0000\n","Training loss (for one batch) at step 463: 16360986624.0000\n","Training loss (for one batch) at step 464: 9121745920.0000\n","Training loss (for one batch) at step 465: 18998337536.0000\n","Training loss (for one batch) at step 466: 6793193472.0000\n","Training loss (for one batch) at step 467: 5603671552.0000\n","Training loss (for one batch) at step 468: 195075211264.0000\n","Training loss (for one batch) at step 469: 49899773952.0000\n","Training loss (for one batch) at step 470: 46138007552.0000\n","Training loss (for one batch) at step 471: 92006432768.0000\n","Training loss (for one batch) at step 472: 9978224640.0000\n","Training loss (for one batch) at step 473: 7401142272.0000\n","Training loss (for one batch) at step 474: 55659610112.0000\n","Training loss (for one batch) at step 475: 557214990336.0000\n","Training loss (for one batch) at step 476: 15146127360.0000\n","Training loss (for one batch) at step 477: 29801850880.0000\n","Training loss (for one batch) at step 478: 22142728192.0000\n","Training loss (for one batch) at step 479: 336200990720.0000\n","Training loss (for one batch) at step 480: 7897747968.0000\n","Training loss (for one batch) at step 481: 27723409408.0000\n","Training loss (for one batch) at step 482: 20489259008.0000\n","Training loss (for one batch) at step 483: 48982884352.0000\n","Training loss (for one batch) at step 484: 216397103104.0000\n","Training loss (for one batch) at step 485: 1485251456.0000\n","Training loss (for one batch) at step 486: 2241498112.0000\n","Training loss (for one batch) at step 487: 164095737856.0000\n","Training loss (for one batch) at step 488: 5735104000.0000\n","Training loss (for one batch) at step 489: 73727508480.0000\n","Training loss (for one batch) at step 490: 85696446464.0000\n","Training loss (for one batch) at step 491: 82954993664.0000\n","Training loss (for one batch) at step 492: 4991032320.0000\n","Training loss (for one batch) at step 493: 10638028800.0000\n","Training loss (for one batch) at step 494: 7832988672.0000\n","Training loss (for one batch) at step 495: 27896969216.0000\n","Training loss (for one batch) at step 496: 282982744064.0000\n","Training loss (for one batch) at step 497: 48454299648.0000\n","Training loss (for one batch) at step 498: 10682362880.0000\n","Training loss (for one batch) at step 499: 92410216448.0000\n","Training loss (for one batch) at step 500: 8827374592.0000\n","Training loss (for one batch) at step 501: 3763167488.0000\n","Training loss (for one batch) at step 502: 34135930880.0000\n","Training loss (for one batch) at step 503: 20468916224.0000\n","Training loss (for one batch) at step 504: 3679855837184.0000\n","Training loss (for one batch) at step 505: 29222256640.0000\n","Training loss (for one batch) at step 506: 287283380224.0000\n","Training loss (for one batch) at step 507: 25300871168.0000\n","Training loss (for one batch) at step 508: 40539512832.0000\n","Training loss (for one batch) at step 509: 4382737920.0000\n","Training loss (for one batch) at step 510: 4230058752.0000\n","Training loss (for one batch) at step 511: 13743386624.0000\n","Training loss (for one batch) at step 512: 21167865856.0000\n","Training loss (for one batch) at step 513: 564918026240.0000\n","Training loss (for one batch) at step 514: 161770307584.0000\n","Training loss (for one batch) at step 515: 28039643136.0000\n","Training loss (for one batch) at step 516: 18405617664.0000\n","Training loss (for one batch) at step 517: 3099113728.0000\n","Training loss (for one batch) at step 518: 9054047232.0000\n","Training loss (for one batch) at step 519: 3615409152.0000\n","Training loss (for one batch) at step 520: 59733729280.0000\n","Training loss (for one batch) at step 521: 29934829568.0000\n","Training loss (for one batch) at step 522: 15288069120.0000\n","Training loss (for one batch) at step 523: 9493424128.0000\n","Training loss (for one batch) at step 524: 1909905280.0000\n","Training loss (for one batch) at step 525: 7939357696.0000\n","Training loss (for one batch) at step 526: 31559348224.0000\n","Training loss (for one batch) at step 527: 13132928000.0000\n","Training loss (for one batch) at step 528: 339856523264.0000\n","Training loss (for one batch) at step 529: 21609619456.0000\n","Training loss (for one batch) at step 530: 304217063424.0000\n","Training loss (for one batch) at step 531: 548270702592.0000\n","Training loss (for one batch) at step 532: 23172149248.0000\n","Training loss (for one batch) at step 533: 10922491904.0000\n","Training loss (for one batch) at step 534: 26532104192.0000\n","Training loss (for one batch) at step 535: 97978580992.0000\n","Training loss (for one batch) at step 536: 367367225344.0000\n","Training loss (for one batch) at step 537: 9305600000.0000\n","Training loss (for one batch) at step 538: 62201364480.0000\n","Training loss (for one batch) at step 539: 2858784768.0000\n","Training loss (for one batch) at step 540: 3685115904.0000\n","Training loss (for one batch) at step 541: 17103907840.0000\n","Training loss (for one batch) at step 542: 59535314944.0000\n","Training loss (for one batch) at step 543: 261770379264.0000\n","Training loss (for one batch) at step 544: 124287205376.0000\n","Training loss (for one batch) at step 545: 66747289600.0000\n","Training loss (for one batch) at step 546: 283366064128.0000\n","Training loss (for one batch) at step 547: 22807773184.0000\n","Training loss (for one batch) at step 548: 331229822976.0000\n","Training loss (for one batch) at step 549: 207562883072.0000\n","Training loss (for one batch) at step 550: 11232683008.0000\n","Training loss (for one batch) at step 551: 1729277696.0000\n","Training loss (for one batch) at step 552: 1023028800.0000\n","Training loss (for one batch) at step 553: 34529390592.0000\n","Training loss (for one batch) at step 554: 200637153280.0000\n","Training loss (for one batch) at step 555: 84869259264.0000\n","Training loss (for one batch) at step 556: 64736514048.0000\n","Training loss (for one batch) at step 557: 26464458752.0000\n","Training loss (for one batch) at step 558: 11941837824.0000\n","Training loss (for one batch) at step 559: 6000384512.0000\n","Training loss (for one batch) at step 560: 20129728512.0000\n","Training loss (for one batch) at step 561: 2375115520.0000\n","Training loss (for one batch) at step 562: 32837339136.0000\n","Training loss (for one batch) at step 563: 26794766336.0000\n","Training loss (for one batch) at step 564: 8480961536.0000\n","Training loss (for one batch) at step 565: 159396855808.0000\n","Training loss (for one batch) at step 566: 12352003072.0000\n","Training loss (for one batch) at step 567: 1187344547840.0000\n","Training loss (for one batch) at step 568: 6117159936.0000\n","Training loss (for one batch) at step 569: 22720923648.0000\n","Training loss (for one batch) at step 570: 6517912576.0000\n","Training loss (for one batch) at step 571: 222393319424.0000\n","Training loss (for one batch) at step 572: 264680177664.0000\n","Training loss (for one batch) at step 573: 2536272384.0000\n","Training loss (for one batch) at step 574: 35139407872.0000\n","Training loss (for one batch) at step 575: 11977180160.0000\n","Training loss (for one batch) at step 576: 61578903552.0000\n","Training loss (for one batch) at step 577: 119799726080.0000\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.initializers import GlorotUniform\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Normalize data\n","scaler = MinMaxScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Use a smaller learning rate\n","optimizer = Adam(learning_rate=0.0000001)\n","\n","# Use a better initializer\n","initializer = GlorotUniform()\n","\n","model = keras.Sequential([\n","    keras.layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1] + 1,), kernel_initializer=initializer),\n","    keras.layers.Dense(64, activation='relu', kernel_initializer=initializer),\n","    keras.layers.Dense(1, kernel_initializer=initializer)\n","])\n","\n","model.compile(optimizer=optimizer, loss='mean_squared_error')\n","\n","# Define the loss function\n","loss_fn = tf.keras.losses.MeanSquaredError()\n","\n","# Training parameters\n","epochs = 100\n","batch_size = 4\n","\n","# Train the model with custom loop\n","for epoch in range(epochs):\n","    print(\"\\nStart of epoch %d\" % (epoch,))\n","    # Initialize feedback for each epoch\n","    output_feedback = np.zeros((batch_size, 1))  # Make sure feedback is correctly sized\n","\n","    for step in range(len(X_train_scaled) // batch_size):\n","        # Select batch\n","        batch_X = X_train_scaled[step * batch_size:(step + 1) * batch_size]\n","        batch_y = y_train[step * batch_size:(step + 1) * batch_size]\n","\n","        # Handle last batch which might be smaller than the batch_size\n","        actual_batch_size = batch_X.shape[0]\n","        if actual_batch_size < batch_size:\n","            output_feedback = np.zeros((actual_batch_size, 1))\n","\n","        # Concatenate the output feedback\n","        batch_X_with_feedback = np.hstack([batch_X, output_feedback])\n","\n","        with tf.GradientTape() as tape:\n","            # Run the model\n","            logits = model(batch_X_with_feedback, training=True)\n","\n","            # Save output for feedback\n","            output_feedback = logits.numpy()\n","\n","            # Compute the loss\n","            loss_value = loss_fn(batch_y, logits)\n","\n","        # Update weights\n","        grads = tape.gradient(loss_value, model.trainable_weights)\n","        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","\n","        print(\"Training loss (for one batch) at step %d: %.4f\" % (step, float(loss_value)))\n","\n","# Evaluate the model\n","X_test_with_feedback = np.hstack([X_test_scaled, np.zeros((len(X_test_scaled), 1))])  # No feedback when testing\n","predictions = model.predict(X_test_with_feedback)\n","\n","# Evaluate the model\n","mae = mean_absolute_error(y_test, predictions)\n","mse = mean_squared_error(y_test, predictions)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, predictions)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ELT-Yx9DtxA0"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"XknEoIzhx1Qb"},"source":["Spiking Neural Network (SNN)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":113701,"status":"ok","timestamp":1719542847175,"user":{"displayName":"Shabana Yasmin","userId":"02459626693949289733"},"user_tz":-330},"id":"2z8J38CzuUDZ","outputId":"c1a03638-48d6-499e-f68f-f88e9d9d659f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting norse\n","  Downloading norse-1.1.0.tar.gz (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from norse) (1.25.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from norse) (2.3.0+cu121)\n","Requirement already satisfied: torchvision>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from norse) (0.18.0+cu121)\n","Collecting nir (from norse)\n","  Downloading nir-1.0.4-py3-none-any.whl (18 kB)\n","Collecting nirtorch (from norse)\n","  Downloading nirtorch-1.0-py3-none-any.whl (13 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->norse) (3.15.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->norse) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->norse) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->norse) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->norse) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->norse) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.0.0->norse)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.0.0->norse)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.0.0->norse)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.0.0->norse)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.0.0->norse)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.0.0->norse)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.0.0->norse)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.0.0->norse)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.0.0->norse)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.0.0->norse)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.0.0->norse)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->norse) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->norse)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.15.0->norse) (9.4.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nir->norse) (3.9.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->norse) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->norse) (1.3.0)\n","Building wheels for collected packages: norse\n","  Building wheel for norse (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for norse: filename=norse-1.1.0-py3-none-any.whl size=1539018 sha256=49886275dc580df6b5640d7f20713988a9d43949aa51cf486a7ea3d73a7ef959\n","  Stored in directory: /root/.cache/pip/wheels/16/fc/0d/4cbb14992b7e5bb35482df57e887a2ab55cad9ea890501cf61\n","Successfully built norse\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nir, nvidia-cusolver-cu12, nirtorch, norse\n","Successfully installed nir-1.0.4 nirtorch-1.0 norse-1.1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"]}],"source":["!pip install norse"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1283,"status":"ok","timestamp":1719539067888,"user":{"displayName":"Shabana Yasmin","userId":"02459626693949289733"},"user_tz":-330},"id":"lblmul71wJcg","outputId":"9ec1f88e-8182-47e1-ad24-1cec0908cd94"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss 2424271798272.0\n","Epoch 1, Loss 2424271798272.0\n","Epoch 2, Loss 2424271798272.0\n","Epoch 3, Loss 2424271798272.0\n","Epoch 4, Loss 2424271798272.0\n","Epoch 5, Loss 2424271798272.0\n","Epoch 6, Loss 2424271798272.0\n","Epoch 7, Loss 2424271798272.0\n","Epoch 8, Loss 2424271536128.0\n","Epoch 9, Loss 2424271536128.0\n","Epoch 10, Loss 2424271536128.0\n","Epoch 11, Loss 2424271536128.0\n","Epoch 12, Loss 2424271536128.0\n","Epoch 13, Loss 2424271536128.0\n","Epoch 14, Loss 2424271536128.0\n","Epoch 15, Loss 2424271536128.0\n","Epoch 16, Loss 2424271536128.0\n","Epoch 17, Loss 2424271536128.0\n","Epoch 18, Loss 2424271536128.0\n","Epoch 19, Loss 2424271536128.0\n","Epoch 20, Loss 2424271536128.0\n","Epoch 21, Loss 2424271536128.0\n","Epoch 22, Loss 2424271536128.0\n","Epoch 23, Loss 2424271536128.0\n","Epoch 24, Loss 2424271536128.0\n","Epoch 25, Loss 2424271536128.0\n","Epoch 26, Loss 2424271536128.0\n","Epoch 27, Loss 2424271536128.0\n","Epoch 28, Loss 2424271536128.0\n","Epoch 29, Loss 2424271536128.0\n","Epoch 30, Loss 2424271536128.0\n","Epoch 31, Loss 2424271536128.0\n","Epoch 32, Loss 2424271536128.0\n","Epoch 33, Loss 2424271536128.0\n","Epoch 34, Loss 2424271536128.0\n","Epoch 35, Loss 2424271536128.0\n","Epoch 36, Loss 2424271536128.0\n","Epoch 37, Loss 2424271536128.0\n","Epoch 38, Loss 2424271536128.0\n","Epoch 39, Loss 2424271273984.0\n","Epoch 40, Loss 2424271011840.0\n","Epoch 41, Loss 2424271011840.0\n","Epoch 42, Loss 2424271011840.0\n","Epoch 43, Loss 2424271011840.0\n","Epoch 44, Loss 2424271011840.0\n","Epoch 45, Loss 2424271011840.0\n","Epoch 46, Loss 2424271011840.0\n","Epoch 47, Loss 2424271011840.0\n","Epoch 48, Loss 2424271011840.0\n","Epoch 49, Loss 2424271011840.0\n","Epoch 50, Loss 2424271011840.0\n","Epoch 51, Loss 2424271011840.0\n","Epoch 52, Loss 2424271011840.0\n","Epoch 53, Loss 2424271011840.0\n","Epoch 54, Loss 2424271011840.0\n","Epoch 55, Loss 2424271011840.0\n","Epoch 56, Loss 2424271011840.0\n","Epoch 57, Loss 2424271011840.0\n","Epoch 58, Loss 2424271011840.0\n","Epoch 59, Loss 2424271011840.0\n","Epoch 60, Loss 2424271011840.0\n","Epoch 61, Loss 2424271011840.0\n","Epoch 62, Loss 2424271011840.0\n","Epoch 63, Loss 2424271011840.0\n","Epoch 64, Loss 2424271011840.0\n","Epoch 65, Loss 2424271011840.0\n","Epoch 66, Loss 2424271011840.0\n","Epoch 67, Loss 2424271011840.0\n","Epoch 68, Loss 2424271011840.0\n","Epoch 69, Loss 2424271011840.0\n","Epoch 70, Loss 2424271011840.0\n","Epoch 71, Loss 2424271011840.0\n","Epoch 72, Loss 2424271011840.0\n","Epoch 73, Loss 2424271011840.0\n","Epoch 74, Loss 2424271011840.0\n","Epoch 75, Loss 2424270749696.0\n","Epoch 76, Loss 2424270749696.0\n","Epoch 77, Loss 2424270749696.0\n","Epoch 78, Loss 2424270749696.0\n","Epoch 79, Loss 2424270749696.0\n","Epoch 80, Loss 2424270749696.0\n","Epoch 81, Loss 2424270749696.0\n","Epoch 82, Loss 2424270749696.0\n","Epoch 83, Loss 2424270749696.0\n","Epoch 84, Loss 2424270749696.0\n","Epoch 85, Loss 2424270749696.0\n","Epoch 86, Loss 2424270749696.0\n","Epoch 87, Loss 2424270749696.0\n","Epoch 88, Loss 2424270749696.0\n","Epoch 89, Loss 2424270487552.0\n","Epoch 90, Loss 2424270487552.0\n","Epoch 91, Loss 2424270487552.0\n","Epoch 92, Loss 2424270487552.0\n","Epoch 93, Loss 2424270487552.0\n","Epoch 94, Loss 2424270487552.0\n","Epoch 95, Loss 2424270487552.0\n","Epoch 96, Loss 2424270487552.0\n","Epoch 97, Loss 2424270487552.0\n","Epoch 98, Loss 2424270487552.0\n","Epoch 99, Loss 2424270487552.0\n","MAE: 203789.3\n","Mean Squared Error: 186765310000.0\n","RMSE: 432163.53\n","R-squared: -0.2859460115432739\n"]}],"source":["import torch\n","from torch import nn\n","from norse.torch import LIFCell, LIFState  # Ensure correct imports for LIFCell and LIFState\n","from torch.optim import Adam\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import pandas as pd\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","# Convert Series to numpy array if necessary\n","if isinstance(y, pd.Series):\n","    y = y.to_numpy()\n","\n","# Splitting dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardizing the features\n","scaler = StandardScaler()\n","X_train_scaled = torch.tensor(scaler.fit_transform(X_train), dtype=torch.float32)\n","X_test_scaled = torch.tensor(scaler.transform(X_test), dtype=torch.float32)\n","y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Make y_train a column vector\n","y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)    # Make y_test a column vector\n","\n","# Define the SNN model\n","class SNNRegressor(nn.Module):\n","    def __init__(self, input_size):\n","        super(SNNRegressor, self).__init__()\n","        self.lif1 = LIFCell()\n","        self.fc = nn.Linear(input_size, 1)  # Output layer\n","\n","    def forward(self, x):\n","        batch_size, seq_len, _ = x.size()\n","        s = torch.zeros(batch_size, 1)\n","        v = torch.zeros(batch_size, 1)\n","        z = torch.zeros(batch_size, 1)  # Initialize adaptation variable `z` to zero\n","        state = LIFState(v=v, i=s, z=z)  # Add the missing `z` parameter\n","\n","        for t in range(seq_len):\n","            out, state = self.lif1(x[:, t], state)\n","\n","        out = self.fc(state.v)\n","        return out\n","\n","# Adjust the input shape to match LIFCell\n","X_train_scaled = X_train_scaled.unsqueeze(1)  # Add a dimension for sequence length\n","X_test_scaled = X_test_scaled.unsqueeze(1)    # Add a dimension for sequence length\n","\n","# Training the SNN\n","input_size = X_train_scaled.shape[2]\n","model = SNNRegressor(input_size=input_size)\n","criterion = nn.MSELoss()\n","optimizer = Adam(model.parameters(), lr=0.02)\n","\n","# Train the model\n","for epoch in range(100):\n","    model.train()\n","    optimizer.zero_grad()\n","    outputs = model(X_train_scaled)\n","    loss = criterion(outputs, y_train)\n","    loss.backward()\n","    optimizer.step()\n","    print(f\"Epoch {epoch}, Loss {loss.item()}\")\n","\n","# Evaluation\n","model.eval()\n","with torch.no_grad():\n","    predictions = model(X_test_scaled)\n","\n","    # Convert tensors to NumPy arrays for compatibility with scikit-learn\n","    y_test_np = y_test.numpy()\n","    predictions_np = predictions.numpy()\n","\n","    # Evaluate the model\n","    mae = mean_absolute_error(y_test_np, predictions_np)\n","    mse = mean_squared_error(y_test_np, predictions_np)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_test_np, predictions_np)\n","\n","    # Printing Result\n","    print(\"MAE:\", mae)\n","    print(\"Mean Squared Error:\", mse)\n","    print(\"RMSE:\", rmse)\n","    print(\"R-squared:\", r2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eHc1kPsAw5od","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719543598197,"user_tz":-330,"elapsed":6354,"user":{"displayName":"Shabana Yasmin","userId":"02459626693949289733"}},"outputId":"9a67de86-eeed-42a6-9456-accd15bff4ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss 2424271798272.0\n","Epoch 1, Loss 2424271798272.0\n","Epoch 2, Loss 2424271536128.0\n","Epoch 3, Loss 2424271536128.0\n","Epoch 4, Loss 2424271011840.0\n","Epoch 5, Loss 2424270749696.0\n","Epoch 6, Loss 2424270749696.0\n","Epoch 7, Loss 2424270749696.0\n","Epoch 8, Loss 2424270487552.0\n","Epoch 9, Loss 2424270225408.0\n","Epoch 10, Loss 2424269701120.0\n","Epoch 11, Loss 2424268914688.0\n","Epoch 12, Loss 2424268652544.0\n","Epoch 13, Loss 2424268652544.0\n","Epoch 14, Loss 2424268914688.0\n","Epoch 15, Loss 2424268914688.0\n","Epoch 16, Loss 2424269176832.0\n","Epoch 17, Loss 2424268914688.0\n","Epoch 18, Loss 2424268390400.0\n","Epoch 19, Loss 2424267079680.0\n","Epoch 20, Loss 2424267079680.0\n","Epoch 21, Loss 2424266555392.0\n","Epoch 22, Loss 2424266555392.0\n","Epoch 23, Loss 2424266817536.0\n","Epoch 24, Loss 2424266293248.0\n","Epoch 25, Loss 2424265768960.0\n","Epoch 26, Loss 2424265244672.0\n","Epoch 27, Loss 2424265244672.0\n","Epoch 28, Loss 2424265506816.0\n","Epoch 29, Loss 2424264982528.0\n","Epoch 30, Loss 2424264720384.0\n","Epoch 31, Loss 2424263933952.0\n","Epoch 32, Loss 2424263933952.0\n","Epoch 33, Loss 2424263147520.0\n","Epoch 34, Loss 2424263147520.0\n","Epoch 35, Loss 2424262361088.0\n","Epoch 36, Loss 2424262098944.0\n","Epoch 37, Loss 2424262098944.0\n","Epoch 38, Loss 2424261836800.0\n","Epoch 39, Loss 2424261312512.0\n","Epoch 40, Loss 2424261312512.0\n","Epoch 41, Loss 2424260788224.0\n","Epoch 42, Loss 2424260788224.0\n","Epoch 43, Loss 2424259739648.0\n","Epoch 44, Loss 2424259739648.0\n","Epoch 45, Loss 2424259477504.0\n","Epoch 46, Loss 2424259739648.0\n","Epoch 47, Loss 2424258691072.0\n","Epoch 48, Loss 2424259477504.0\n","Epoch 49, Loss 2424257380352.0\n","Epoch 50, Loss 2424259477504.0\n","Epoch 51, Loss 2424256856064.0\n","Epoch 52, Loss 2424257904640.0\n","Epoch 53, Loss 2424255807488.0\n","Epoch 54, Loss 2424257904640.0\n","Epoch 55, Loss 2424255807488.0\n","Epoch 56, Loss 2424256856064.0\n","Epoch 57, Loss 2424255807488.0\n","Epoch 58, Loss 2424255545344.0\n","Epoch 59, Loss 2424255021056.0\n","Epoch 60, Loss 2424255021056.0\n","Epoch 61, Loss 2424255021056.0\n","Epoch 62, Loss 2424253448192.0\n","Epoch 63, Loss 2424256069632.0\n","Epoch 64, Loss 2424252399616.0\n","Epoch 65, Loss 2424253710336.0\n","Epoch 66, Loss 2424257118208.0\n","Epoch 67, Loss 2424251875328.0\n","Epoch 68, Loss 2424252923904.0\n","Epoch 69, Loss 2424254234624.0\n","Epoch 70, Loss 2424253710336.0\n","Epoch 71, Loss 2424251613184.0\n","Epoch 72, Loss 2424251351040.0\n","Epoch 73, Loss 2424252137472.0\n","Epoch 74, Loss 2424252399616.0\n","Epoch 75, Loss 2424249253888.0\n","Epoch 76, Loss 2424250564608.0\n","Epoch 77, Loss 2424252399616.0\n","Epoch 78, Loss 2424248467456.0\n","Epoch 79, Loss 2424249778176.0\n","Epoch 80, Loss 2424249253888.0\n","Epoch 81, Loss 2424248991744.0\n","Epoch 82, Loss 2424247943168.0\n","Epoch 83, Loss 2424248205312.0\n","Epoch 84, Loss 2424248205312.0\n","Epoch 85, Loss 2424247943168.0\n","Epoch 86, Loss 2424246894592.0\n","Epoch 87, Loss 2424246370304.0\n","Epoch 88, Loss 2424245846016.0\n","Epoch 89, Loss 2424247681024.0\n","Epoch 90, Loss 2424244535296.0\n","Epoch 91, Loss 2424246370304.0\n","Epoch 92, Loss 2424248205312.0\n","Epoch 93, Loss 2424243486720.0\n","Epoch 94, Loss 2424245846016.0\n","Epoch 95, Loss 2424245059584.0\n","Epoch 96, Loss 2424247418880.0\n","Epoch 97, Loss 2424242438144.0\n","Epoch 98, Loss 2424243748864.0\n","Epoch 99, Loss 2424249778176.0\n","MAE: 203727.8\n","Mean Squared Error: 186740150000.0\n","RMSE: 432134.4\n","R-squared: -0.28577280044555664\n"]}],"source":["import torch\n","from torch import nn\n","from norse.torch import LIFCell, LIFState  # Ensure correct imports for LIFCell and LIFState\n","from torch.optim import Adam\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import pandas as pd\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import numpy as np\n","\n","# Load your dataset `df`\n","# Assuming df is your pandas DataFrame with appropriate columns\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","# Convert Series to numpy array if necessary\n","if isinstance(y, pd.Series):\n","    y = y.to_numpy()\n","\n","# Splitting dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardizing the features\n","scaler = StandardScaler()\n","X_train_scaled = torch.tensor(scaler.fit_transform(X_train), dtype=torch.float32)\n","X_test_scaled = torch.tensor(scaler.transform(X_test), dtype=torch.float32)\n","y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Make y_train a column vector\n","y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)    # Make y_test a column vector\n","\n","# Define the SNN model with enhanced architecture\n","class EnhancedSNNRegressor(nn.Module):\n","    def __init__(self, input_size):\n","        super(EnhancedSNNRegressor, self).__init__()\n","        self.lif1 = LIFCell()\n","        self.lif2 = LIFCell()\n","        self.fc1 = nn.Linear(input_size, 128)  # Increase neurons in hidden layer\n","        self.fc2 = nn.Linear(128, 64)  # Additional layer\n","        self.fc3 = nn.Linear(64, 1)  # Output layer\n","\n","    def forward(self, x):\n","        batch_size, seq_len, _ = x.size()\n","        s = torch.zeros(batch_size, 1)\n","        v = torch.zeros(batch_size, 1)\n","        z = torch.zeros(batch_size, 1)  # Initialize adaptation variable `z` to zero\n","        state = LIFState(v=v, i=s, z=z)  # Add the missing `z` parameter\n","\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","\n","        for t in range(seq_len):\n","            out, state = self.lif1(x[:, t], state)\n","\n","        out = self.fc3(state.v)\n","        return out\n","\n","# Adjust the input shape to match LIFCell\n","X_train_scaled = X_train_scaled.unsqueeze(1)  # Add a dimension for sequence length\n","X_test_scaled = X_test_scaled.unsqueeze(1)    # Add a dimension for sequence length\n","\n","# Training the Enhanced SNN\n","input_size = X_train_scaled.shape[2]\n","model = EnhancedSNNRegressor(input_size=input_size)\n","criterion = nn.MSELoss()\n","optimizer = Adam(model.parameters(), lr=0.02, weight_decay=1e-5)  # Add weight decay\n","\n","# Train the model\n","for epoch in range(100):\n","    model.train()\n","    optimizer.zero_grad()\n","    outputs = model(X_train_scaled)\n","    loss = criterion(outputs, y_train)\n","    loss.backward()\n","    optimizer.step()\n","    print(f\"Epoch {epoch}, Loss {loss.item()}\")\n","\n","# Evaluation\n","model.eval()\n","with torch.no_grad():\n","    predictions = model(X_test_scaled)\n","\n","    # Convert tensors to NumPy arrays for compatibility with scikit-learn\n","    y_test_np = y_test.numpy()\n","    predictions_np = predictions.numpy()\n","\n","    # Evaluate the model\n","    mae = mean_absolute_error(y_test_np, predictions_np)\n","    mse = mean_squared_error(y_test_np, predictions_np)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_test_np, predictions_np)\n","\n","    # Printing Result\n","    print(\"MAE:\", mae)\n","    print(\"Mean Squared Error:\", mse)\n","    print(\"RMSE:\", rmse)\n","    print(\"R-squared:\", r2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9yRK6KoB4NkY"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["FF with Backpropagation"],"metadata":{"id":"jK8e8BqnS7SD"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Scale the data\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Reshape X_train to the correct shape (number of samples, number of features)\n","X_train = X_train.reshape(X_train.shape[0], -1)\n","X_test = X_test.reshape(X_test.shape[0], -1)\n","\n","# Example of further hyperparameter tuning and regularization\n","model_ffnn = tf.keras.models.Sequential([\n","    tf.keras.layers.Dense(128, input_dim=X_train.shape[1], activation='relu'),\n","    tf.keras.layers.Dropout(0.3),\n","    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n","    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n","    tf.keras.layers.Dense(1)\n","])\n","\n","# Compile with adjusted optimizer and learning rate\n","optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n","model_ffnn.compile(optimizer=optimizer, loss='mean_squared_error')\n","\n","# Further adjust epochs and batch size\n","model_ffnn.fit(X_train, y_train, epochs=200, batch_size=64, validation_split=0.1)\n","\n","\n","# Evaluate the model\n","loss_ffnn = model_ffnn.evaluate(X_test, y_test)\n","print(f\"FFNN Test Loss: {loss_ffnn}\")\n","\n","# Predicting costs using the FFNN\n","predictions_ffnn = model_ffnn.predict(X_test)\n","mae = mean_absolute_error(y_test, predictions_ffnn)\n","mse = mean_squared_error(y_test, predictions_ffnn)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, predictions_ffnn)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hsWzqjEr1zd_","executionInfo":{"status":"ok","timestamp":1719545506961,"user_tz":-330,"elapsed":119288,"user":{"displayName":"Shabana Yasmin","userId":"02459626693949289733"}},"outputId":"3ff936eb-dc4f-4bd5-ece7-d62367036890"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 241919754240.0000 - val_loss: 16680259420160.0000\n","Epoch 2/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 738527477760.0000 - val_loss: 16676299997184.0000\n","Epoch 3/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 447248367616.0000 - val_loss: 16654859763712.0000\n","Epoch 4/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 965605130240.0000 - val_loss: 16602805305344.0000\n","Epoch 5/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 653027704832.0000 - val_loss: 16547384918016.0000\n","Epoch 6/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 211037978624.0000 - val_loss: 16521638182912.0000\n","Epoch 7/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 165765562368.0000 - val_loss: 16515961192448.0000\n","Epoch 8/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 714164469760.0000 - val_loss: 16514816147456.0000\n","Epoch 9/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1193882550272.0000 - val_loss: 16514447048704.0000\n","Epoch 10/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 241461772288.0000 - val_loss: 16512572194816.0000\n","Epoch 11/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1462969040896.0000 - val_loss: 16511722848256.0000\n","Epoch 12/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 216877711360.0000 - val_loss: 16510545297408.0000\n","Epoch 13/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 586677878784.0000 - val_loss: 16508410396672.0000\n","Epoch 14/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2310734610432.0000 - val_loss: 16507701559296.0000\n","Epoch 15/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 458786078720.0000 - val_loss: 16504995184640.0000\n","Epoch 16/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 312533188608.0000 - val_loss: 16503293345792.0000\n","Epoch 17/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 202394255360.0000 - val_loss: 16500993818624.0000\n","Epoch 18/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 600702648320.0000 - val_loss: 16498474090496.0000\n","Epoch 19/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 285226270720.0000 - val_loss: 16496196583424.0000\n","Epoch 20/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1274483048448.0000 - val_loss: 16494872231936.0000\n","Epoch 21/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 230575308800.0000 - val_loss: 16492584239104.0000\n","Epoch 22/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 342775037952.0000 - val_loss: 16489953361920.0000\n","Epoch 23/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 391943684096.0000 - val_loss: 16487523811328.0000\n","Epoch 24/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 75782430720.0000 - val_loss: 16485319704576.0000\n","Epoch 25/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 660437925888.0000 - val_loss: 16482470723584.0000\n","Epoch 26/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 933402247168.0000 - val_loss: 16480947142656.0000\n","Epoch 27/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 146086625280.0000 - val_loss: 16479629082624.0000\n","Epoch 28/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 937942122496.0000 - val_loss: 16477585408000.0000\n","Epoch 29/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 108528066560.0000 - val_loss: 16475125448704.0000\n","Epoch 30/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1295357968384.0000 - val_loss: 16471918903296.0000\n","Epoch 31/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 325483790336.0000 - val_loss: 16471164977152.0000\n","Epoch 32/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3213383696384.0000 - val_loss: 16469293268992.0000\n","Epoch 33/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 557928349696.0000 - val_loss: 16468773175296.0000\n","Epoch 34/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 576483819520.0000 - val_loss: 16468506836992.0000\n","Epoch 35/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 136046567424.0000 - val_loss: 16467009470464.0000\n","Epoch 36/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 897800273920.0000 - val_loss: 16467058753536.0000\n","Epoch 37/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 50328125440.0000 - val_loss: 16465412489216.0000\n","Epoch 38/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 350989647872.0000 - val_loss: 16464242278400.0000\n","Epoch 39/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 136262164480.0000 - val_loss: 16464527491072.0000\n","Epoch 40/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 491034050560.0000 - val_loss: 16462935752704.0000\n","Epoch 41/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 310986964992.0000 - val_loss: 16462937849856.0000\n","Epoch 42/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 532169326592.0000 - val_loss: 16462131494912.0000\n","Epoch 43/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 317967269888.0000 - val_loss: 16462957772800.0000\n","Epoch 44/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 78938193920.0000 - val_loss: 16462417756160.0000\n","Epoch 45/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 93939228672.0000 - val_loss: 16461523320832.0000\n","Epoch 46/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 686973911040.0000 - val_loss: 16461909196800.0000\n","Epoch 47/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 81442447360.0000 - val_loss: 16461964771328.0000\n","Epoch 48/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 427618107392.0000 - val_loss: 16462701920256.0000\n","Epoch 49/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 911229648896.0000 - val_loss: 16463140225024.0000\n","Epoch 50/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2284595183616.0000 - val_loss: 16463431729152.0000\n","Epoch 51/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 126602215424.0000 - val_loss: 16463487303680.0000\n","Epoch 52/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 162281848832.0000 - val_loss: 16462839283712.0000\n","Epoch 53/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 410397900800.0000 - val_loss: 16462258372608.0000\n","Epoch 54/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1208257085440.0000 - val_loss: 16463233548288.0000\n","Epoch 55/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 373015838720.0000 - val_loss: 16463868985344.0000\n","Epoch 56/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 853540995072.0000 - val_loss: 16463497789440.0000\n","Epoch 57/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 70721757184.0000 - val_loss: 16464005300224.0000\n","Epoch 58/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 392117977088.0000 - val_loss: 16463131836416.0000\n","Epoch 59/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 900126343168.0000 - val_loss: 16463814459392.0000\n","Epoch 60/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1775953117184.0000 - val_loss: 16464881909760.0000\n","Epoch 61/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 972937428992.0000 - val_loss: 16464359718912.0000\n","Epoch 62/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 373434941440.0000 - val_loss: 16464014737408.0000\n","Epoch 63/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1799900495872.0000 - val_loss: 16463484157952.0000\n","Epoch 64/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1652790132736.0000 - val_loss: 16464580968448.0000\n","Epoch 65/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1028506517504.0000 - val_loss: 16464462479360.0000\n","Epoch 66/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 271623815168.0000 - val_loss: 16464288415744.0000\n","Epoch 67/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1290532421632.0000 - val_loss: 16464497082368.0000\n","Epoch 68/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 182398074880.0000 - val_loss: 16464152100864.0000\n","Epoch 69/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1112240947200.0000 - val_loss: 16463894151168.0000\n","Epoch 70/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 825603063808.0000 - val_loss: 16464793829376.0000\n","Epoch 71/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 550210699264.0000 - val_loss: 16464213966848.0000\n","Epoch 72/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 574911283200.0000 - val_loss: 16464167829504.0000\n","Epoch 73/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 434101387264.0000 - val_loss: 16464160489472.0000\n","Epoch 74/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 122147504128.0000 - val_loss: 16463620472832.0000\n","Epoch 75/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 310250143744.0000 - val_loss: 16463370911744.0000\n","Epoch 76/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1072240590848.0000 - val_loss: 16464360767488.0000\n","Epoch 77/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 229380947968.0000 - val_loss: 16463829139456.0000\n","Epoch 78/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 930016198656.0000 - val_loss: 16464095477760.0000\n","Epoch 79/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 81840095232.0000 - val_loss: 16464164683776.0000\n","Epoch 80/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 616687403008.0000 - val_loss: 16462790000640.0000\n","Epoch 81/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1396188512256.0000 - val_loss: 16463328968704.0000\n","Epoch 82/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 160292388864.0000 - val_loss: 16463639347200.0000\n","Epoch 83/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 297863348224.0000 - val_loss: 16463057387520.0000\n","Epoch 84/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 948696121344.0000 - val_loss: 16462382104576.0000\n","Epoch 85/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 303438364672.0000 - val_loss: 16462715551744.0000\n","Epoch 86/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 83317702656.0000 - val_loss: 16463493595136.0000\n","Epoch 87/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 710774816768.0000 - val_loss: 16462571896832.0000\n","Epoch 88/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 420921737216.0000 - val_loss: 16462156660736.0000\n","Epoch 89/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 874901274624.0000 - val_loss: 16461594624000.0000\n","Epoch 90/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1016959860736.0000 - val_loss: 16461517029376.0000\n","Epoch 91/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1311152930816.0000 - val_loss: 16461963722752.0000\n","Epoch 92/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 84142882816.0000 - val_loss: 16461383860224.0000\n","Epoch 93/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 849469636608.0000 - val_loss: 16461372325888.0000\n","Epoch 94/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1121973305344.0000 - val_loss: 16460986449920.0000\n","Epoch 95/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 119506436096.0000 - val_loss: 16461523320832.0000\n","Epoch 96/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 267972526080.0000 - val_loss: 16460434898944.0000\n","Epoch 97/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 135800610816.0000 - val_loss: 16461117521920.0000\n","Epoch 98/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 562496602112.0000 - val_loss: 16460698091520.0000\n","Epoch 99/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 85001895936.0000 - val_loss: 16460970721280.0000\n","Epoch 100/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2295431692288.0000 - val_loss: 16460360450048.0000\n","Epoch 101/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 831614681088.0000 - val_loss: 16459621203968.0000\n","Epoch 102/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 643267624960.0000 - val_loss: 16459391565824.0000\n","Epoch 103/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2565726797824.0000 - val_loss: 16459689361408.0000\n","Epoch 104/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 754620760064.0000 - val_loss: 16458574725120.0000\n","Epoch 105/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1009828823040.0000 - val_loss: 16458529636352.0000\n","Epoch 106/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 812283068416.0000 - val_loss: 16458702651392.0000\n","Epoch 107/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 885120761856.0000 - val_loss: 16457631006720.0000\n","Epoch 108/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 799038963712.0000 - val_loss: 16457538732032.0000\n","Epoch 109/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 450269642752.0000 - val_loss: 16457009201152.0000\n","Epoch 110/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 215299489792.0000 - val_loss: 16457014444032.0000\n","Epoch 111/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1087517949952.0000 - val_loss: 16457215770624.0000\n","Epoch 112/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 278238724096.0000 - val_loss: 16456349646848.0000\n","Epoch 113/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 578457436160.0000 - val_loss: 16456223817728.0000\n","Epoch 114/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 181110865920.0000 - val_loss: 16455673315328.0000\n","Epoch 115/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 122355695616.0000 - val_loss: 16456611790848.0000\n","Epoch 116/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 679845167104.0000 - val_loss: 16455680655360.0000\n","Epoch 117/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 214987243520.0000 - val_loss: 16455413268480.0000\n","Epoch 118/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 54573178880.0000 - val_loss: 16455514980352.0000\n","Epoch 119/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 224807944192.0000 - val_loss: 16454609010688.0000\n","Epoch 120/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 323107553280.0000 - val_loss: 16453503811584.0000\n","Epoch 121/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 598655565824.0000 - val_loss: 16454274514944.0000\n","Epoch 122/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 569203884032.0000 - val_loss: 16454757908480.0000\n","Epoch 123/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 340021084160.0000 - val_loss: 16454181191680.0000\n","Epoch 124/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 438507012096.0000 - val_loss: 16453817335808.0000\n","Epoch 125/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1280242352128.0000 - val_loss: 16454417121280.0000\n","Epoch 126/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 122449739776.0000 - val_loss: 16453781684224.0000\n","Epoch 127/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 886875750400.0000 - val_loss: 16452876763136.0000\n","Epoch 128/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 186936147968.0000 - val_loss: 16453411536896.0000\n","Epoch 129/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 604068773888.0000 - val_loss: 16452488790016.0000\n","Epoch 130/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 632232214528.0000 - val_loss: 16452684873728.0000\n","Epoch 131/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 166525140992.0000 - val_loss: 16452431118336.0000\n","Epoch 132/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1189444452352.0000 - val_loss: 16452238180352.0000\n","Epoch 133/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1582059487232.0000 - val_loss: 16452307386368.0000\n","Epoch 134/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259960668160.0000 - val_loss: 16452602036224.0000\n","Epoch 135/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1040749887488.0000 - val_loss: 16451900538880.0000\n","Epoch 136/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 178498617344.0000 - val_loss: 16451246227456.0000\n","Epoch 137/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1528518279168.0000 - val_loss: 16452187848704.0000\n","Epoch 138/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 489683288064.0000 - val_loss: 16451477962752.0000\n","Epoch 139/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 314724483072.0000 - val_loss: 16450925363200.0000\n","Epoch 140/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 1736575418368.0000 - val_loss: 16451493691392.0000\n","Epoch 141/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1359141011456.0000 - val_loss: 16451347939328.0000\n","Epoch 142/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1764495589376.0000 - val_loss: 16451123544064.0000\n","Epoch 143/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 669632036864.0000 - val_loss: 16450373812224.0000\n","Epoch 144/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1385871048704.0000 - val_loss: 16449578991616.0000\n","Epoch 145/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 487556448256.0000 - val_loss: 16449473085440.0000\n","Epoch 146/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1695244746752.0000 - val_loss: 16449303216128.0000\n","Epoch 147/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 422837649408.0000 - val_loss: 16449560117248.0000\n","Epoch 148/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1279495503872.0000 - val_loss: 16449259175936.0000\n","Epoch 149/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 527860531200.0000 - val_loss: 16449035829248.0000\n","Epoch 150/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 929507573760.0000 - val_loss: 16448256737280.0000\n","Epoch 151/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1109060616192.0000 - val_loss: 16448229474304.0000\n","Epoch 152/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 446402166784.0000 - val_loss: 16447531122688.0000\n","Epoch 153/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 224204455936.0000 - val_loss: 16447592988672.0000\n","Epoch 154/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 236700860416.0000 - val_loss: 16447534268416.0000\n","Epoch 155/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 283452211200.0000 - val_loss: 16446977474560.0000\n","Epoch 156/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 946295603200.0000 - val_loss: 16447284707328.0000\n","Epoch 157/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2283120361472.0000 - val_loss: 16447314067456.0000\n","Epoch 158/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 713560227840.0000 - val_loss: 16447462965248.0000\n","Epoch 159/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 225512849408.0000 - val_loss: 16446487789568.0000\n","Epoch 160/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 677252890624.0000 - val_loss: 16446090379264.0000\n","Epoch 161/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 242454577152.0000 - val_loss: 16445718134784.0000\n","Epoch 162/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 683444338688.0000 - val_loss: 16445376299008.0000\n","Epoch 163/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 99372957696.0000 - val_loss: 16445247324160.0000\n","Epoch 164/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 134380199936.0000 - val_loss: 16445709746176.0000\n","Epoch 165/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1589511061504.0000 - val_loss: 16445202235392.0000\n","Epoch 166/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 246584246272.0000 - val_loss: 16444671655936.0000\n","Epoch 167/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 724744011776.0000 - val_loss: 16444650684416.0000\n","Epoch 168/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 379294613504.0000 - val_loss: 16444719890432.0000\n","Epoch 169/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2561561329664.0000 - val_loss: 16444616081408.0000\n","Epoch 170/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 565333458944.0000 - val_loss: 16443589525504.0000\n","Epoch 171/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 124373065728.0000 - val_loss: 16444883468288.0000\n","Epoch 172/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 974073495552.0000 - val_loss: 16443643002880.0000\n","Epoch 173/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 268175261696.0000 - val_loss: 16443009662976.0000\n","Epoch 174/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 315790426112.0000 - val_loss: 16442595475456.0000\n","Epoch 175/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 653906804736.0000 - val_loss: 16442517880832.0000\n","Epoch 176/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 135691403264.0000 - val_loss: 16443015954432.0000\n","Epoch 177/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 277359067136.0000 - val_loss: 16442710818816.0000\n","Epoch 178/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260560240640.0000 - val_loss: 16442242105344.0000\n","Epoch 179/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 53913894912.0000 - val_loss: 16442704527360.0000\n","Epoch 180/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 262974734336.0000 - val_loss: 16441714671616.0000\n","Epoch 181/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 656877027328.0000 - val_loss: 16441720963072.0000\n","Epoch 182/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 794539720704.0000 - val_loss: 16441247006720.0000\n","Epoch 183/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1189653512192.0000 - val_loss: 16441162072064.0000\n","Epoch 184/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 209773068288.0000 - val_loss: 16441576259584.0000\n","Epoch 185/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 698886455296.0000 - val_loss: 16440677629952.0000\n","Epoch 186/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 198800703488.0000 - val_loss: 16441466159104.0000\n","Epoch 187/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 527703113728.0000 - val_loss: 16440013881344.0000\n","Epoch 188/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 44536889344.0000 - val_loss: 16440783536128.0000\n","Epoch 189/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 172237045760.0000 - val_loss: 16440187944960.0000\n","Epoch 190/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 433798578176.0000 - val_loss: 16439490641920.0000\n","Epoch 191/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 917815427072.0000 - val_loss: 16439622762496.0000\n","Epoch 192/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 559005302784.0000 - val_loss: 16439263100928.0000\n","Epoch 193/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 873684140032.0000 - val_loss: 16439299801088.0000\n","Epoch 194/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1898730356736.0000 - val_loss: 16438420045824.0000\n","Epoch 195/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 535615963136.0000 - val_loss: 16438981033984.0000\n","Epoch 196/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1466369966080.0000 - val_loss: 16439133077504.0000\n","Epoch 197/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 808403533824.0000 - val_loss: 16438067724288.0000\n","Epoch 198/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 576235110400.0000 - val_loss: 16437590622208.0000\n","Epoch 199/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1347283320832.0000 - val_loss: 16437750005760.0000\n","Epoch 200/200\n","\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 715868405760.0000 - val_loss: 16437439627264.0000\n","\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18312093696.0000\n","FFNN Test Loss: 19312246784.0\n","\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","MAE: 63576.01117111019\n","Mean Squared Error: 19312246711.256477\n","RMSE: 138968.50978281547\n","R-squared: 0.8670282686167284\n"]}]},{"cell_type":"markdown","source":["Elamnn"],"metadata":{"id":"PUCbM8HDTD1R"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import SimpleRNN\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Build the Elman-like RNN model\n","model_enn = tf.keras.models.Sequential([\n","    SimpleRNN(50, activation='relu', input_shape=(10, 1), return_sequences=False),\n","    tf.keras.layers.Dense(1)  # Output layer\n","])\n","\n","# Compile the model\n","model_enn.compile(optimizer='adam', loss='mean_squared_error')\n","\n","# Train the model\n","model_enn.fit(X_train, y_train, epochs=100, batch_size=10, validation_split=0.1)\n","\n","# Evaluate the model\n","loss_enn = model_enn.evaluate(X_test, y_test)\n","print(f\"ENN Test Loss: {loss_enn}\")\n","\n","# Predicting costs using the ENN\n","predictions_enn = model_enn.predict(X_test)\n","mae = mean_absolute_error(y_test, predictions_enn)\n","mse = mean_squared_error(y_test, predictions_enn)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, predictions_enn)\n","\n","# Printing Result\n","print(\"MAE:\", mae)\n","print(\"Mean Squared Error:\", mse)\n","print(\"RMSE:\", rmse)\n","print(\"R-squared:\", r2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D9uT1cIiTEoz","executionInfo":{"status":"ok","timestamp":1719545953113,"user_tz":-330,"elapsed":420971,"user":{"displayName":"Shabana Yasmin","userId":"02459626693949289733"}},"outputId":"315d5cd7-fdd3-43b9-8f8a-0281760c42bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 728654872576.0000 - val_loss: 16548003577856.0000\n","Epoch 2/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 434242191360.0000 - val_loss: 16339376799744.0000\n","Epoch 3/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 1996036112384.0000 - val_loss: 16366032650240.0000\n","Epoch 4/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 332363431936.0000 - val_loss: 16382746951680.0000\n","Epoch 5/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 265485336576.0000 - val_loss: 16475526004736.0000\n","Epoch 6/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 1650376835072.0000 - val_loss: 16395858345984.0000\n","Epoch 7/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 797223485440.0000 - val_loss: 16409353519104.0000\n","Epoch 8/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 125107937280.0000 - val_loss: 16380623585280.0000\n","Epoch 9/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 1215825838080.0000 - val_loss: 16372306280448.0000\n","Epoch 10/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 572184068096.0000 - val_loss: 16503727456256.0000\n","Epoch 11/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 181368619008.0000 - val_loss: 16381681598464.0000\n","Epoch 12/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 2081460060160.0000 - val_loss: 16386814377984.0000\n","Epoch 13/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 250754875392.0000 - val_loss: 16374217834496.0000\n","Epoch 14/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 392578465792.0000 - val_loss: 16387562012672.0000\n","Epoch 15/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 1399559946240.0000 - val_loss: 16379344322560.0000\n","Epoch 16/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 311323885568.0000 - val_loss: 16388879024128.0000\n","Epoch 17/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 153890045952.0000 - val_loss: 16336565567488.0000\n","Epoch 18/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 198207913984.0000 - val_loss: 16407157800960.0000\n","Epoch 19/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 1301283471360.0000 - val_loss: 16350337564672.0000\n","Epoch 20/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 482240692224.0000 - val_loss: 16377333153792.0000\n","Epoch 21/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 287218630656.0000 - val_loss: 16365722271744.0000\n","Epoch 22/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 1716551024640.0000 - val_loss: 16392075083776.0000\n","Epoch 23/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 194190508032.0000 - val_loss: 16410257391616.0000\n","Epoch 24/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 1437892083712.0000 - val_loss: 16367138897920.0000\n","Epoch 25/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 574640553984.0000 - val_loss: 16385992294400.0000\n","Epoch 26/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 419107471360.0000 - val_loss: 16377076252672.0000\n","Epoch 27/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 168271839232.0000 - val_loss: 16423611006976.0000\n","Epoch 28/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 1909705015296.0000 - val_loss: 16373817278464.0000\n","Epoch 29/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 256677576704.0000 - val_loss: 16388812963840.0000\n","Epoch 30/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 908361269248.0000 - val_loss: 16396010389504.0000\n","Epoch 31/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 492682280960.0000 - val_loss: 16370398920704.0000\n","Epoch 32/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 828651339776.0000 - val_loss: 16385967128576.0000\n","Epoch 33/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 833088978944.0000 - val_loss: 16398734589952.0000\n","Epoch 34/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 71099424768.0000 - val_loss: 16411136098304.0000\n","Epoch 35/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2439755333632.0000 - val_loss: 16351409209344.0000\n","Epoch 36/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 526419787776.0000 - val_loss: 16409012731904.0000\n","Epoch 37/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 1666861105152.0000 - val_loss: 16392729395200.0000\n","Epoch 38/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 389170561024.0000 - val_loss: 16406930259968.0000\n","Epoch 39/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 84779401216.0000 - val_loss: 16342930423808.0000\n","Epoch 40/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 570678837248.0000 - val_loss: 16392767143936.0000\n","Epoch 41/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 464603414528.0000 - val_loss: 16395255414784.0000\n","Epoch 42/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 741379014656.0000 - val_loss: 16392422162432.0000\n","Epoch 43/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 913517117440.0000 - val_loss: 16388431282176.0000\n","Epoch 44/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2024098496512.0000 - val_loss: 16393481224192.0000\n","Epoch 45/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 236846628864.0000 - val_loss: 16374324789248.0000\n","Epoch 46/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2331393392640.0000 - val_loss: 16383681232896.0000\n","Epoch 47/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 649275768832.0000 - val_loss: 16381532700672.0000\n","Epoch 48/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 58367381504.0000 - val_loss: 16370217517056.0000\n","Epoch 49/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 234524426240.0000 - val_loss: 16387165650944.0000\n","Epoch 50/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 1820493217792.0000 - val_loss: 16395032068096.0000\n","Epoch 51/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 205013843968.0000 - val_loss: 16396896436224.0000\n","Epoch 52/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 341409955840.0000 - val_loss: 16389900337152.0000\n","Epoch 53/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 238160134144.0000 - val_loss: 16383193645056.0000\n","Epoch 54/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 539856961536.0000 - val_loss: 16386898264064.0000\n","Epoch 55/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 2929503502336.0000 - val_loss: 16391670333440.0000\n","Epoch 56/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 298081026048.0000 - val_loss: 16363328372736.0000\n","Epoch 57/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 81152868352.0000 - val_loss: 16484981014528.0000\n","Epoch 58/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 531343507456.0000 - val_loss: 16394169090048.0000\n","Epoch 59/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 363792302080.0000 - val_loss: 16399255732224.0000\n","Epoch 60/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1845649211392.0000 - val_loss: 16392724152320.0000\n","Epoch 61/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 1822380261376.0000 - val_loss: 16371528237056.0000\n","Epoch 62/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 248290508800.0000 - val_loss: 16419870736384.0000\n","Epoch 63/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 1691133804544.0000 - val_loss: 16383940231168.0000\n","Epoch 64/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 176395452416.0000 - val_loss: 16353243168768.0000\n","Epoch 65/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 1061698600960.0000 - val_loss: 16370792136704.0000\n","Epoch 66/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 72449064960.0000 - val_loss: 16358869827584.0000\n","Epoch 67/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 318569447424.0000 - val_loss: 16395960057856.0000\n","Epoch 68/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 270647885824.0000 - val_loss: 16377880510464.0000\n","Epoch 69/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 1219052437504.0000 - val_loss: 16379541454848.0000\n","Epoch 70/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 322390884352.0000 - val_loss: 16386233466880.0000\n","Epoch 71/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 449600192512.0000 - val_loss: 16376847663104.0000\n","Epoch 72/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 1386002513920.0000 - val_loss: 16373005680640.0000\n","Epoch 73/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 59939299328.0000 - val_loss: 16377603686400.0000\n","Epoch 74/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 441394135040.0000 - val_loss: 16378364952576.0000\n","Epoch 75/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 119276806144.0000 - val_loss: 16389900337152.0000\n","Epoch 76/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 1491187138560.0000 - val_loss: 16379426111488.0000\n","Epoch 77/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 1798355025920.0000 - val_loss: 16373561425920.0000\n","Epoch 78/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 735490211840.0000 - val_loss: 16379551940608.0000\n","Epoch 79/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 217787351040.0000 - val_loss: 16392032092160.0000\n","Epoch 80/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 442402045952.0000 - val_loss: 16388624220160.0000\n","Epoch 81/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 242846334976.0000 - val_loss: 16373170307072.0000\n","Epoch 82/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 1526411034624.0000 - val_loss: 16390534725632.0000\n","Epoch 83/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 603448279040.0000 - val_loss: 16376016142336.0000\n","Epoch 84/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 501474557952.0000 - val_loss: 16396220104704.0000\n","Epoch 85/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 558416789504.0000 - val_loss: 16375773921280.0000\n","Epoch 86/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 335218442240.0000 - val_loss: 16374830202880.0000\n","Epoch 87/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 1542479675392.0000 - val_loss: 16392905555968.0000\n","Epoch 88/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 1099997511680.0000 - val_loss: 16412066185216.0000\n","Epoch 89/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 402818400256.0000 - val_loss: 16390908018688.0000\n","Epoch 90/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 195771252736.0000 - val_loss: 16398277410816.0000\n","Epoch 91/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 79278022656.0000 - val_loss: 16372701593600.0000\n","Epoch 92/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 604716990464.0000 - val_loss: 16390759120896.0000\n","Epoch 93/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 169149186048.0000 - val_loss: 16369581031424.0000\n","Epoch 94/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 673492303872.0000 - val_loss: 16391258243072.0000\n","Epoch 95/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 1501827301376.0000 - val_loss: 16385848639488.0000\n","Epoch 96/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 201314402304.0000 - val_loss: 16401747148800.0000\n","Epoch 97/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 1116873555968.0000 - val_loss: 16388434427904.0000\n","Epoch 98/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 341813526528.0000 - val_loss: 16398467203072.0000\n","Epoch 99/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 230275547136.0000 - val_loss: 16350583980032.0000\n","Epoch 100/100\n","\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 2220313018368.0000 - val_loss: 16390832521216.0000\n","\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17025278976.0000\n","ENN Test Loss: 18975889408.0\n","\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","MAE: 57954.707331179\n","Mean Squared Error: 18975886841.18162\n","RMSE: 137752.9921315019\n","R-squared: 0.8693442267214702\n"]}]},{"cell_type":"markdown","source":["Ensemble Method"],"metadata":{"id":"8Lpq8DnYkcYa"}},{"cell_type":"code","source":["from sklearn.neural_network import MLPRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import numpy as np\n","import pandas as pd\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define and train the first model\n","model1 = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', random_state=42)\n","model1.fit(X_train, y_train)\n","\n","# Define and train the second model\n","model2 = MLPRegressor(hidden_layer_sizes=(150, 100, 50), activation='tanh', solver='sgd', random_state=42)\n","model2.fit(X_train, y_train)\n","\n","# Define and train the third model\n","model3 = MLPRegressor(hidden_layer_sizes=(200, 100), activation='relu', solver='adam', alpha=0.01, random_state=42)\n","model3.fit(X_train, y_train)\n"],"metadata":{"id":"_NgZpfqa7nKw","colab":{"base_uri":"https://localhost:8080/","height":80},"executionInfo":{"status":"ok","timestamp":1719556862599,"user_tz":-330,"elapsed":48446,"user":{"displayName":"Shabana Yasmin","userId":"13381702277896287900"}},"outputId":"a3fc4d18-e42c-4737-cbd1-3b2ecaed2dfb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MLPRegressor(alpha=0.01, hidden_layer_sizes=(200, 100), random_state=42)"],"text/html":["<style>#sk-container-id-1 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: black;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-1 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-1 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-1 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-1 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-1 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-1 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-1 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-1 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: block;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"▸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-1 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"▾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-1 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-1 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-1 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-1 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-1 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 1ex;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-1 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-1 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(alpha=0.01, hidden_layer_sizes=(200, 100), random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPRegressor.html\">?<span>Documentation for MLPRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPRegressor(alpha=0.01, hidden_layer_sizes=(200, 100), random_state=42)</pre></div> </div></div></div></div>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Make predictions with each model\n","y_pred1 = model1.predict(X_test)\n","y_pred2 = model2.predict(X_test)\n","y_pred3 = model3.predict(X_test)\n","# Combine the predictions by averaging\n","y_pred_ensemble = (y_pred1 + y_pred2 + y_pred3) / 3\n","# Evaluate the ensemble model\n","mae_ensemble = mean_absolute_error(y_test, y_pred_ensemble)\n","mse_ensemble = mean_squared_error(y_test, y_pred_ensemble)\n","rmse_ensemble = np.sqrt(mse_ensemble)\n","r2_ensemble = r2_score(y_test, y_pred_ensemble)\n","\n","# Print the results\n","print(\"Ensemble MAE:\", mae_ensemble)\n","print(\"Ensemble MSE:\", mse_ensemble)\n","print(\"Ensemble RMSE:\", rmse_ensemble)\n","print(\"Ensemble R-squared:\", r2_ensemble)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6DCbOC0pkrIN","executionInfo":{"status":"ok","timestamp":1719556879593,"user_tz":-330,"elapsed":726,"user":{"displayName":"Shabana Yasmin","userId":"13381702277896287900"}},"outputId":"97d5c8ab-7d4a-49e0-e977-40843b260691"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ensemble MAE: 113440.47904572096\n","Ensemble MSE: 47627894372.30749\n","Ensemble RMSE: 218238.15975284314\n","Ensemble R-squared: 0.6720648989465371\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"CjQjI1fsk66a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import numpy as np\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define and train the first model\n","model1 = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', random_state=42)\n","model1.fit(X_train, y_train)\n","\n","# Define and train the second model\n","model2 = MLPRegressor(hidden_layer_sizes=(150, 100, 50), activation='tanh', solver='sgd', random_state=42)\n","model2.fit(X_train, y_train)\n","\n","# Define and train the third model\n","model3 = MLPRegressor(hidden_layer_sizes=(200, 100), activation='relu', solver='adam', alpha=0.01, random_state=42)\n","model3.fit(X_train, y_train)\n","\n","# Make predictions with each model\n","y_pred1 = model1.predict(X_train)\n","y_pred2 = model2.predict(X_train)\n","y_pred3 = model3.predict(X_train)\n","\n","# Stack predictions for the training set of the meta-model\n","X_meta_train = np.column_stack((y_pred1, y_pred2, y_pred3))\n","\n","# Meta-model\n","meta_model = LinearRegression()\n","meta_model.fit(X_meta_train, y_train)\n","\n","# Make predictions with each model on the test set\n","y_pred1_test = model1.predict(X_test)\n","y_pred2_test = model2.predict(X_test)\n","y_pred3_test = model3.predict(X_test)\n","\n","# Stack predictions for the test set of the meta-model\n","X_meta_test = np.column_stack((y_pred1_test, y_pred2_test, y_pred3_test))\n","\n","# Meta-model predictions\n","y_pred_meta = meta_model.predict(X_meta_test)\n","\n","# Evaluate the stacked model\n","mae_meta = mean_absolute_error(y_test, y_pred_meta)\n","mse_meta = mean_squared_error(y_test, y_pred_meta)\n","rmse_meta = np.sqrt(mse_meta)\n","r2_meta = r2_score(y_test, y_pred_meta)\n","\n","# Print the results\n","print(\"Stacked Model MAE:\", mae_meta)\n","print(\"Stacked Model MSE:\", mse_meta)\n","print(\"Stacked Model RMSE:\", rmse_meta)\n","print(\"Stacked Model R-squared:\", r2_meta)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fjTPzeN8k-dN","executionInfo":{"status":"ok","timestamp":1719556941698,"user_tz":-330,"elapsed":46404,"user":{"displayName":"Shabana Yasmin","userId":"13381702277896287900"}},"outputId":"85bb2693-3fc7-4a63-86dd-12898f4f64c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Stacked Model MAE: 107786.62984078709\n","Stacked Model MSE: 36897154157.031334\n","Stacked Model RMSE: 192086.3195467895\n","Stacked Model R-squared: 0.745949886373592\n"]}]},{"cell_type":"code","source":["from sklearn.neural_network import MLPRegressor\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import numpy as np\n","import pandas as pd\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define the base models\n","model1 = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', random_state=42)\n","model2 = MLPRegressor(hidden_layer_sizes=(150, 100, 50), activation='tanh', solver='sgd', random_state=42)\n","model3 = MLPRegressor(hidden_layer_sizes=(200, 100), activation='relu', solver='adam', alpha=0.01, random_state=42)\n","\n","# Initialize the meta-features array\n","meta_features = np.zeros((X_train.shape[0], 3))  # Adjust number of columns to match the number of base models\n","meta_targets = y_train\n","\n","# Perform cross-validation to create meta-features\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","for train_idx, val_idx in kf.split(X_train):\n","    X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n","    # Use .iloc to slice y_train based on integer position\n","    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n","\n","    # Train each model on the training fold\n","    model1.fit(X_train_fold, y_train_fold)\n","    model2.fit(X_train_fold, y_train_fold)\n","    model3.fit(X_train_fold, y_train_fold)\n","\n","    # Generate meta-features for the validation fold\n","    meta_features[val_idx, 0] = model1.predict(X_val_fold)\n","    meta_features[val_idx, 1] = model2.predict(X_val_fold)\n","    meta_features[val_idx, 2] = model3.predict(X_val_fold)\n","\n","# Train the meta-model on the meta-features\n","meta_model = LinearRegression()\n","meta_model.fit(meta_features, meta_targets)\n","\n","# Generate meta-features for the test set\n","X_meta_test = np.column_stack((model1.predict(X_test), model2.predict(X_test), model3.predict(X_test)))\n","\n","# Meta-model predictions\n","y_pred_meta = meta_model.predict(X_meta_test)\n","\n","# Evaluate the stacked model\n","mae_meta = mean_absolute_error(y_test, y_pred_meta)\n","mse_meta = mean_squared_error(y_test, y_pred_meta)\n","rmse_meta = np.sqrt(mse_meta)\n","r2_meta = r2_score(y_test, y_pred_meta)\n","\n","# Print the results\n","print(\"Stacked Model MAE:\", mae_meta)\n","print(\"Stacked Model MSE:\", mse_meta)\n","print(\"Stacked Model RMSE:\", rmse_meta)\n","print(\"Stacked Model R-squared:\", r2_meta)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w9WT2e49k-8H","executionInfo":{"status":"ok","timestamp":1719558053066,"user_tz":-330,"elapsed":152655,"user":{"displayName":"Shabana Yasmin","userId":"13381702277896287900"}},"outputId":"446b5c20-46fc-4f64-d09d-5e8fe88c487f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Stacked Model MAE: 66475.08064520282\n","Stacked Model MSE: 24247347105.265633\n","Stacked Model RMSE: 155715.59685935648\n","Stacked Model R-squared: 0.8330483358956345\n"]}]},{"cell_type":"markdown","source":["Augmentation"],"metadata":{"id":"4bWQ0nl9luqb"}},{"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import numpy as np\n","\n","# Separate features and target variable\n","X = df.drop(\"Total Project Cost (USD)\", axis=1)\n","y = df[\"Total Project Cost (USD)\"]\n","\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","def add_gaussian_noise(X, y, noise_level=0.01):\n","    noise = np.random.normal(0, noise_level, X.shape)\n","    X_noisy = X + noise\n","    return X_noisy, y\n","\n","def transform_data(X, y):\n","    # Handle potential negative or zero values before log transformation\n","    X_non_negative = np.where(X <= 0, 1e-10, X)  # Replace non-positive values with a small positive value\n","    X_transformed = np.log1p(X_non_negative)  # Apply log transformation\n","    return X_transformed, y\n","\n","def data_augmentation_pipeline(X, y):\n","    # Step 1: Add Gaussian noise\n","    X_noisy, y_noisy = add_gaussian_noise(X, y)\n","\n","    # Step 2: Apply log transformation\n","    X_transformed, y_transformed = transform_data(X, y)\n","\n","    # Combine original and augmented data\n","    X_augmented = np.vstack((X, X_noisy, X_transformed))\n","    y_augmented = np.hstack((y, y_noisy, y_transformed))\n","\n","    return X_augmented, y_augmented\n","\n","# Apply the data augmentation pipeline\n","X_train_augmented, y_train_augmented = data_augmentation_pipeline(X_train, y_train)\n","\n","# Define and train the MLPNN model with augmented data\n","mlp = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', random_state=42)\n","mlp.fit(X_train_augmented, y_train_augmented)\n","\n","# Make predictions\n","y_pred_augmented = mlp.predict(X_test)\n","\n","# Evaluate the model\n","mae_augmented = mean_absolute_error(y_test, y_pred_augmented)\n","mse_augmented = mean_squared_error(y_test, y_pred_augmented)\n","rmse_augmented = np.sqrt(mse_augmented)\n","r2_augmented = r2_score(y_test, y_pred_augmented)\n","\n","# Printing Result\n","print(\"MAE with Augmented Data:\", mae_augmented)\n","print(\"Mean Squared Error with Augmented Data:\", mse_augmented)\n","print(\"RMSE with Augmented Data:\", rmse_augmented)\n","print(\"R-squared with Augmented Data:\", r2_augmented)"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dfl1PssymhMC","executionInfo":{"status":"ok","timestamp":1719557342431,"user_tz":-330,"elapsed":42717,"user":{"displayName":"Shabana Yasmin","userId":"13381702277896287900"}},"outputId":"4c16e0af-b37a-4644-c0f7-07587c2d5360"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MAE with Augmented Data: 102948.81284347716\n","Mean Squared Error with Augmented Data: 35122481867.61503\n","RMSE with Augmented Data: 187409.93001336677\n","R-squared with Augmented Data: 0.7581691403262694\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"SRHEoBQPmPKO"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1DhP-PAV40gJdhELwAT7emiubz43LcEB9","timestamp":1719383264423},{"file_id":"1hyIICkHPpniWOYzKUkBtZa8IVEWGfdTv","timestamp":1716868413575},{"file_id":"1TVaO3Gmq8-gJkKSJoG8DGchhq0b80LVY","timestamp":1713296403617}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}